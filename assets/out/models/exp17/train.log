/home/shandilya/Desktop/Projects/NavigationController
Env Type: maze
Task: GoalRewardSimple
Traceback (most recent call last):
  File "train.py", line 80, in <module>
    lmbda = args.lmbda
  File "/home/shandilya/Desktop/Projects/NavigationController/learning/explore.py", line 73, in __init__
    lambda : sb3.common.monitor.Monitor(env_class(
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/Desktop/Projects/NavigationController/learning/explore.py", line 78, in <lambda>
    history_steps
  File "/home/shandilya/Desktop/Projects/NavigationController/simulations/maze_env.py", line 117, in __init__
    self.set_env()
  File "/home/shandilya/Desktop/Projects/NavigationController/simulations/maze_env.py", line 244, in set_env
    self.sampled_path = self.__sample_path()
  File "/home/shandilya/Desktop/Projects/NavigationController/simulations/maze_env.py", line 310, in __sample_path
    target
  File "/home/shandilya/py36/lib/python3.6/site-packages/networkx/algorithms/shortest_paths/generic.py", line 526, in _build_paths_from_predecessors
    f"Target {target} cannot be reached" f"from given sources"
networkx.exception.NetworkXNoPath: Target 9 cannot be reachedfrom given sources
2021-12-17 12:50:39.042658: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-17 12:50:39.042694: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Env Type: maze
Task: GoalRewardSimple
Model: <class 'stable_baselines3.td3.td3.TD3'>
Using cpu device
Logging to assets/out/models/exp17/TD3_7
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | 0.742    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 65       |
|    time_elapsed    | 13       |
|    total timesteps | 900      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 11.7     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 74       |
|    time_elapsed    | 25       |
|    total timesteps | 1874     |
---------------------------------
Traceback (most recent call last):
  File "train.py", line 82, in <module>
    model.learn(args.timesteps)
  File "/home/shandilya/Desktop/Projects/NavigationController/learning/explore.py", line 229, in learn
    callback = self.rl_callback
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 211, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 359, in learn
    log_interval=log_interval,
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 563, in collect_rollouts
    action, buffer_action = self._sample_action(learning_starts, action_noise)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 407, in _sample_action
    unscaled_action, _ = self.predict(self._last_obs, deterministic=False)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/base_class.py", line 539, in predict
    return self.policy.predict(observation, state, mask, deterministic)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/policies.py", line 302, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/td3/policies.py", line 226, in _predict
    return self.actor(observation)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/td3/policies.py", line 77, in forward
    features = self.extract_features(obs)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/policies.py", line 128, in extract_features
    return self.features_extractor(preprocessed_obs)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/shandilya/Desktop/Projects/NavigationController/utils/td3_utils.py", line 98, in forward
    out = self.fc(x)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 96, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/functional.py", line 1847, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x900 and 1200x300)
2021-12-17 12:52:33.380954: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-17 12:52:33.380998: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Env Type: maze
Task: GoalRewardSimple
Model: <class 'stable_baselines3.td3.td3.TD3'>
Using cpu device
Logging to assets/out/models/exp17/TD3_9
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | 1.84     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 76       |
|    time_elapsed    | 13       |
|    total timesteps | 995      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 242      |
|    ep_rew_mean     | 8.84     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 79       |
|    time_elapsed    | 24       |
|    total timesteps | 1932     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 245      |
|    ep_rew_mean     | -5.35    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 56       |
|    time_elapsed    | 51       |
|    total timesteps | 2936     |
| train/             |          |
|    actor_loss      | -0.106   |
|    critic_loss     | 2.79     |
|    learning_rate   | 0.000997 |
|    n_updates       | 251      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 245      |
|    ep_rew_mean     | -1.52    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 31       |
|    time_elapsed    | 125      |
|    total timesteps | 3914     |
| train/             |          |
|    actor_loss      | -0.189   |
|    critic_loss     | 2.03     |
|    learning_rate   | 0.000996 |
|    n_updates       | 1229     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | 1.71     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 23       |
|    time_elapsed    | 205      |
|    total timesteps | 4918     |
| train/             |          |
|    actor_loss      | -0.156   |
|    critic_loss     | 2.36     |
|    learning_rate   | 0.000995 |
|    n_updates       | 2233     |
---------------------------------
Eval num_timesteps=5000, episode_reward=54.92 +/- 12.81
Episode length: 180.40 +/- 7.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 54.9     |
| time/              |          |
|    total_timesteps | 5000     |
| train/             |          |
|    actor_loss      | -0.15    |
|    critic_loss     | 2.04     |
|    learning_rate   | 0.000995 |
|    n_updates       | 2484     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | 0.601    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 19       |
|    time_elapsed    | 298      |
|    total timesteps | 5914     |
| train/             |          |
|    actor_loss      | -0.176   |
|    critic_loss     | 2.07     |
|    learning_rate   | 0.000994 |
|    n_updates       | 3229     |
---------------------------------
Terminated
2021-12-17 12:58:28.688505: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-17 12:58:28.688548: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Env Type: maze
Task: GoalRewardSimple
Model: <class 'stable_baselines3.td3.td3.TD3'>
Using cpu device
Logging to assets/out/models/exp17/TD3_10
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -42.4    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 73       |
|    time_elapsed    | 13       |
|    total timesteps | 1004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 242      |
|    ep_rew_mean     | -31.8    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 78       |
|    time_elapsed    | 24       |
|    total timesteps | 1937     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 245      |
|    ep_rew_mean     | -25.8    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 55       |
|    time_elapsed    | 52       |
|    total timesteps | 2941     |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 3.74     |
|    learning_rate   | 0.000997 |
|    n_updates       | 251      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | -19.4    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 30       |
|    time_elapsed    | 126      |
|    total timesteps | 3934     |
| train/             |          |
|    actor_loss      | 0.394    |
|    critic_loss     | 2.3      |
|    learning_rate   | 0.000996 |
|    n_updates       | 1244     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -16.5    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 23       |
|    time_elapsed    | 207      |
|    total timesteps | 4938     |
| train/             |          |
|    actor_loss      | 0.556    |
|    critic_loss     | 1.79     |
|    learning_rate   | 0.000995 |
|    n_updates       | 2248     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-32.29 +/- 18.11
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -32.3    |
| time/              |          |
|    total_timesteps | 5000     |
| train/             |          |
|    actor_loss      | 0.584    |
|    critic_loss     | 1.87     |
|    learning_rate   | 0.000995 |
|    n_updates       | 2499     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -17.2    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 19       |
|    time_elapsed    | 305      |
|    total timesteps | 5942     |
| train/             |          |
|    actor_loss      | 0.699    |
|    critic_loss     | 2.17     |
|    learning_rate   | 0.000994 |
|    n_updates       | 3252     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -18.2    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 18       |
|    time_elapsed    | 385      |
|    total timesteps | 6946     |
| train/             |          |
|    actor_loss      | 0.823    |
|    critic_loss     | 2.31     |
|    learning_rate   | 0.000993 |
|    n_updates       | 4256     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -19.5    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 17       |
|    time_elapsed    | 464      |
|    total timesteps | 7950     |
| train/             |          |
|    actor_loss      | 1        |
|    critic_loss     | 2.12     |
|    learning_rate   | 0.000992 |
|    n_updates       | 5260     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -17.3    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 16       |
|    time_elapsed    | 545      |
|    total timesteps | 8954     |
| train/             |          |
|    actor_loss      | 1.14     |
|    critic_loss     | 2.03     |
|    learning_rate   | 0.000991 |
|    n_updates       | 6264     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -20.4    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 15       |
|    time_elapsed    | 624      |
|    total timesteps | 9958     |
| train/             |          |
|    actor_loss      | 1.24     |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.00099  |
|    n_updates       | 7268     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-30.62 +/- 18.57
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -30.6    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.29     |
|    critic_loss     | 2.32     |
|    learning_rate   | 0.00099  |
|    n_updates       | 7519     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -21.8    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 15       |
|    time_elapsed    | 726      |
|    total timesteps | 10962    |
| train/             |          |
|    actor_loss      | 1.36     |
|    critic_loss     | 1.92     |
|    learning_rate   | 0.000989 |
|    n_updates       | 8272     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -22.6    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 14       |
|    time_elapsed    | 807      |
|    total timesteps | 11966    |
| train/             |          |
|    actor_loss      | 1.55     |
|    critic_loss     | 2.66     |
|    learning_rate   | 0.000988 |
|    n_updates       | 9276     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -22.9    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 14       |
|    time_elapsed    | 886      |
|    total timesteps | 12970    |
| train/             |          |
|    actor_loss      | 1.7      |
|    critic_loss     | 2.26     |
|    learning_rate   | 0.000987 |
|    n_updates       | 10280    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -22.2    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 14       |
|    time_elapsed    | 969      |
|    total timesteps | 13974    |
| train/             |          |
|    actor_loss      | 1.87     |
|    critic_loss     | 2.78     |
|    learning_rate   | 0.000986 |
|    n_updates       | 11284    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -23      |
| time/              |          |
|    episodes        | 60       |
|    fps             | 14       |
|    time_elapsed    | 1050     |
|    total timesteps | 14978    |
| train/             |          |
|    actor_loss      | 2.07     |
|    critic_loss     | 2.81     |
|    learning_rate   | 0.000985 |
|    n_updates       | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-32.23 +/- 7.68
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -32.2    |
| time/              |          |
|    total_timesteps | 15000    |
| train/             |          |
|    actor_loss      | 2.09     |
|    critic_loss     | 2.68     |
|    learning_rate   | 0.000985 |
|    n_updates       | 12539    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -22.6    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 13       |
|    time_elapsed    | 1150     |
|    total timesteps | 15982    |
| train/             |          |
|    actor_loss      | 2.19     |
|    critic_loss     | 2.67     |
|    learning_rate   | 0.000984 |
|    n_updates       | 13292    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -21.3    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 13       |
|    time_elapsed    | 1229     |
|    total timesteps | 16962    |
| train/             |          |
|    actor_loss      | 2.31     |
|    critic_loss     | 2.68     |
|    learning_rate   | 0.000983 |
|    n_updates       | 14272    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -21      |
| time/              |          |
|    episodes        | 72       |
|    fps             | 13       |
|    time_elapsed    | 1309     |
|    total timesteps | 17966    |
| train/             |          |
|    actor_loss      | 2.43     |
|    critic_loss     | 2.62     |
|    learning_rate   | 0.000982 |
|    n_updates       | 15276    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -20.9    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 13       |
|    time_elapsed    | 1390     |
|    total timesteps | 18970    |
| train/             |          |
|    actor_loss      | 2.55     |
|    critic_loss     | 2.33     |
|    learning_rate   | 0.000981 |
|    n_updates       | 16280    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -19.9    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 13       |
|    time_elapsed    | 1469     |
|    total timesteps | 19974    |
| train/             |          |
|    actor_loss      | 2.66     |
|    critic_loss     | 2.21     |
|    learning_rate   | 0.00098  |
|    n_updates       | 17284    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-36.23 +/- 14.11
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -36.2    |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 2.67     |
|    critic_loss     | 2.11     |
|    learning_rate   | 0.00098  |
|    n_updates       | 17535    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -19.8    |
| time/              |          |
|    episodes        | 84       |
|    fps             | 13       |
|    time_elapsed    | 1571     |
|    total timesteps | 20978    |
| train/             |          |
|    actor_loss      | 2.75     |
|    critic_loss     | 2.49     |
|    learning_rate   | 0.000979 |
|    n_updates       | 18288    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -20.3    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 13       |
|    time_elapsed    | 1654     |
|    total timesteps | 21982    |
| train/             |          |
|    actor_loss      | 2.84     |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.000978 |
|    n_updates       | 19292    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -19.2    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 13       |
|    time_elapsed    | 1735     |
|    total timesteps | 22986    |
| train/             |          |
|    actor_loss      | 2.97     |
|    critic_loss     | 3.2      |
|    learning_rate   | 0.000977 |
|    n_updates       | 20296    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -18.6    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 13       |
|    time_elapsed    | 1816     |
|    total timesteps | 23990    |
| train/             |          |
|    actor_loss      | 3.03     |
|    critic_loss     | 2.67     |
|    learning_rate   | 0.000976 |
|    n_updates       | 21300    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -17.9    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 13       |
|    time_elapsed    | 1891     |
|    total timesteps | 24927    |
| train/             |          |
|    actor_loss      | 3.12     |
|    critic_loss     | 2.42     |
|    learning_rate   | 0.000976 |
|    n_updates       | 22237    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-42.58 +/- 19.09
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -42.6    |
| time/              |          |
|    total_timesteps | 25000    |
| train/             |          |
|    actor_loss      | 3.15     |
|    critic_loss     | 2.8      |
|    learning_rate   | 0.000975 |
|    n_updates       | 22488    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -17      |
| time/              |          |
|    episodes        | 104      |
|    fps             | 13       |
|    time_elapsed    | 1990     |
|    total timesteps | 25931    |
| train/             |          |
|    actor_loss      | 3.21     |
|    critic_loss     | 2.13     |
|    learning_rate   | 0.000975 |
|    n_updates       | 23241    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    episodes        | 108      |
|    fps             | 13       |
|    time_elapsed    | 2070     |
|    total timesteps | 26935    |
| train/             |          |
|    actor_loss      | 3.27     |
|    critic_loss     | 2.52     |
|    learning_rate   | 0.000974 |
|    n_updates       | 24245    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16      |
| time/              |          |
|    episodes        | 112      |
|    fps             | 12       |
|    time_elapsed    | 2150     |
|    total timesteps | 27939    |
| train/             |          |
|    actor_loss      | 3.33     |
|    critic_loss     | 2.18     |
|    learning_rate   | 0.000973 |
|    n_updates       | 25249    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.9    |
| time/              |          |
|    episodes        | 116      |
|    fps             | 12       |
|    time_elapsed    | 2231     |
|    total timesteps | 28943    |
| train/             |          |
|    actor_loss      | 3.4      |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.000972 |
|    n_updates       | 26253    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -17.8    |
| time/              |          |
|    episodes        | 120      |
|    fps             | 12       |
|    time_elapsed    | 2313     |
|    total timesteps | 29947    |
| train/             |          |
|    actor_loss      | 3.45     |
|    critic_loss     | 2.44     |
|    learning_rate   | 0.000971 |
|    n_updates       | 27257    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-39.10 +/- 12.67
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -39.1    |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 3.47     |
|    critic_loss     | 2.37     |
|    learning_rate   | 0.00097  |
|    n_updates       | 27508    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -18      |
| time/              |          |
|    episodes        | 124      |
|    fps             | 12       |
|    time_elapsed    | 2413     |
|    total timesteps | 30951    |
| train/             |          |
|    actor_loss      | 3.48     |
|    critic_loss     | 2.28     |
|    learning_rate   | 0.00097  |
|    n_updates       | 28261    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -18      |
| time/              |          |
|    episodes        | 128      |
|    fps             | 12       |
|    time_elapsed    | 2493     |
|    total timesteps | 31955    |
| train/             |          |
|    actor_loss      | 3.53     |
|    critic_loss     | 2        |
|    learning_rate   | 0.000969 |
|    n_updates       | 29265    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.7    |
| time/              |          |
|    episodes        | 132      |
|    fps             | 12       |
|    time_elapsed    | 2575     |
|    total timesteps | 32959    |
| train/             |          |
|    actor_loss      | 3.61     |
|    critic_loss     | 2.39     |
|    learning_rate   | 0.000968 |
|    n_updates       | 30269    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.7    |
| time/              |          |
|    episodes        | 136      |
|    fps             | 12       |
|    time_elapsed    | 2656     |
|    total timesteps | 33963    |
| train/             |          |
|    actor_loss      | 3.64     |
|    critic_loss     | 1.84     |
|    learning_rate   | 0.000967 |
|    n_updates       | 31273    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -14.8    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 12       |
|    time_elapsed    | 2736     |
|    total timesteps | 34967    |
| train/             |          |
|    actor_loss      | 3.67     |
|    critic_loss     | 1.69     |
|    learning_rate   | 0.000966 |
|    n_updates       | 32277    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-35.89 +/- 11.40
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -35.9    |
| time/              |          |
|    total_timesteps | 35000    |
| train/             |          |
|    actor_loss      | 3.7      |
|    critic_loss     | 2.08     |
|    learning_rate   | 0.000965 |
|    n_updates       | 32528    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -14.9    |
| time/              |          |
|    episodes        | 144      |
|    fps             | 12       |
|    time_elapsed    | 2835     |
|    total timesteps | 35971    |
| train/             |          |
|    actor_loss      | 3.7      |
|    critic_loss     | 1.69     |
|    learning_rate   | 0.000965 |
|    n_updates       | 33281    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -14      |
| time/              |          |
|    episodes        | 148      |
|    fps             | 12       |
|    time_elapsed    | 2916     |
|    total timesteps | 36975    |
| train/             |          |
|    actor_loss      | 3.76     |
|    critic_loss     | 2.39     |
|    learning_rate   | 0.000964 |
|    n_updates       | 34285    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -13.6    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 12       |
|    time_elapsed    | 3001     |
|    total timesteps | 37979    |
| train/             |          |
|    actor_loss      | 3.81     |
|    critic_loss     | 2.3      |
|    learning_rate   | 0.000963 |
|    n_updates       | 35289    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -14      |
| time/              |          |
|    episodes        | 156      |
|    fps             | 12       |
|    time_elapsed    | 3084     |
|    total timesteps | 38983    |
| train/             |          |
|    actor_loss      | 3.88     |
|    critic_loss     | 2.43     |
|    learning_rate   | 0.000962 |
|    n_updates       | 36293    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -14.1    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 12       |
|    time_elapsed    | 3167     |
|    total timesteps | 39987    |
| train/             |          |
|    actor_loss      | 3.91     |
|    critic_loss     | 2.42     |
|    learning_rate   | 0.000961 |
|    n_updates       | 37297    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-31.85 +/- 15.23
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -31.8    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 3.9      |
|    critic_loss     | 2.21     |
|    learning_rate   | 0.00096  |
|    n_updates       | 37548    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    episodes        | 164      |
|    fps             | 12       |
|    time_elapsed    | 3269     |
|    total timesteps | 40991    |
| train/             |          |
|    actor_loss      | 3.97     |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.00096  |
|    n_updates       | 38301    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -15.5    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 12       |
|    time_elapsed    | 3354     |
|    total timesteps | 41995    |
| train/             |          |
|    actor_loss      | 3.99     |
|    critic_loss     | 2.39     |
|    learning_rate   | 0.000959 |
|    n_updates       | 39305    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -15.4    |
| time/              |          |
|    episodes        | 172      |
|    fps             | 12       |
|    time_elapsed    | 3438     |
|    total timesteps | 42999    |
| train/             |          |
|    actor_loss      | 3.96     |
|    critic_loss     | 1.94     |
|    learning_rate   | 0.000958 |
|    n_updates       | 40309    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -15.6    |
| time/              |          |
|    episodes        | 176      |
|    fps             | 12       |
|    time_elapsed    | 3524     |
|    total timesteps | 44003    |
| train/             |          |
|    actor_loss      | 4.04     |
|    critic_loss     | 2.3      |
|    learning_rate   | 0.000957 |
|    n_updates       | 41313    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-32.50 +/- 15.60
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -32.5    |
| time/              |          |
|    total_timesteps | 45000    |
| train/             |          |
|    actor_loss      | 4.09     |
|    critic_loss     | 2.23     |
|    learning_rate   | 0.000956 |
|    n_updates       | 42317    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.5    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 12       |
|    time_elapsed    | 3628     |
|    total timesteps | 45007    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -15.9    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 12       |
|    time_elapsed    | 3715     |
|    total timesteps | 46011    |
| train/             |          |
|    actor_loss      | 4.12     |
|    critic_loss     | 2.46     |
|    learning_rate   | 0.000955 |
|    n_updates       | 43321    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -15.5    |
| time/              |          |
|    episodes        | 188      |
|    fps             | 12       |
|    time_elapsed    | 3802     |
|    total timesteps | 47015    |
| train/             |          |
|    actor_loss      | 4.18     |
|    critic_loss     | 2.28     |
|    learning_rate   | 0.000954 |
|    n_updates       | 44325    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.2    |
| time/              |          |
|    episodes        | 192      |
|    fps             | 12       |
|    time_elapsed    | 3889     |
|    total timesteps | 48019    |
| train/             |          |
|    actor_loss      | 4.22     |
|    critic_loss     | 2.16     |
|    learning_rate   | 0.000953 |
|    n_updates       | 45329    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.4    |
| time/              |          |
|    episodes        | 196      |
|    fps             | 12       |
|    time_elapsed    | 3981     |
|    total timesteps | 49023    |
| train/             |          |
|    actor_loss      | 4.25     |
|    critic_loss     | 2.08     |
|    learning_rate   | 0.000952 |
|    n_updates       | 46333    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-40.35 +/- 13.72
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -40.4    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 4.23     |
|    critic_loss     | 2.01     |
|    learning_rate   | 0.000951 |
|    n_updates       | 47337    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -17      |
| time/              |          |
|    episodes        | 200      |
|    fps             | 12       |
|    time_elapsed    | 4087     |
|    total timesteps | 50027    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -17.3    |
| time/              |          |
|    episodes        | 204      |
|    fps             | 12       |
|    time_elapsed    | 4177     |
|    total timesteps | 51031    |
| train/             |          |
|    actor_loss      | 4.24     |
|    critic_loss     | 2.33     |
|    learning_rate   | 0.00095  |
|    n_updates       | 48341    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -17.5    |
| time/              |          |
|    episodes        | 208      |
|    fps             | 12       |
|    time_elapsed    | 4265     |
|    total timesteps | 52035    |
| train/             |          |
|    actor_loss      | 4.27     |
|    critic_loss     | 2.14     |
|    learning_rate   | 0.000949 |
|    n_updates       | 49345    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -18.1    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 12       |
|    time_elapsed    | 4355     |
|    total timesteps | 53039    |
| train/             |          |
|    actor_loss      | 4.31     |
|    critic_loss     | 2.12     |
|    learning_rate   | 0.000948 |
|    n_updates       | 50349    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -18.1    |
| time/              |          |
|    episodes        | 216      |
|    fps             | 12       |
|    time_elapsed    | 4444     |
|    total timesteps | 54043    |
| train/             |          |
|    actor_loss      | 4.36     |
|    critic_loss     | 2.49     |
|    learning_rate   | 0.000947 |
|    n_updates       | 51353    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-24.70 +/- 4.63
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -24.7    |
| time/              |          |
|    total_timesteps | 55000    |
| train/             |          |
|    actor_loss      | 4.36     |
|    critic_loss     | 2.11     |
|    learning_rate   | 0.000946 |
|    n_updates       | 52357    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -17.9    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 12       |
|    time_elapsed    | 4551     |
|    total timesteps | 55047    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -17.8    |
| time/              |          |
|    episodes        | 224      |
|    fps             | 12       |
|    time_elapsed    | 4641     |
|    total timesteps | 56051    |
| train/             |          |
|    actor_loss      | 4.35     |
|    critic_loss     | 1.99     |
|    learning_rate   | 0.000945 |
|    n_updates       | 53361    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -17.7    |
| time/              |          |
|    episodes        | 228      |
|    fps             | 12       |
|    time_elapsed    | 4732     |
|    total timesteps | 57055    |
| train/             |          |
|    actor_loss      | 4.35     |
|    critic_loss     | 1.68     |
|    learning_rate   | 0.000944 |
|    n_updates       | 54365    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -18.5    |
| time/              |          |
|    episodes        | 232      |
|    fps             | 12       |
|    time_elapsed    | 4822     |
|    total timesteps | 58059    |
| train/             |          |
|    actor_loss      | 4.4      |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.000943 |
|    n_updates       | 55369    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -19      |
| time/              |          |
|    episodes        | 236      |
|    fps             | 12       |
|    time_elapsed    | 4914     |
|    total timesteps | 59063    |
| train/             |          |
|    actor_loss      | 4.46     |
|    critic_loss     | 2.77     |
|    learning_rate   | 0.000942 |
|    n_updates       | 56373    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-45.81 +/- 23.92
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -45.8    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 4.44     |
|    critic_loss     | 1.88     |
|    learning_rate   | 0.000941 |
|    n_updates       | 57377    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -19.6    |
| time/              |          |
|    episodes        | 240      |
|    fps             | 11       |
|    time_elapsed    | 5025     |
|    total timesteps | 60067    |
---------------------------------
Terminated
2021-12-17 14:22:46.480619: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-17 14:22:46.480655: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Env Type: maze
Task: GoalRewardSimple
Model: <class 'stable_baselines3.td3.td3.TD3'>
Using cpu device
Logging to assets/out/models/exp17/TD3_11
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 68       |
|    time_elapsed    | 14       |
|    total timesteps | 1004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -21      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 72       |
|    time_elapsed    | 27       |
|    total timesteps | 2008     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -22.8    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 32       |
|    time_elapsed    | 93       |
|    total timesteps | 3012     |
| train/             |          |
|    actor_loss      | 0.129    |
|    critic_loss     | 2.39     |
|    learning_rate   | 0.000997 |
|    n_updates       | 502      |
---------------------------------
2021-12-17 14:28:32.398221: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-17 14:28:32.398271: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Env Type: maze
Task: GoalRewardSimple
Model: <class 'stable_baselines3.td3.td3.TD3'>
Using cuda device
Logging to assets/out/models/exp17/TD3_13
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -27.1    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 100      |
|    time_elapsed    | 9        |
|    total timesteps | 1004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -18.2    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 109      |
|    time_elapsed    | 18       |
|    total timesteps | 2008     |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -22.1    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 69       |
|    time_elapsed    | 43       |
|    total timesteps | 3012     |
| train/             |          |
|    actor_loss      | 0.199    |
|    critic_loss     | 2.71     |
|    learning_rate   | 0.000997 |
|    n_updates       | 502      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 243      |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 48       |
|    time_elapsed    | 80       |
|    total timesteps | 3892     |
| train/             |          |
|    actor_loss      | 0.285    |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.000996 |
|    n_updates       | 1382     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 242      |
|    ep_rew_mean     | -14.7    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 39       |
|    time_elapsed    | 122      |
|    total timesteps | 4839     |
| train/             |          |
|    actor_loss      | 0.401    |
|    critic_loss     | 1.72     |
|    learning_rate   | 0.000995 |
|    n_updates       | 2386     |
---------------------------------
Eval num_timesteps=5000, episode_reward=34.65 +/- 27.09
Episode length: 247.40 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 34.7     |
| time/              |          |
|    total_timesteps | 5000     |
| train/             |          |
|    actor_loss      | 0.389    |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.000995 |
|    n_updates       | 2580     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 243      |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 33       |
|    time_elapsed    | 176      |
|    total timesteps | 5843     |
| train/             |          |
|    actor_loss      | 0.497    |
|    critic_loss     | 1.79     |
|    learning_rate   | 0.000994 |
|    n_updates       | 3333     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 244      |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 31       |
|    time_elapsed    | 219      |
|    total timesteps | 6822     |
| train/             |          |
|    actor_loss      | 0.592    |
|    critic_loss     | 1.94     |
|    learning_rate   | 0.000993 |
|    n_updates       | 4337     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 245      |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 29       |
|    time_elapsed    | 262      |
|    total timesteps | 7826     |
| train/             |          |
|    actor_loss      | 0.687    |
|    critic_loss     | 1.88     |
|    learning_rate   | 0.000993 |
|    n_updates       | 5316     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 245      |
|    ep_rew_mean     | -10.8    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 28       |
|    time_elapsed    | 306      |
|    total timesteps | 8830     |
| train/             |          |
|    actor_loss      | 0.712    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.000992 |
|    n_updates       | 6320     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | -10.1    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 28       |
|    time_elapsed    | 349      |
|    total timesteps | 9834     |
| train/             |          |
|    actor_loss      | 0.785    |
|    critic_loss     | 1.98     |
|    learning_rate   | 0.000991 |
|    n_updates       | 7324     |
---------------------------------
Eval num_timesteps=10000, episode_reward=37.22 +/- 14.60
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 37.2     |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 0.799    |
|    critic_loss     | 1.84     |
|    learning_rate   | 0.00099  |
|    n_updates       | 7575     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | -10.1    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 26       |
|    time_elapsed    | 407      |
|    total timesteps | 10838    |
| train/             |          |
|    actor_loss      | 0.869    |
|    critic_loss     | 2.18     |
|    learning_rate   | 0.00099  |
|    n_updates       | 8328     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -10.6    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 26       |
|    time_elapsed    | 451      |
|    total timesteps | 11842    |
| train/             |          |
|    actor_loss      | 0.909    |
|    critic_loss     | 1.37     |
|    learning_rate   | 0.000989 |
|    n_updates       | 9332     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 25       |
|    time_elapsed    | 494      |
|    total timesteps | 12846    |
| train/             |          |
|    actor_loss      | 0.987    |
|    critic_loss     | 1.38     |
|    learning_rate   | 0.000988 |
|    n_updates       | 10336    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 25       |
|    time_elapsed    | 537      |
|    total timesteps | 13850    |
| train/             |          |
|    actor_loss      | 1.08     |
|    critic_loss     | 1.99     |
|    learning_rate   | 0.000987 |
|    n_updates       | 11340    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 25       |
|    time_elapsed    | 579      |
|    total timesteps | 14854    |
| train/             |          |
|    actor_loss      | 1.15     |
|    critic_loss     | 1.92     |
|    learning_rate   | 0.000986 |
|    n_updates       | 12344    |
---------------------------------
Eval num_timesteps=15000, episode_reward=44.53 +/- 20.42
Episode length: 240.00 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 44.5     |
| time/              |          |
|    total_timesteps | 15000    |
| train/             |          |
|    actor_loss      | 1.17     |
|    critic_loss     | 1.81     |
|    learning_rate   | 0.000985 |
|    n_updates       | 12595    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -11      |
| time/              |          |
|    episodes        | 64       |
|    fps             | 24       |
|    time_elapsed    | 634      |
|    total timesteps | 15858    |
| train/             |          |
|    actor_loss      | 1.23     |
|    critic_loss     | 1.9      |
|    learning_rate   | 0.000985 |
|    n_updates       | 13348    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 24       |
|    time_elapsed    | 676      |
|    total timesteps | 16862    |
| train/             |          |
|    actor_loss      | 1.26     |
|    critic_loss     | 1.78     |
|    learning_rate   | 0.000984 |
|    n_updates       | 14352    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -9.64    |
| time/              |          |
|    episodes        | 72       |
|    fps             | 24       |
|    time_elapsed    | 720      |
|    total timesteps | 17866    |
| train/             |          |
|    actor_loss      | 1.32     |
|    critic_loss     | 1.69     |
|    learning_rate   | 0.000983 |
|    n_updates       | 15356    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -10.8    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 24       |
|    time_elapsed    | 763      |
|    total timesteps | 18870    |
| train/             |          |
|    actor_loss      | 1.36     |
|    critic_loss     | 2.09     |
|    learning_rate   | 0.000982 |
|    n_updates       | 16360    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -11      |
| time/              |          |
|    episodes        | 80       |
|    fps             | 24       |
|    time_elapsed    | 805      |
|    total timesteps | 19874    |
| train/             |          |
|    actor_loss      | 1.42     |
|    critic_loss     | 1.8      |
|    learning_rate   | 0.000981 |
|    n_updates       | 17364    |
---------------------------------
Eval num_timesteps=20000, episode_reward=33.59 +/- 17.90
Episode length: 240.20 +/- 21.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 33.6     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 1.41     |
|    critic_loss     | 1.42     |
|    learning_rate   | 0.00098  |
|    n_updates       | 17615    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -9.81    |
| time/              |          |
|    episodes        | 84       |
|    fps             | 24       |
|    time_elapsed    | 860      |
|    total timesteps | 20870    |
| train/             |          |
|    actor_loss      | 1.46     |
|    critic_loss     | 1.88     |
|    learning_rate   | 0.00098  |
|    n_updates       | 18368    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 24       |
|    time_elapsed    | 904      |
|    total timesteps | 21874    |
| train/             |          |
|    actor_loss      | 1.47     |
|    critic_loss     | 1.47     |
|    learning_rate   | 0.000979 |
|    n_updates       | 19364    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 24       |
|    time_elapsed    | 947      |
|    total timesteps | 22878    |
| train/             |          |
|    actor_loss      | 1.49     |
|    critic_loss     | 1.36     |
|    learning_rate   | 0.000978 |
|    n_updates       | 20368    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -10.4    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 24       |
|    time_elapsed    | 991      |
|    total timesteps | 23882    |
| train/             |          |
|    actor_loss      | 1.58     |
|    critic_loss     | 1.71     |
|    learning_rate   | 0.000977 |
|    n_updates       | 21372    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -10.1    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 24       |
|    time_elapsed    | 1035     |
|    total timesteps | 24886    |
| train/             |          |
|    actor_loss      | 1.62     |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.000976 |
|    n_updates       | 22376    |
---------------------------------
Eval num_timesteps=25000, episode_reward=41.53 +/- 16.17
Episode length: 240.80 +/- 20.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 41.5     |
| time/              |          |
|    total_timesteps | 25000    |
| train/             |          |
|    actor_loss      | 1.64     |
|    critic_loss     | 1.86     |
|    learning_rate   | 0.000975 |
|    n_updates       | 22627    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -9.59    |
| time/              |          |
|    episodes        | 104      |
|    fps             | 23       |
|    time_elapsed    | 1090     |
|    total timesteps | 25876    |
| train/             |          |
|    actor_loss      | 1.66     |
|    critic_loss     | 1.8      |
|    learning_rate   | 0.000975 |
|    n_updates       | 23366    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -9.92    |
| time/              |          |
|    episodes        | 108      |
|    fps             | 23       |
|    time_elapsed    | 1133     |
|    total timesteps | 26880    |
| train/             |          |
|    actor_loss      | 1.73     |
|    critic_loss     | 1.76     |
|    learning_rate   | 0.000974 |
|    n_updates       | 24370    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -8.38    |
| time/              |          |
|    episodes        | 112      |
|    fps             | 23       |
|    time_elapsed    | 1177     |
|    total timesteps | 27884    |
| train/             |          |
|    actor_loss      | 1.74     |
|    critic_loss     | 1.87     |
|    learning_rate   | 0.000973 |
|    n_updates       | 25374    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -8.7     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 23       |
|    time_elapsed    | 1219     |
|    total timesteps | 28888    |
| train/             |          |
|    actor_loss      | 1.75     |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.000972 |
|    n_updates       | 26378    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -8.5     |
| time/              |          |
|    episodes        | 120      |
|    fps             | 23       |
|    time_elapsed    | 1262     |
|    total timesteps | 29851    |
| train/             |          |
|    actor_loss      | 1.79     |
|    critic_loss     | 1.93     |
|    learning_rate   | 0.000971 |
|    n_updates       | 27382    |
---------------------------------
Eval num_timesteps=30000, episode_reward=50.09 +/- 23.30
Episode length: 250.60 +/- 0.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 50.1     |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 1.8      |
|    critic_loss     | 2        |
|    learning_rate   | 0.00097  |
|    n_updates       | 27592    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -8.58    |
| time/              |          |
|    episodes        | 124      |
|    fps             | 23       |
|    time_elapsed    | 1316     |
|    total timesteps | 30855    |
| train/             |          |
|    actor_loss      | 1.79     |
|    critic_loss     | 1.75     |
|    learning_rate   | 0.00097  |
|    n_updates       | 28345    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -8.92    |
| time/              |          |
|    episodes        | 128      |
|    fps             | 23       |
|    time_elapsed    | 1358     |
|    total timesteps | 31859    |
| train/             |          |
|    actor_loss      | 1.81     |
|    critic_loss     | 1.49     |
|    learning_rate   | 0.000969 |
|    n_updates       | 29349    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -8.8     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 23       |
|    time_elapsed    | 1401     |
|    total timesteps | 32863    |
| train/             |          |
|    actor_loss      | 1.87     |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.000968 |
|    n_updates       | 30353    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -8.93    |
| time/              |          |
|    episodes        | 136      |
|    fps             | 23       |
|    time_elapsed    | 1444     |
|    total timesteps | 33867    |
| train/             |          |
|    actor_loss      | 1.91     |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.000967 |
|    n_updates       | 31357    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -9.18    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 23       |
|    time_elapsed    | 1488     |
|    total timesteps | 34871    |
| train/             |          |
|    actor_loss      | 1.92     |
|    critic_loss     | 1.88     |
|    learning_rate   | 0.000966 |
|    n_updates       | 32361    |
---------------------------------
Eval num_timesteps=35000, episode_reward=38.12 +/- 16.32
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 38.1     |
| time/              |          |
|    total_timesteps | 35000    |
| train/             |          |
|    actor_loss      | 1.94     |
|    critic_loss     | 1.6      |
|    learning_rate   | 0.000965 |
|    n_updates       | 32612    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -8.53    |
| time/              |          |
|    episodes        | 144      |
|    fps             | 23       |
|    time_elapsed    | 1538     |
|    total timesteps | 35771    |
| train/             |          |
|    actor_loss      | 1.96     |
|    critic_loss     | 1.61     |
|    learning_rate   | 0.000965 |
|    n_updates       | 33261    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -8.41    |
| time/              |          |
|    episodes        | 148      |
|    fps             | 23       |
|    time_elapsed    | 1581     |
|    total timesteps | 36775    |
| train/             |          |
|    actor_loss      | 1.98     |
|    critic_loss     | 1.48     |
|    learning_rate   | 0.000964 |
|    n_updates       | 34265    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -9.02    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 23       |
|    time_elapsed    | 1625     |
|    total timesteps | 37779    |
| train/             |          |
|    actor_loss      | 2.03     |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.000963 |
|    n_updates       | 35269    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -8.75    |
| time/              |          |
|    episodes        | 156      |
|    fps             | 23       |
|    time_elapsed    | 1669     |
|    total timesteps | 38783    |
| train/             |          |
|    actor_loss      | 2.06     |
|    critic_loss     | 1.67     |
|    learning_rate   | 0.000962 |
|    n_updates       | 36273    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -9.13    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 23       |
|    time_elapsed    | 1709     |
|    total timesteps | 39703    |
| train/             |          |
|    actor_loss      | 2.09     |
|    critic_loss     | 1.84     |
|    learning_rate   | 0.000961 |
|    n_updates       | 37193    |
---------------------------------
Eval num_timesteps=40000, episode_reward=21.01 +/- 11.45
Episode length: 240.20 +/- 21.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 21       |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 2.11     |
|    critic_loss     | 1.92     |
|    learning_rate   | 0.00096  |
|    n_updates       | 37695    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -8.23    |
| time/              |          |
|    episodes        | 164      |
|    fps             | 23       |
|    time_elapsed    | 1764     |
|    total timesteps | 40707    |
| train/             |          |
|    actor_loss      | 2.09     |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.00096  |
|    n_updates       | 38197    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -8.25    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 23       |
|    time_elapsed    | 1806     |
|    total timesteps | 41711    |
| train/             |          |
|    actor_loss      | 2.13     |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.000959 |
|    n_updates       | 39201    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -9.74    |
| time/              |          |
|    episodes        | 172      |
|    fps             | 23       |
|    time_elapsed    | 1849     |
|    total timesteps | 42715    |
| train/             |          |
|    actor_loss      | 2.19     |
|    critic_loss     | 2.14     |
|    learning_rate   | 0.000958 |
|    n_updates       | 40205    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -9.56    |
| time/              |          |
|    episodes        | 176      |
|    fps             | 23       |
|    time_elapsed    | 1892     |
|    total timesteps | 43719    |
| train/             |          |
|    actor_loss      | 2.19     |
|    critic_loss     | 1.69     |
|    learning_rate   | 0.000957 |
|    n_updates       | 41209    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -9.13    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 23       |
|    time_elapsed    | 1935     |
|    total timesteps | 44723    |
| train/             |          |
|    actor_loss      | 2.23     |
|    critic_loss     | 1.59     |
|    learning_rate   | 0.000956 |
|    n_updates       | 42213    |
---------------------------------
Eval num_timesteps=45000, episode_reward=43.19 +/- 17.82
Episode length: 241.40 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 43.2     |
| time/              |          |
|    total_timesteps | 45000    |
| train/             |          |
|    actor_loss      | 2.24     |
|    critic_loss     | 1.8      |
|    learning_rate   | 0.000955 |
|    n_updates       | 42715    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 22       |
|    time_elapsed    | 1990     |
|    total timesteps | 45727    |
| train/             |          |
|    actor_loss      | 2.27     |
|    critic_loss     | 2.07     |
|    learning_rate   | 0.000955 |
|    n_updates       | 43217    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -9.93    |
| time/              |          |
|    episodes        | 188      |
|    fps             | 22       |
|    time_elapsed    | 2027     |
|    total timesteps | 46604    |
| train/             |          |
|    actor_loss      | 2.25     |
|    critic_loss     | 1.54     |
|    learning_rate   | 0.000954 |
|    n_updates       | 44094    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -9.85    |
| time/              |          |
|    episodes        | 192      |
|    fps             | 23       |
|    time_elapsed    | 2069     |
|    total timesteps | 47608    |
| train/             |          |
|    actor_loss      | 2.25     |
|    critic_loss     | 1.45     |
|    learning_rate   | 0.000953 |
|    n_updates       | 45098    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -9.92    |
| time/              |          |
|    episodes        | 196      |
|    fps             | 23       |
|    time_elapsed    | 2113     |
|    total timesteps | 48612    |
| train/             |          |
|    actor_loss      | 2.29     |
|    critic_loss     | 2.07     |
|    learning_rate   | 0.000952 |
|    n_updates       | 46102    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -9.97    |
| time/              |          |
|    episodes        | 200      |
|    fps             | 22       |
|    time_elapsed    | 2157     |
|    total timesteps | 49616    |
| train/             |          |
|    actor_loss      | 2.31     |
|    critic_loss     | 2.09     |
|    learning_rate   | 0.000951 |
|    n_updates       | 47106    |
---------------------------------
Eval num_timesteps=50000, episode_reward=41.28 +/- 16.44
Episode length: 231.60 +/- 24.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 41.3     |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 2.31     |
|    critic_loss     | 2.05     |
|    learning_rate   | 0.000951 |
|    n_updates       | 47608    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -9.14    |
| time/              |          |
|    episodes        | 204      |
|    fps             | 22       |
|    time_elapsed    | 2209     |
|    total timesteps | 50575    |
| train/             |          |
|    actor_loss      | 2.33     |
|    critic_loss     | 2        |
|    learning_rate   | 0.00095  |
|    n_updates       | 48065    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -9.33    |
| time/              |          |
|    episodes        | 208      |
|    fps             | 22       |
|    time_elapsed    | 2252     |
|    total timesteps | 51579    |
| train/             |          |
|    actor_loss      | 2.31     |
|    critic_loss     | 1.6      |
|    learning_rate   | 0.000949 |
|    n_updates       | 49069    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -9.98    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 22       |
|    time_elapsed    | 2295     |
|    total timesteps | 52583    |
| train/             |          |
|    actor_loss      | 2.3      |
|    critic_loss     | 1.56     |
|    learning_rate   | 0.000948 |
|    n_updates       | 50073    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -9.91    |
| time/              |          |
|    episodes        | 216      |
|    fps             | 22       |
|    time_elapsed    | 2338     |
|    total timesteps | 53587    |
| train/             |          |
|    actor_loss      | 2.35     |
|    critic_loss     | 1.9      |
|    learning_rate   | 0.000947 |
|    n_updates       | 51077    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 22       |
|    time_elapsed    | 2382     |
|    total timesteps | 54591    |
| train/             |          |
|    actor_loss      | 2.38     |
|    critic_loss     | 2.11     |
|    learning_rate   | 0.000946 |
|    n_updates       | 52081    |
---------------------------------
Eval num_timesteps=55000, episode_reward=39.82 +/- 14.51
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 39.8     |
| time/              |          |
|    total_timesteps | 55000    |
| train/             |          |
|    actor_loss      | 2.38     |
|    critic_loss     | 2.01     |
|    learning_rate   | 0.000946 |
|    n_updates       | 52583    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -11      |
| time/              |          |
|    episodes        | 224      |
|    fps             | 22       |
|    time_elapsed    | 2437     |
|    total timesteps | 55595    |
| train/             |          |
|    actor_loss      | 2.41     |
|    critic_loss     | 2.18     |
|    learning_rate   | 0.000945 |
|    n_updates       | 53085    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -9.22    |
| time/              |          |
|    episodes        | 228      |
|    fps             | 22       |
|    time_elapsed    | 2478     |
|    total timesteps | 56535    |
| train/             |          |
|    actor_loss      | 2.41     |
|    critic_loss     | 2.01     |
|    learning_rate   | 0.000944 |
|    n_updates       | 54025    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -9.61    |
| time/              |          |
|    episodes        | 232      |
|    fps             | 22       |
|    time_elapsed    | 2521     |
|    total timesteps | 57539    |
| train/             |          |
|    actor_loss      | 2.39     |
|    critic_loss     | 1.8      |
|    learning_rate   | 0.000943 |
|    n_updates       | 55029    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | -9.79    |
| time/              |          |
|    episodes        | 236      |
|    fps             | 22       |
|    time_elapsed    | 2563     |
|    total timesteps | 58498    |
| train/             |          |
|    actor_loss      | 2.42     |
|    critic_loss     | 1.61     |
|    learning_rate   | 0.000942 |
|    n_updates       | 55988    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | -10.8    |
| time/              |          |
|    episodes        | 240      |
|    fps             | 22       |
|    time_elapsed    | 2606     |
|    total timesteps | 59502    |
| train/             |          |
|    actor_loss      | 2.43     |
|    critic_loss     | 2.23     |
|    learning_rate   | 0.000941 |
|    n_updates       | 56992    |
---------------------------------
Eval num_timesteps=60000, episode_reward=36.53 +/- 20.64
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 36.5     |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 2.44     |
|    critic_loss     | 1.65     |
|    learning_rate   | 0.000941 |
|    n_updates       | 57494    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    episodes        | 244      |
|    fps             | 22       |
|    time_elapsed    | 2663     |
|    total timesteps | 60506    |
| train/             |          |
|    actor_loss      | 2.46     |
|    critic_loss     | 2.02     |
|    learning_rate   | 0.00094  |
|    n_updates       | 57996    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -10.5    |
| time/              |          |
|    episodes        | 248      |
|    fps             | 22       |
|    time_elapsed    | 2703     |
|    total timesteps | 61431    |
| train/             |          |
|    actor_loss      | 2.5      |
|    critic_loss     | 2.16     |
|    learning_rate   | 0.000939 |
|    n_updates       | 58921    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | -9.36    |
| time/              |          |
|    episodes        | 252      |
|    fps             | 22       |
|    time_elapsed    | 2747     |
|    total timesteps | 62381    |
| train/             |          |
|    actor_loss      | 2.5      |
|    critic_loss     | 1.93     |
|    learning_rate   | 0.000938 |
|    n_updates       | 59925    |
---------------------------------
