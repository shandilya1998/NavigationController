running build_ext
building 'mujoco_py.cymj' extension
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o -fopenmp -w
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py
gcc -pthread -shared /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mjpro150/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mjpro150/bin -lmujoco150 -lglewegl -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp
2021-12-10 12:44:46.309827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-10 12:44:46.309897: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp12/TD3_6
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -60.8    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 36       |
|    time_elapsed    | 217      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 1.3      |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 3003     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-7.53 +/- 28.57
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -7.53    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.38     |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.0005   |
|    n_updates       | 4999     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -16.4    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 20       |
|    time_elapsed    | 766      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 1.36     |
|    critic_loss     | 0.453    |
|    learning_rate   | 0.0005   |
|    n_updates       | 11007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=12.79 +/- 26.09
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 1.09     |
|    critic_loss     | 0.791    |
|    learning_rate   | 0.0005   |
|    n_updates       | 14999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.03    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 18       |
|    time_elapsed    | 1319     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 1.06     |
|    critic_loss     | 0.817    |
|    learning_rate   | 0.0005   |
|    n_updates       | 19011    |
---------------------------------
Eval num_timesteps=30000, episode_reward=10.64 +/- 19.60
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 0.754    |
|    critic_loss     | 2.36     |
|    learning_rate   | 0.0005   |
|    n_updates       | 24999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -12.8    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 17       |
|    time_elapsed    | 1869     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 0.702    |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.0005   |
|    n_updates       | 27015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=39.09 +/- 8.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 39.1     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.61     |
|    critic_loss     | 1.24     |
|    learning_rate   | 0.0005   |
|    n_updates       | 34999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.174   |
| time/              |          |
|    episodes        | 20       |
|    fps             | 16       |
|    time_elapsed    | 2418     |
|    total timesteps | 40020    |
| train/             |          |
|    actor_loss      | 0.595    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 35019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 16       |
|    time_elapsed    | 2853     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.444    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 43023    |
---------------------------------
Eval num_timesteps=50000, episode_reward=4.66 +/- 21.49
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 4.66     |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.434    |
|    critic_loss     | 0.0914   |
|    learning_rate   | 0.0005   |
|    n_updates       | 44999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 16       |
|    time_elapsed    | 3416     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 0.336    |
|    critic_loss     | 0.0682   |
|    learning_rate   | 0.0005   |
|    n_updates       | 51027    |
---------------------------------
Eval num_timesteps=60000, episode_reward=16.64 +/- 21.97
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.296    |
|    critic_loss     | 0.0819   |
|    learning_rate   | 0.0005   |
|    n_updates       | 54999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.51    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 16       |
|    time_elapsed    | 3963     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 0.177    |
|    critic_loss     | 1.14     |
|    learning_rate   | 0.0005   |
|    n_updates       | 59031    |
---------------------------------
Eval num_timesteps=70000, episode_reward=23.46 +/- 45.85
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 0.401    |
|    critic_loss     | 0.543    |
|    learning_rate   | 0.0005   |
|    n_updates       | 64999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.77    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 15       |
|    time_elapsed    | 4504     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 0.261    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 67035    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-12.04 +/- 46.12
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -12      |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.287    |
|    critic_loss     | 0.841    |
|    learning_rate   | 0.0005   |
|    n_updates       | 74999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.15    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 15       |
|    time_elapsed    | 5047     |
|    total timesteps | 80040    |
| train/             |          |
|    actor_loss      | 0.0988   |
|    critic_loss     | 1.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 75039    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.57    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 16       |
|    time_elapsed    | 5477     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 0.313    |
|    critic_loss     | 0.0833   |
|    learning_rate   | 0.0005   |
|    n_updates       | 83043    |
---------------------------------
Eval num_timesteps=90000, episode_reward=12.72 +/- 17.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 0.0842   |
|    critic_loss     | 0.848    |
|    learning_rate   | 0.0005   |
|    n_updates       | 84999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.43    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 15       |
|    time_elapsed    | 6028     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 0.251    |
|    critic_loss     | 1.56     |
|    learning_rate   | 0.0005   |
|    n_updates       | 91047    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-14.20 +/- 25.45
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.2    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.259    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 94999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.95    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 15       |
|    time_elapsed    | 6567     |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | 0.156    |
|    critic_loss     | 0.432    |
|    learning_rate   | 0.0005   |
|    n_updates       | 99051    |
---------------------------------
Eval num_timesteps=110000, episode_reward=12.22 +/- 25.37
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -0.0837  |
|    critic_loss     | 0.979    |
|    learning_rate   | 0.0005   |
|    n_updates       | 104999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.12    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 15       |
|    time_elapsed    | 7107     |
|    total timesteps | 112056   |
| train/             |          |
|    actor_loss      | 0.353    |
|    critic_loss     | 0.822    |
|    learning_rate   | 0.0005   |
|    n_updates       | 107055   |
---------------------------------
Eval num_timesteps=120000, episode_reward=7.41 +/- 33.57
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 7.41     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 0.104    |
|    critic_loss     | 0.443    |
|    learning_rate   | 0.0005   |
|    n_updates       | 114999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.54    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 15       |
|    time_elapsed    | 7645     |
|    total timesteps | 120060   |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 115059   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.66    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 15       |
|    time_elapsed    | 8074     |
|    total timesteps | 128064   |
| train/             |          |
|    actor_loss      | 0.265    |
|    critic_loss     | 1.15     |
|    learning_rate   | 0.0005   |
|    n_updates       | 123063   |
---------------------------------
Eval num_timesteps=130000, episode_reward=5.56 +/- 26.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 5.56     |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 0.407    |
|    critic_loss     | 0.865    |
|    learning_rate   | 0.0005   |
|    n_updates       | 124999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.71    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 15       |
|    time_elapsed    | 8611     |
|    total timesteps | 136068   |
| train/             |          |
|    actor_loss      | 0.204    |
|    critic_loss     | 0.786    |
|    learning_rate   | 0.0005   |
|    n_updates       | 131067   |
---------------------------------
Eval num_timesteps=140000, episode_reward=12.87 +/- 29.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 0.294    |
|    critic_loss     | 0.425    |
|    learning_rate   | 0.0005   |
|    n_updates       | 134999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.1     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 15       |
|    time_elapsed    | 9150     |
|    total timesteps | 144072   |
| train/             |          |
|    actor_loss      | 0.304    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 139071   |
---------------------------------
Eval num_timesteps=150000, episode_reward=11.68 +/- 45.69
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 0.182    |
|    critic_loss     | 0.817    |
|    learning_rate   | 0.0005   |
|    n_updates       | 144999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.23    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 15       |
|    time_elapsed    | 9690     |
|    total timesteps | 152076   |
| train/             |          |
|    actor_loss      | 0.239    |
|    critic_loss     | 0.792    |
|    learning_rate   | 0.0005   |
|    n_updates       | 147075   |
---------------------------------
Eval num_timesteps=160000, episode_reward=41.53 +/- 9.87
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 41.5     |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 0.168    |
|    critic_loss     | 2.81     |
|    learning_rate   | 0.0005   |
|    n_updates       | 154999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.24    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 15       |
|    time_elapsed    | 10236    |
|    total timesteps | 160080   |
| train/             |          |
|    actor_loss      | 0.135    |
|    critic_loss     | 0.0318   |
|    learning_rate   | 0.0005   |
|    n_updates       | 155079   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 84       |
|    fps             | 15       |
|    time_elapsed    | 10668    |
|    total timesteps | 168084   |
| train/             |          |
|    actor_loss      | 0.122    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 163083   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-1.32 +/- 16.36
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.32    |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 0.203    |
|    critic_loss     | 1.19     |
|    learning_rate   | 0.0005   |
|    n_updates       | 164999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.05    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 15       |
|    time_elapsed    | 11213    |
|    total timesteps | 176088   |
| train/             |          |
|    actor_loss      | 0.0774   |
|    critic_loss     | 0.83     |
|    learning_rate   | 0.0005   |
|    n_updates       | 171087   |
---------------------------------
Eval num_timesteps=180000, episode_reward=1.79 +/- 39.62
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 1.79     |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 0.262    |
|    critic_loss     | 0.832    |
|    learning_rate   | 0.0005   |
|    n_updates       | 174999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.96    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 15       |
|    time_elapsed    | 11755    |
|    total timesteps | 184092   |
| train/             |          |
|    actor_loss      | 0.124    |
|    critic_loss     | 0.419    |
|    learning_rate   | 0.0005   |
|    n_updates       | 179091   |
---------------------------------
Eval num_timesteps=190000, episode_reward=18.36 +/- 32.43
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.0005   |
|    n_updates       | 184999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.87    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 15       |
|    time_elapsed    | 12299    |
|    total timesteps | 192096   |
| train/             |          |
|    actor_loss      | 0.177    |
|    critic_loss     | 0.0267   |
|    learning_rate   | 0.0005   |
|    n_updates       | 187095   |
---------------------------------
Eval num_timesteps=200000, episode_reward=2.23 +/- 21.25
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.23     |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 0.154    |
|    critic_loss     | 0.368    |
|    learning_rate   | 0.0005   |
|    n_updates       | 194999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.58    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 15       |
|    time_elapsed    | 12847    |
|    total timesteps | 200100   |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 0.819    |
|    learning_rate   | 0.0005   |
|    n_updates       | 195099   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.98    |
| time/              |          |
|    episodes        | 104      |
|    fps             | 15       |
|    time_elapsed    | 13277    |
|    total timesteps | 208104   |
| train/             |          |
|    actor_loss      | 0.161    |
|    critic_loss     | 1.92     |
|    learning_rate   | 0.0005   |
|    n_updates       | 203103   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-3.41 +/- 14.25
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -3.41    |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | 0.0874   |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.0005   |
|    n_updates       | 204999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.22    |
| time/              |          |
|    episodes        | 108      |
|    fps             | 15       |
|    time_elapsed    | 13817    |
|    total timesteps | 216108   |
| train/             |          |
|    actor_loss      | 0.305    |
|    critic_loss     | 1.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 211107   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-10.53 +/- 26.86
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.5    |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 0.228    |
|    critic_loss     | 0.421    |
|    learning_rate   | 0.0005   |
|    n_updates       | 214999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.09    |
| time/              |          |
|    episodes        | 112      |
|    fps             | 15       |
|    time_elapsed    | 14365    |
|    total timesteps | 224112   |
| train/             |          |
|    actor_loss      | 0.281    |
|    critic_loss     | 0.81     |
|    learning_rate   | 0.0005   |
|    n_updates       | 219111   |
---------------------------------
Eval num_timesteps=230000, episode_reward=-3.04 +/- 34.71
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -3.04    |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 0.182    |
|    critic_loss     | 0.418    |
|    learning_rate   | 0.0005   |
|    n_updates       | 224999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.71    |
| time/              |          |
|    episodes        | 116      |
|    fps             | 15       |
|    time_elapsed    | 14915    |
|    total timesteps | 232116   |
| train/             |          |
|    actor_loss      | 0.117    |
|    critic_loss     | 1.68     |
|    learning_rate   | 0.0005   |
|    n_updates       | 227115   |
---------------------------------
Eval num_timesteps=240000, episode_reward=27.70 +/- 48.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 27.7     |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.211    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 234999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5       |
| time/              |          |
|    episodes        | 120      |
|    fps             | 15       |
|    time_elapsed    | 15460    |
|    total timesteps | 240120   |
| train/             |          |
|    actor_loss      | 0.23     |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 235119   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.26    |
| time/              |          |
|    episodes        | 124      |
|    fps             | 15       |
|    time_elapsed    | 15889    |
|    total timesteps | 248124   |
| train/             |          |
|    actor_loss      | 0.405    |
|    critic_loss     | 0.502    |
|    learning_rate   | 0.0005   |
|    n_updates       | 243123   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-8.25 +/- 18.65
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -8.25    |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | 0.245    |
|    critic_loss     | 1.16     |
|    learning_rate   | 0.0005   |
|    n_updates       | 244999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.63    |
| time/              |          |
|    episodes        | 128      |
|    fps             | 15       |
|    time_elapsed    | 16431    |
|    total timesteps | 256128   |
| train/             |          |
|    actor_loss      | 0.173    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 251127   |
---------------------------------
Eval num_timesteps=260000, episode_reward=11.06 +/- 37.20
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 0.0595   |
|    critic_loss     | 0.0595   |
|    learning_rate   | 0.0005   |
|    n_updates       | 254999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.29    |
| time/              |          |
|    episodes        | 132      |
|    fps             | 15       |
|    time_elapsed    | 16977    |
|    total timesteps | 264132   |
| train/             |          |
|    actor_loss      | 0.355    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 259131   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-5.53 +/- 23.55
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -5.53    |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | 0.158    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 264999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.33    |
| time/              |          |
|    episodes        | 136      |
|    fps             | 15       |
|    time_elapsed    | 17517    |
|    total timesteps | 272136   |
| train/             |          |
|    actor_loss      | 0.273    |
|    critic_loss     | 2.74     |
|    learning_rate   | 0.0005   |
|    n_updates       | 267135   |
---------------------------------
Eval num_timesteps=280000, episode_reward=24.80 +/- 28.97
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 24.8     |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 0.213    |
|    critic_loss     | 0.422    |
|    learning_rate   | 0.0005   |
|    n_updates       | 274999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.58    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 15       |
|    time_elapsed    | 18054    |
|    total timesteps | 280140   |
| train/             |          |
|    actor_loss      | 0.0186   |
|    critic_loss     | 0.059    |
|    learning_rate   | 0.0005   |
|    n_updates       | 275139   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.25    |
| time/              |          |
|    episodes        | 144      |
|    fps             | 15       |
|    time_elapsed    | 18481    |
|    total timesteps | 288144   |
| train/             |          |
|    actor_loss      | 0.216    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 283143   |
---------------------------------
Eval num_timesteps=290000, episode_reward=15.64 +/- 19.14
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | 0.288    |
|    critic_loss     | 1.19     |
|    learning_rate   | 0.0005   |
|    n_updates       | 284999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.52    |
| time/              |          |
|    episodes        | 148      |
|    fps             | 15       |
|    time_elapsed    | 19019    |
|    total timesteps | 296148   |
| train/             |          |
|    actor_loss      | 0.125    |
|    critic_loss     | 0.434    |
|    learning_rate   | 0.0005   |
|    n_updates       | 291147   |
---------------------------------
Eval num_timesteps=300000, episode_reward=2.52 +/- 13.41
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 0.176    |
|    critic_loss     | 0.809    |
|    learning_rate   | 0.0005   |
|    n_updates       | 294999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.74    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 15       |
|    time_elapsed    | 19564    |
|    total timesteps | 304152   |
| train/             |          |
|    actor_loss      | 0.338    |
|    critic_loss     | 1.62     |
|    learning_rate   | 0.0005   |
|    n_updates       | 299151   |
---------------------------------
Eval num_timesteps=310000, episode_reward=11.91 +/- 18.83
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | 0.254    |
|    critic_loss     | 0.807    |
|    learning_rate   | 0.0005   |
|    n_updates       | 304999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.18    |
| time/              |          |
|    episodes        | 156      |
|    fps             | 15       |
|    time_elapsed    | 20110    |
|    total timesteps | 312156   |
| train/             |          |
|    actor_loss      | 0.386    |
|    critic_loss     | 0.505    |
|    learning_rate   | 0.0005   |
|    n_updates       | 307155   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-2.69 +/- 21.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.69    |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 0.251    |
|    critic_loss     | 1.18     |
|    learning_rate   | 0.0005   |
|    n_updates       | 314999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.12    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 15       |
|    time_elapsed    | 20655    |
|    total timesteps | 320160   |
| train/             |          |
|    actor_loss      | 0.226    |
|    critic_loss     | 0.416    |
|    learning_rate   | 0.0005   |
|    n_updates       | 315159   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.6     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 15       |
|    time_elapsed    | 21085    |
|    total timesteps | 328164   |
| train/             |          |
|    actor_loss      | 0.165    |
|    critic_loss     | 0.775    |
|    learning_rate   | 0.0005   |
|    n_updates       | 323163   |
---------------------------------
Eval num_timesteps=330000, episode_reward=2.94 +/- 48.37
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | 0.319    |
|    critic_loss     | 2.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 324999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.45    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 15       |
|    time_elapsed    | 21627    |
|    total timesteps | 336168   |
| train/             |          |
|    actor_loss      | 0.293    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 331167   |
---------------------------------
Eval num_timesteps=340000, episode_reward=21.44 +/- 11.68
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | 0.173    |
|    critic_loss     | 1.98     |
|    learning_rate   | 0.0005   |
|    n_updates       | 334999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.29    |
| time/              |          |
|    episodes        | 172      |
|    fps             | 15       |
|    time_elapsed    | 22163    |
|    total timesteps | 344013   |
| train/             |          |
|    actor_loss      | 0.253    |
|    critic_loss     | 0.472    |
|    learning_rate   | 0.0005   |
|    n_updates       | 339012   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-12.57 +/- 7.20
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -12.6    |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | 0.155    |
|    critic_loss     | 0.417    |
|    learning_rate   | 0.0005   |
|    n_updates       | 344999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -8.84    |
| time/              |          |
|    episodes        | 176      |
|    fps             | 15       |
|    time_elapsed    | 22638    |
|    total timesteps | 350790   |
| train/             |          |
|    actor_loss      | 0.253    |
|    critic_loss     | 1.17     |
|    learning_rate   | 0.0005   |
|    n_updates       | 345789   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -9.39    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 15       |
|    time_elapsed    | 23070    |
|    total timesteps | 358794   |
| train/             |          |
|    actor_loss      | 0.316    |
|    critic_loss     | 0.409    |
|    learning_rate   | 0.0005   |
|    n_updates       | 353793   |
---------------------------------
Eval num_timesteps=360000, episode_reward=4.75 +/- 26.92
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 4.75     |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | 0.17     |
|    critic_loss     | 0.854    |
|    learning_rate   | 0.0005   |
|    n_updates       | 354999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -12.1    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 15       |
|    time_elapsed    | 23611    |
|    total timesteps | 366798   |
| train/             |          |
|    actor_loss      | 0.232    |
|    critic_loss     | 0.833    |
|    learning_rate   | 0.0005   |
|    n_updates       | 361797   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-14.27 +/- 19.63
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.3    |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | 0.301    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 364999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -13.7    |
| time/              |          |
|    episodes        | 188      |
|    fps             | 15       |
|    time_elapsed    | 24152    |
|    total timesteps | 374802   |
| train/             |          |
|    actor_loss      | 0.308    |
|    critic_loss     | 0.0458   |
|    learning_rate   | 0.0005   |
|    n_updates       | 369801   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-17.33 +/- 27.10
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -17.3    |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | 0.318    |
|    critic_loss     | 1.96     |
|    learning_rate   | 0.0005   |
|    n_updates       | 374999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    episodes        | 192      |
|    fps             | 15       |
|    time_elapsed    | 24699    |
|    total timesteps | 382806   |
| train/             |          |
|    actor_loss      | 0.256    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 377805   |
---------------------------------
Eval num_timesteps=390000, episode_reward=-25.28 +/- 43.91
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -25.3    |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | 0.289    |
|    critic_loss     | 0.422    |
|    learning_rate   | 0.0005   |
|    n_updates       | 384999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -11      |
| time/              |          |
|    episodes        | 196      |
|    fps             | 15       |
|    time_elapsed    | 25254    |
|    total timesteps | 390810   |
| train/             |          |
|    actor_loss      | 0.282    |
|    critic_loss     | 0.0353   |
|    learning_rate   | 0.0005   |
|    n_updates       | 385809   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    episodes        | 200      |
|    fps             | 15       |
|    time_elapsed    | 25683    |
|    total timesteps | 398814   |
| train/             |          |
|    actor_loss      | 0.138    |
|    critic_loss     | 0.465    |
|    learning_rate   | 0.0005   |
|    n_updates       | 393813   |
---------------------------------
Eval num_timesteps=400000, episode_reward=21.54 +/- 17.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 21.5     |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | 0.314    |
|    critic_loss     | 0.446    |
|    learning_rate   | 0.0005   |
|    n_updates       | 394999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    episodes        | 204      |
|    fps             | 15       |
|    time_elapsed    | 26222    |
|    total timesteps | 406818   |
| train/             |          |
|    actor_loss      | 0.34     |
|    critic_loss     | 1.18     |
|    learning_rate   | 0.0005   |
|    n_updates       | 401817   |
---------------------------------
Eval num_timesteps=410000, episode_reward=4.81 +/- 16.76
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 4.81     |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | 0.301    |
|    critic_loss     | 1.16     |
|    learning_rate   | 0.0005   |
|    n_updates       | 404999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    episodes        | 208      |
|    fps             | 15       |
|    time_elapsed    | 26772    |
|    total timesteps | 414822   |
| train/             |          |
|    actor_loss      | 0.255    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 409821   |
---------------------------------
Eval num_timesteps=420000, episode_reward=5.04 +/- 20.54
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 5.04     |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | 0.349    |
|    critic_loss     | 0.427    |
|    learning_rate   | 0.0005   |
|    n_updates       | 414999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 15       |
|    time_elapsed    | 27313    |
|    total timesteps | 422826   |
| train/             |          |
|    actor_loss      | 0.264    |
|    critic_loss     | 0.406    |
|    learning_rate   | 0.0005   |
|    n_updates       | 417825   |
---------------------------------
Eval num_timesteps=430000, episode_reward=5.13 +/- 27.14
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 5.13     |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | 0.163    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 424999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -8.79    |
| time/              |          |
|    episodes        | 216      |
|    fps             | 15       |
|    time_elapsed    | 27855    |
|    total timesteps | 430830   |
| train/             |          |
|    actor_loss      | 0.343    |
|    critic_loss     | 1.15     |
|    learning_rate   | 0.0005   |
|    n_updates       | 425829   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 15       |
|    time_elapsed    | 28283    |
|    total timesteps | 438834   |
| train/             |          |
|    actor_loss      | 0.393    |
|    critic_loss     | 0.0812   |
|    learning_rate   | 0.0005   |
|    n_updates       | 433833   |
---------------------------------
Eval num_timesteps=440000, episode_reward=33.16 +/- 30.72
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 33.2     |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | 0.296    |
|    critic_loss     | 1.61     |
|    learning_rate   | 0.0005   |
|    n_updates       | 434999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    episodes        | 224      |
|    fps             | 15       |
|    time_elapsed    | 28827    |
|    total timesteps | 446838   |
| train/             |          |
|    actor_loss      | 0.378    |
|    critic_loss     | 0.844    |
|    learning_rate   | 0.0005   |
|    n_updates       | 441837   |
---------------------------------
Terminated
2021-12-10 20:48:30.280480: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-10 20:48:30.280542: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp12/TD3_7
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -98.7    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 47       |
|    time_elapsed    | 168      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 1.48     |
|    critic_loss     | 2.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 2001     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-26.38 +/- 29.78
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -26.4    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.64     |
|    critic_loss     | 2.1      |
|    learning_rate   | 0.0005   |
|    n_updates       | 4002     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -67.3    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 23       |
|    time_elapsed    | 690      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 1.49     |
|    critic_loss     | 1.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 10005    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-52.96 +/- 32.83
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -53      |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 1.39     |
|    critic_loss     | 1.71     |
|    learning_rate   | 0.0005   |
|    n_updates       | 14007    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -37.2    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 19       |
|    time_elapsed    | 1219     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 1.27     |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.0005   |
|    n_updates       | 18009    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-32.46 +/- 44.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -32.5    |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 1.07     |
|    critic_loss     | 1.4      |
|    learning_rate   | 0.0005   |
|    n_updates       | 24012    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -19      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 18       |
|    time_elapsed    | 1748     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 1        |
|    critic_loss     | 1.31     |
|    learning_rate   | 0.0005   |
|    n_updates       | 26013    |
---------------------------------
Eval num_timesteps=40000, episode_reward=2.98 +/- 32.15
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.732    |
|    critic_loss     | 1.17     |
|    learning_rate   | 0.0005   |
|    n_updates       | 34017    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.13    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 17       |
|    time_elapsed    | 2278     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.22    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 17       |
|    time_elapsed    | 2692     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.522    |
|    critic_loss     | 1.09     |
|    learning_rate   | 0.0005   |
|    n_updates       | 42021    |
---------------------------------
Eval num_timesteps=50000, episode_reward=1.74 +/- 36.94
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.489    |
|    critic_loss     | 1.11     |
|    learning_rate   | 0.0005   |
|    n_updates       | 44022    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.3     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 17       |
|    time_elapsed    | 3215     |
|    total timesteps | 55914    |
| train/             |          |
|    actor_loss      | 0.392    |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.0005   |
|    n_updates       | 49911    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-43.01 +/- 30.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -43      |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.343    |
|    critic_loss     | 1.11     |
|    learning_rate   | 0.0005   |
|    n_updates       | 55914    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.96    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 17       |
|    time_elapsed    | 3745     |
|    total timesteps | 63918    |
| train/             |          |
|    actor_loss      | 0.335    |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.0005   |
|    n_updates       | 57915    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-10.24 +/- 29.63
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.2    |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 0.283    |
|    critic_loss     | 1.07     |
|    learning_rate   | 0.0005   |
|    n_updates       | 65743    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -4.71    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 16       |
|    time_elapsed    | 4264     |
|    total timesteps | 71746    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -0.948   |
| time/              |          |
|    episodes        | 40       |
|    fps             | 17       |
|    time_elapsed    | 4676     |
|    total timesteps | 79750    |
| train/             |          |
|    actor_loss      | 0.271    |
|    critic_loss     | 1.07     |
|    learning_rate   | 0.0005   |
|    n_updates       | 73747    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-43.43 +/- 33.51
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -43.4    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.263    |
|    critic_loss     | 1.05     |
|    learning_rate   | 0.0005   |
|    n_updates       | 75748    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.12    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 16       |
|    time_elapsed    | 5199     |
|    total timesteps | 87754    |
| train/             |          |
|    actor_loss      | 0.226    |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.0005   |
|    n_updates       | 81751    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-29.27 +/- 13.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29.3    |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 0.213    |
|    critic_loss     | 1.02     |
|    learning_rate   | 0.0005   |
|    n_updates       | 85753    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.58     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 16       |
|    time_elapsed    | 5720     |
|    total timesteps | 95758    |
| train/             |          |
|    actor_loss      | 0.185    |
|    critic_loss     | 0.984    |
|    learning_rate   | 0.0005   |
|    n_updates       | 89755    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-12.53 +/- 14.10
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -12.5    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.154    |
|    critic_loss     | 0.981    |
|    learning_rate   | 0.0005   |
|    n_updates       | 95758    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 16       |
|    time_elapsed    | 6248     |
|    total timesteps | 103762   |
| train/             |          |
|    actor_loss      | 0.146    |
|    critic_loss     | 0.997    |
|    learning_rate   | 0.0005   |
|    n_updates       | 97759    |
---------------------------------
Eval num_timesteps=110000, episode_reward=-23.67 +/- 11.69
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.7    |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 0.136    |
|    critic_loss     | 0.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 105763   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.31     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 16       |
|    time_elapsed    | 6770     |
|    total timesteps | 111766   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.46     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 16       |
|    time_elapsed    | 7164     |
|    total timesteps | 119402   |
| train/             |          |
|    actor_loss      | 0.115    |
|    critic_loss     | 0.984    |
|    learning_rate   | 0.0005   |
|    n_updates       | 113399   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-33.51 +/- 30.50
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -33.5    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 0.105    |
|    critic_loss     | 0.973    |
|    learning_rate   | 0.0005   |
|    n_updates       | 115400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.2      |
| time/              |          |
|    episodes        | 64       |
|    fps             | 16       |
|    time_elapsed    | 7689     |
|    total timesteps | 127406   |
| train/             |          |
|    actor_loss      | 0.0933   |
|    critic_loss     | 0.987    |
|    learning_rate   | 0.0005   |
|    n_updates       | 121403   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-13.17 +/- 38.38
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -13.2    |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 0.0832   |
|    critic_loss     | 0.959    |
|    learning_rate   | 0.0005   |
|    n_updates       | 125405   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.1      |
| time/              |          |
|    episodes        | 68       |
|    fps             | 16       |
|    time_elapsed    | 8216     |
|    total timesteps | 135410   |
| train/             |          |
|    actor_loss      | 0.0639   |
|    critic_loss     | 0.959    |
|    learning_rate   | 0.0005   |
|    n_updates       | 129407   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-23.83 +/- 21.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.8    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 0.0522   |
|    critic_loss     | 0.964    |
|    learning_rate   | 0.0005   |
|    n_updates       | 135410   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.75     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 16       |
|    time_elapsed    | 8744     |
|    total timesteps | 143414   |
| train/             |          |
|    actor_loss      | 0.0547   |
|    critic_loss     | 0.981    |
|    learning_rate   | 0.0005   |
|    n_updates       | 137411   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-44.46 +/- 27.99
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -44.5    |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 0.0584   |
|    critic_loss     | 0.974    |
|    learning_rate   | 0.0005   |
|    n_updates       | 145415   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.84     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 16       |
|    time_elapsed    | 9273     |
|    total timesteps | 151418   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 0.416    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 16       |
|    time_elapsed    | 9686     |
|    total timesteps | 159422   |
| train/             |          |
|    actor_loss      | 0.0884   |
|    critic_loss     | 0.96     |
|    learning_rate   | 0.0005   |
|    n_updates       | 153419   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-8.92 +/- 16.88
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -8.92    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 0.0977   |
|    critic_loss     | 0.989    |
|    learning_rate   | 0.0005   |
|    n_updates       | 155420   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.23     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 16       |
|    time_elapsed    | 10212    |
|    total timesteps | 167426   |
| train/             |          |
|    actor_loss      | 0.11     |
|    critic_loss     | 0.987    |
|    learning_rate   | 0.0005   |
|    n_updates       | 161423   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-28.36 +/- 9.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -28.4    |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 0.117    |
|    critic_loss     | 0.968    |
|    learning_rate   | 0.0005   |
|    n_updates       | 165425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 16       |
|    time_elapsed    | 10736    |
|    total timesteps | 175430   |
| train/             |          |
|    actor_loss      | 0.109    |
|    critic_loss     | 0.945    |
|    learning_rate   | 0.0005   |
|    n_updates       | 169427   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-47.40 +/- 32.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -47.4    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 0.121    |
|    critic_loss     | 0.971    |
|    learning_rate   | 0.0005   |
|    n_updates       | 175430   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.14     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 16       |
|    time_elapsed    | 11261    |
|    total timesteps | 183434   |
| train/             |          |
|    actor_loss      | 0.124    |
|    critic_loss     | 0.939    |
|    learning_rate   | 0.0005   |
|    n_updates       | 177431   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-39.76 +/- 29.93
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -39.8    |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 0.107    |
|    critic_loss     | 0.954    |
|    learning_rate   | 0.0005   |
|    n_updates       | 185435   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.4      |
| time/              |          |
|    episodes        | 96       |
|    fps             | 16       |
|    time_elapsed    | 11788    |
|    total timesteps | 191438   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.34     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 16       |
|    time_elapsed    | 12201    |
|    total timesteps | 199442   |
| train/             |          |
|    actor_loss      | 0.114    |
|    critic_loss     | 0.964    |
|    learning_rate   | 0.0005   |
|    n_updates       | 193439   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-42.47 +/- 29.25
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -42.5    |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 0.114    |
|    critic_loss     | 0.941    |
|    learning_rate   | 0.0005   |
|    n_updates       | 195440   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.28     |
| time/              |          |
|    episodes        | 104      |
|    fps             | 16       |
|    time_elapsed    | 12729    |
|    total timesteps | 207446   |
| train/             |          |
|    actor_loss      | 0.121    |
|    critic_loss     | 0.966    |
|    learning_rate   | 0.0005   |
|    n_updates       | 201443   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-66.04 +/- 48.88
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -66      |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | 0.124    |
|    critic_loss     | 0.983    |
|    learning_rate   | 0.0005   |
|    n_updates       | 205445   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.41     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 16       |
|    time_elapsed    | 13256    |
|    total timesteps | 215450   |
| train/             |          |
|    actor_loss      | 0.125    |
|    critic_loss     | 0.965    |
|    learning_rate   | 0.0005   |
|    n_updates       | 209447   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-19.07 +/- 18.77
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19.1    |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 0.123    |
|    critic_loss     | 0.953    |
|    learning_rate   | 0.0005   |
|    n_updates       | 215450   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.08     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 16       |
|    time_elapsed    | 13784    |
|    total timesteps | 223454   |
| train/             |          |
|    actor_loss      | 0.122    |
|    critic_loss     | 0.936    |
|    learning_rate   | 0.0005   |
|    n_updates       | 217451   |
---------------------------------
Eval num_timesteps=230000, episode_reward=-23.82 +/- 19.96
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.8    |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 0.133    |
|    critic_loss     | 0.961    |
|    learning_rate   | 0.0005   |
|    n_updates       | 225455   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 16       |
|    time_elapsed    | 14313    |
|    total timesteps | 231458   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -0.311   |
| time/              |          |
|    episodes        | 120      |
|    fps             | 16       |
|    time_elapsed    | 14729    |
|    total timesteps | 239462   |
| train/             |          |
|    actor_loss      | 0.157    |
|    critic_loss     | 0.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 233459   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-24.19 +/- 30.92
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -24.2    |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.169    |
|    critic_loss     | 0.974    |
|    learning_rate   | 0.0005   |
|    n_updates       | 235460   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -0.368   |
| time/              |          |
|    episodes        | 124      |
|    fps             | 16       |
|    time_elapsed    | 15252    |
|    total timesteps | 247466   |
| train/             |          |
|    actor_loss      | 0.173    |
|    critic_loss     | 0.993    |
|    learning_rate   | 0.0005   |
|    n_updates       | 241463   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-19.14 +/- 12.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19.1    |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | 0.17     |
|    critic_loss     | 0.963    |
|    learning_rate   | 0.0005   |
|    n_updates       | 245465   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.25     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 16       |
|    time_elapsed    | 15780    |
|    total timesteps | 255470   |
| train/             |          |
|    actor_loss      | 0.174    |
|    critic_loss     | 0.994    |
|    learning_rate   | 0.0005   |
|    n_updates       | 249467   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-17.86 +/- 34.81
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -17.9    |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 0.189    |
|    critic_loss     | 0.984    |
|    learning_rate   | 0.0005   |
|    n_updates       | 255470   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.022   |
| time/              |          |
|    episodes        | 132      |
|    fps             | 16       |
|    time_elapsed    | 16308    |
|    total timesteps | 263474   |
| train/             |          |
|    actor_loss      | 0.186    |
|    critic_loss     | 0.96     |
|    learning_rate   | 0.0005   |
|    n_updates       | 257471   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-20.61 +/- 30.86
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -20.6    |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | 0.177    |
|    critic_loss     | 0.958    |
|    learning_rate   | 0.0005   |
|    n_updates       | 265475   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.816   |
| time/              |          |
|    episodes        | 136      |
|    fps             | 16       |
|    time_elapsed    | 16837    |
|    total timesteps | 271478   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.09    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 16       |
|    time_elapsed    | 17252    |
|    total timesteps | 279482   |
| train/             |          |
|    actor_loss      | 0.185    |
|    critic_loss     | 0.966    |
|    learning_rate   | 0.0005   |
|    n_updates       | 273479   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-21.44 +/- 28.90
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -21.4    |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 0.19     |
|    critic_loss     | 0.963    |
|    learning_rate   | 0.0005   |
|    n_updates       | 275480   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.705   |
| time/              |          |
|    episodes        | 144      |
|    fps             | 16       |
|    time_elapsed    | 17798    |
|    total timesteps | 287486   |
| train/             |          |
|    actor_loss      | 0.188    |
|    critic_loss     | 0.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 281483   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-10.86 +/- 18.83
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.9    |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | 0.18     |
|    critic_loss     | 0.944    |
|    learning_rate   | 0.0005   |
|    n_updates       | 285485   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.82    |
| time/              |          |
|    episodes        | 148      |
|    fps             | 16       |
|    time_elapsed    | 18331    |
|    total timesteps | 295490   |
| train/             |          |
|    actor_loss      | 0.181    |
|    critic_loss     | 0.957    |
|    learning_rate   | 0.0005   |
|    n_updates       | 289487   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-15.29 +/- 35.70
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -15.3    |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 0.179    |
|    critic_loss     | 0.966    |
|    learning_rate   | 0.0005   |
|    n_updates       | 295490   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.39    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 16       |
|    time_elapsed    | 18860    |
|    total timesteps | 303494   |
| train/             |          |
|    actor_loss      | 0.183    |
|    critic_loss     | 0.951    |
|    learning_rate   | 0.0005   |
|    n_updates       | 297491   |
---------------------------------
Eval num_timesteps=310000, episode_reward=-46.92 +/- 16.87
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -46.9    |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | 0.172    |
|    critic_loss     | 0.941    |
|    learning_rate   | 0.0005   |
|    n_updates       | 305495   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.75    |
| time/              |          |
|    episodes        | 156      |
|    fps             | 16       |
|    time_elapsed    | 19385    |
|    total timesteps | 311498   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.19    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 16       |
|    time_elapsed    | 19797    |
|    total timesteps | 319502   |
| train/             |          |
|    actor_loss      | 0.187    |
|    critic_loss     | 0.938    |
|    learning_rate   | 0.0005   |
|    n_updates       | 313499   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-44.73 +/- 45.77
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -44.7    |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 0.184    |
|    critic_loss     | 0.947    |
|    learning_rate   | 0.0005   |
|    n_updates       | 315500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.46    |
| time/              |          |
|    episodes        | 164      |
|    fps             | 16       |
|    time_elapsed    | 20327    |
|    total timesteps | 327506   |
| train/             |          |
|    actor_loss      | 0.189    |
|    critic_loss     | 0.955    |
|    learning_rate   | 0.0005   |
|    n_updates       | 321503   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-4.51 +/- 13.40
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -4.51    |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | 0.193    |
|    critic_loss     | 0.934    |
|    learning_rate   | 0.0005   |
|    n_updates       | 325505   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.14    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 16       |
|    time_elapsed    | 20851    |
|    total timesteps | 335510   |
| train/             |          |
|    actor_loss      | 0.18     |
|    critic_loss     | 0.939    |
|    learning_rate   | 0.0005   |
|    n_updates       | 329507   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-28.16 +/- 29.89
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -28.2    |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | 0.184    |
|    critic_loss     | 0.942    |
|    learning_rate   | 0.0005   |
|    n_updates       | 335510   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.72    |
| time/              |          |
|    episodes        | 172      |
|    fps             | 16       |
|    time_elapsed    | 21375    |
|    total timesteps | 343514   |
| train/             |          |
|    actor_loss      | 0.187    |
|    critic_loss     | 0.976    |
|    learning_rate   | 0.0005   |
|    n_updates       | 337511   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-24.34 +/- 19.03
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -24.3    |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | 0.195    |
|    critic_loss     | 0.942    |
|    learning_rate   | 0.0005   |
|    n_updates       | 345515   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.57    |
| time/              |          |
|    episodes        | 176      |
|    fps             | 16       |
|    time_elapsed    | 21899    |
|    total timesteps | 351518   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.08    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 16       |
|    time_elapsed    | 22310    |
|    total timesteps | 359522   |
| train/             |          |
|    actor_loss      | 0.21     |
|    critic_loss     | 0.946    |
|    learning_rate   | 0.0005   |
|    n_updates       | 353519   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-29.39 +/- 25.42
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29.4    |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | 0.205    |
|    critic_loss     | 0.928    |
|    learning_rate   | 0.0005   |
|    n_updates       | 355520   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.19    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 16       |
|    time_elapsed    | 22837    |
|    total timesteps | 367526   |
| train/             |          |
|    actor_loss      | 0.214    |
|    critic_loss     | 0.947    |
|    learning_rate   | 0.0005   |
|    n_updates       | 361523   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-22.85 +/- 40.12
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -22.9    |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | 0.218    |
|    critic_loss     | 0.945    |
|    learning_rate   | 0.0005   |
|    n_updates       | 365525   |
---------------------------------
