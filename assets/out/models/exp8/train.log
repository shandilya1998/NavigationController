2021-12-04 23:41:51.447763: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-04 23:41:51.447830: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp8/TD3_9
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 757      |
|    ep_rew_mean     | 230      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 54       |
|    time_elapsed    | 55       |
|    total timesteps | 3027     |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 975      |
|    ep_rew_mean      | 200      |
| time/               |          |
|    episodes         | 8        |
|    fps              | 22       |
|    time_elapsed     | 342      |
|    total timesteps  | 7803     |
| train/              |          |
|    actor_loss       | -0.187   |
|    critic_loss      | 0.442    |
|    learning_rate    | 0.001    |
|    n_updates        | 360      |
|    total_actor_loss | 0.105    |
|    value_loss       | 0.292    |
----------------------------------
Eval num_timesteps=9000, episode_reward=21.46 +/- 49.41
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 21.5     |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | -0.474   |
|    critic_loss      | 0.13     |
|    learning_rate    | 0.001    |
|    n_updates        | 598      |
|    total_actor_loss | -0.429   |
|    value_loss       | 0.0445   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.15e+03 |
|    ep_rew_mean      | 141      |
| time/               |          |
|    episodes         | 12       |
|    fps              | 9        |
|    time_elapsed     | 1502     |
|    total timesteps  | 13807    |
| train/              |          |
|    actor_loss       | -0.307   |
|    critic_loss      | 0.159    |
|    learning_rate    | 0.001    |
|    n_updates        | 1560     |
|    total_actor_loss | -0.173   |
|    value_loss       | 0.133    |
----------------------------------
Eval num_timesteps=18000, episode_reward=-3.56 +/- 33.92
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -3.56    |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | -0.439   |
|    critic_loss      | 0.423    |
|    learning_rate    | 0.001    |
|    n_updates        | 2398     |
|    total_actor_loss | -0.242   |
|    value_loss       | 0.197    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.24e+03 |
|    ep_rew_mean      | 106      |
| time/               |          |
|    episodes         | 16       |
|    fps              | 7        |
|    time_elapsed     | 2664     |
|    total timesteps  | 19811    |
| train/              |          |
|    actor_loss       | -0.565   |
|    critic_loss      | 0.256    |
|    learning_rate    | 0.001    |
|    n_updates        | 2762     |
|    total_actor_loss | -0.364   |
|    value_loss       | 0.201    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.29e+03 |
|    ep_rew_mean      | 91.4     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 7        |
|    time_elapsed     | 3446     |
|    total timesteps  | 25815    |
| train/              |          |
|    actor_loss       | -1.03    |
|    critic_loss      | 0.961    |
|    learning_rate    | 0.001    |
|    n_updates        | 3962     |
|    total_actor_loss | -0.599   |
|    value_loss       | 0.436    |
----------------------------------
Eval num_timesteps=27000, episode_reward=18.22 +/- 63.14
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 18.2     |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | -0.705   |
|    critic_loss      | 1.69     |
|    learning_rate    | 0.001    |
|    n_updates        | 4198     |
|    total_actor_loss | 0.77     |
|    value_loss       | 1.47     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.33e+03 |
|    ep_rew_mean      | 76       |
| time/               |          |
|    episodes         | 24       |
|    fps              | 6        |
|    time_elapsed     | 4618     |
|    total timesteps  | 31819    |
| train/              |          |
|    actor_loss       | -0.619   |
|    critic_loss      | 0.316    |
|    learning_rate    | 0.001    |
|    n_updates        | 5162     |
|    total_actor_loss | -0.386   |
|    value_loss       | 0.233    |
----------------------------------
Eval num_timesteps=36000, episode_reward=-17.87 +/- 50.62
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -17.9    |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | -0.567   |
|    critic_loss      | 0.539    |
|    learning_rate    | 0.001    |
|    n_updates        | 5998     |
|    total_actor_loss | -0.402   |
|    value_loss       | 0.165    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.35e+03 |
|    ep_rew_mean      | 68.7     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 6        |
|    time_elapsed     | 5792     |
|    total timesteps  | 37823    |
| train/              |          |
|    actor_loss       | -0.418   |
|    critic_loss      | 0.549    |
|    learning_rate    | 0.001    |
|    n_updates        | 6364     |
|    total_actor_loss | -0.301   |
|    value_loss       | 0.117    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.37e+03 |
|    ep_rew_mean      | 55.7     |
| time/               |          |
|    episodes         | 32       |
|    fps              | 6        |
|    time_elapsed     | 6573     |
|    total timesteps  | 43827    |
| train/              |          |
|    actor_loss       | -0.461   |
|    critic_loss      | 0.685    |
|    learning_rate    | 0.001    |
|    n_updates        | 7564     |
|    total_actor_loss | -0.199   |
|    value_loss       | 0.262    |
----------------------------------
Eval num_timesteps=45000, episode_reward=-7.16 +/- 49.22
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -7.16    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | -0.704   |
|    critic_loss      | 0.536    |
|    learning_rate    | 0.001    |
|    n_updates        | 7798     |
|    total_actor_loss | 0.152    |
|    value_loss       | 0.857    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.38e+03 |
|    ep_rew_mean      | 56.7     |
| time/               |          |
|    episodes         | 36       |
|    fps              | 6        |
|    time_elapsed     | 7723     |
|    total timesteps  | 49831    |
| train/              |          |
|    actor_loss       | -0.857   |
|    critic_loss      | 1.94     |
|    learning_rate    | 0.001    |
|    n_updates        | 8766     |
|    total_actor_loss | 0.0597   |
|    value_loss       | 0.917    |
----------------------------------
Eval num_timesteps=54000, episode_reward=-29.28 +/- 18.67
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -29.3    |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | -0.981   |
|    critic_loss      | 2.36     |
|    learning_rate    | 0.001    |
|    n_updates        | 9598     |
|    total_actor_loss | 0.863    |
|    value_loss       | 1.84     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.4e+03  |
|    ep_rew_mean      | 49.1     |
| time/               |          |
|    episodes         | 40       |
|    fps              | 6        |
|    time_elapsed     | 8878     |
|    total timesteps  | 55835    |
| train/              |          |
|    actor_loss       | -0.749   |
|    critic_loss      | 0.928    |
|    learning_rate    | 0.001    |
|    n_updates        | 9966     |
|    total_actor_loss | 0.179    |
|    value_loss       | 0.927    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.41e+03 |
|    ep_rew_mean      | 45.3     |
| time/               |          |
|    episodes         | 44       |
|    fps              | 6        |
|    time_elapsed     | 9659     |
|    total timesteps  | 61839    |
| train/              |          |
|    actor_loss       | -0.515   |
|    critic_loss      | 2.85     |
|    learning_rate    | 0.001    |
|    n_updates        | 11166    |
|    total_actor_loss | -0.331   |
|    value_loss       | 0.184    |
----------------------------------
Eval num_timesteps=63000, episode_reward=98.22 +/- 109.10
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 98.2     |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    actor_loss       | -0.73    |
|    critic_loss      | 0.442    |
|    learning_rate    | 0.001    |
|    n_updates        | 11398    |
|    total_actor_loss | -0.56    |
|    value_loss       | 0.17     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.41e+03 |
|    ep_rew_mean      | 55.9     |
| time/               |          |
|    episodes         | 48       |
|    fps              | 6        |
|    time_elapsed     | 10808    |
|    total timesteps  | 67843    |
| train/              |          |
|    actor_loss       | -0.788   |
|    critic_loss      | 1.22     |
|    learning_rate    | 0.001    |
|    n_updates        | 12368    |
|    total_actor_loss | 0.108    |
|    value_loss       | 0.896    |
----------------------------------
[W python_anomaly_mode.cpp:104] Warning: Error detected in ExpBackward. Traceback of forward call that caused the error:
  File "train.py", line 39, in <module>
    model.learn(args.timesteps)
  File "/root/trainer/learning/explore.py", line 78, in learn
    callback = self.rl_callback
  File "/root/trainer/utils/td3_utils.py", line 713, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 369, in learn
    self.train(batch_size=self.batch_size, gradient_steps=gradient_steps)
  File "/root/trainer/utils/td3_utils.py", line 667, in train
    actions, vt = self.actor(replay_data.observations)
  File "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/trainer/utils/td3_utils.py", line 94, in forward
    return self.mu(features)
  File "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/trainer/bg/models.py", line 227, in forward
    bg_out, vt  = self.bg([stimulus_t, stimulus_t_1])
  File "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/trainer/bg/models.py", line 87, in forward
    lamd2 = 1 / (1 + torch.exp(self.log_a2.exp() * (deltavf - self.thetad2)))
 (function _print_stack)
Traceback (most recent call last):
  File "train.py", line 39, in <module>
    model.learn(args.timesteps)
  File "/root/trainer/learning/explore.py", line 78, in learn
    callback = self.rl_callback
  File "/root/trainer/utils/td3_utils.py", line 713, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 369, in learn
    self.train(batch_size=self.batch_size, gradient_steps=gradient_steps)
  File "/root/trainer/utils/td3_utils.py", line 676, in train
    loss.backward()
  File "/usr/local/lib/python3.6/site-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 149, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
RuntimeError: Function 'ExpBackward' returned nan values in its 0th output.
