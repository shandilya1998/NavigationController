2021-12-13 01:36:02.971210: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-13 01:36:02.971254: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_6
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 9.53     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 58       |
|    time_elapsed    | 136      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.93e+03 |
|    ep_rew_mean     | 0.0176   |
| time/              |          |
|    episodes        | 8        |
|    fps             | 59       |
|    time_elapsed    | 260      |
|    total timesteps | 15450    |
---------------------------------
Terminated
2021-12-13 01:42:24.842419: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-13 01:42:24.842455: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_7
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -35.4    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 59       |
|    time_elapsed    | 135      |
|    total timesteps | 7989     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -53.8    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 59       |
|    time_elapsed    | 268      |
|    total timesteps | 15993    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-100.22 +/- 74.05
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -100     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -90.3    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 31       |
|    time_elapsed    | 768      |
|    total timesteps | 23997    |
| train/             |          |
|    actor_loss      | 2.35     |
|    critic_loss     | 4.92     |
|    learning_rate   | 0.000735 |
|    n_updates       | 2001     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -118     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 23       |
|    time_elapsed    | 1372     |
|    total timesteps | 32001    |
| train/             |          |
|    actor_loss      | 2.84     |
|    critic_loss     | 0.791    |
|    learning_rate   | 0.000729 |
|    n_updates       | 10005    |
---------------------------------
Eval num_timesteps=40000, episode_reward=11.00 +/- 49.18
Episode length: 1669.00 +/- 536.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.67e+03 |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 3.05     |
|    critic_loss     | 0.994    |
|    learning_rate   | 0.000723 |
|    n_updates       | 18009    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -122     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 18       |
|    time_elapsed    | 2161     |
|    total timesteps | 40005    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -122     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 17       |
|    time_elapsed    | 2774     |
|    total timesteps | 48009    |
| train/             |          |
|    actor_loss      | 7.13     |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.000718 |
|    n_updates       | 26013    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -127     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 16       |
|    time_elapsed    | 3393     |
|    total timesteps | 56013    |
| train/             |          |
|    actor_loss      | 6.37     |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.000712 |
|    n_updates       | 34017    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-64.17 +/- 67.94
Episode length: 1981.40 +/- 39.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.98e+03 |
|    mean_reward     | -64.2    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 5.99     |
|    critic_loss     | 1        |
|    learning_rate   | 0.000709 |
|    n_updates       | 38019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -124     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 15       |
|    time_elapsed    | 4177     |
|    total timesteps | 63104    |
| train/             |          |
|    actor_loss      | 5.61     |
|    critic_loss     | 0.915    |
|    learning_rate   | 0.000707 |
|    n_updates       | 41344    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -122     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 14       |
|    time_elapsed    | 4793     |
|    total timesteps | 71069    |
| train/             |          |
|    actor_loss      | 4.97     |
|    critic_loss     | 0.773    |
|    learning_rate   | 0.000702 |
|    n_updates       | 49112    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -126     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 14       |
|    time_elapsed    | 5419     |
|    total timesteps | 79073    |
| train/             |          |
|    actor_loss      | 4.31     |
|    critic_loss     | 0.775    |
|    learning_rate   | 0.000696 |
|    n_updates       | 57077    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-67.67 +/- 74.81
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -67.7    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 4.11     |
|    critic_loss     | 0.673    |
|    learning_rate   | 0.000695 |
|    n_updates       | 59078    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -130     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 13       |
|    time_elapsed    | 6293     |
|    total timesteps | 87077    |
| train/             |          |
|    actor_loss      | 3.84     |
|    critic_loss     | 0.689    |
|    learning_rate   | 0.00069  |
|    n_updates       | 65081    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -131     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 13       |
|    time_elapsed    | 6985     |
|    total timesteps | 95081    |
| train/             |          |
|    actor_loss      | 3.6      |
|    critic_loss     | 0.745    |
|    learning_rate   | 0.000685 |
|    n_updates       | 73085    |
---------------------------------
Eval num_timesteps=100000, episode_reward=35.80 +/- 39.18
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 35.8     |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 3.49     |
|    critic_loss     | 0.667    |
|    learning_rate   | 0.000681 |
|    n_updates       | 79088    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -133     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 13       |
|    time_elapsed    | 7926     |
|    total timesteps | 103085   |
| train/             |          |
|    actor_loss      | 3.48     |
|    critic_loss     | 0.811    |
|    learning_rate   | 0.000679 |
|    n_updates       | 81089    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -134     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 12       |
|    time_elapsed    | 8719     |
|    total timesteps | 111089   |
| train/             |          |
|    actor_loss      | 3.39     |
|    critic_loss     | 0.705    |
|    learning_rate   | 0.000674 |
|    n_updates       | 89093    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -132     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 12       |
|    time_elapsed    | 9472     |
|    total timesteps | 118047   |
| train/             |          |
|    actor_loss      | 3.28     |
|    critic_loss     | 0.703    |
|    learning_rate   | 0.000669 |
|    n_updates       | 96051    |
---------------------------------
Eval num_timesteps=120000, episode_reward=38.85 +/- 35.28
Episode length: 1895.60 +/- 210.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.9e+03  |
|    mean_reward     | 38.8     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 3.27     |
|    critic_loss     | 0.671    |
|    learning_rate   | 0.000667 |
|    n_updates       | 98052    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -132     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 11       |
|    time_elapsed    | 10563    |
|    total timesteps | 126051   |
| train/             |          |
|    actor_loss      | 3.2      |
|    critic_loss     | 0.54     |
|    learning_rate   | 0.000663 |
|    n_updates       | 104055   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -136     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 11       |
|    time_elapsed    | 11457    |
|    total timesteps | 134055   |
| train/             |          |
|    actor_loss      | 3.12     |
|    critic_loss     | 0.76     |
|    learning_rate   | 0.000658 |
|    n_updates       | 112059   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-6.16 +/- 23.77
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -6.16    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 3.15     |
|    critic_loss     | 0.796    |
|    learning_rate   | 0.000653 |
|    n_updates       | 118062   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -138     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 11       |
|    time_elapsed    | 12592    |
|    total timesteps | 142059   |
| train/             |          |
|    actor_loss      | 3.19     |
|    critic_loss     | 0.886    |
|    learning_rate   | 0.000652 |
|    n_updates       | 120063   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -138     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 11       |
|    time_elapsed    | 13519    |
|    total timesteps | 150063   |
| train/             |          |
|    actor_loss      | 3.3      |
|    critic_loss     | 0.794    |
|    learning_rate   | 0.000646 |
|    n_updates       | 128067   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -145     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 10       |
|    time_elapsed    | 14466    |
|    total timesteps | 158067   |
| train/             |          |
|    actor_loss      | 3.41     |
|    critic_loss     | 0.781    |
|    learning_rate   | 0.000641 |
|    n_updates       | 136071   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-14.29 +/- 23.84
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.3    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 3.44     |
|    critic_loss     | 0.909    |
|    learning_rate   | 0.000639 |
|    n_updates       | 138072   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -144     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 10       |
|    time_elapsed    | 15437    |
|    total timesteps | 164348   |
| train/             |          |
|    actor_loss      | 3.48     |
|    critic_loss     | 0.832    |
|    learning_rate   | 0.000636 |
|    n_updates       | 142352   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -148     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 10       |
|    time_elapsed    | 16392    |
|    total timesteps | 172352   |
| train/             |          |
|    actor_loss      | 3.57     |
|    critic_loss     | 0.856    |
|    learning_rate   | 0.000631 |
|    n_updates       | 150356   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-22.75 +/- 86.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -22.7    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 3.63     |
|    critic_loss     | 0.972    |
|    learning_rate   | 0.000625 |
|    n_updates       | 158360   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -147     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 10       |
|    time_elapsed    | 17573    |
|    total timesteps | 180356   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -145     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 10       |
|    time_elapsed    | 18416    |
|    total timesteps | 187368   |
| train/             |          |
|    actor_loss      | 3.73     |
|    critic_loss     | 0.892    |
|    learning_rate   | 0.00062  |
|    n_updates       | 165372   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -145     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 10       |
|    time_elapsed    | 19258    |
|    total timesteps | 194374   |
| train/             |          |
|    actor_loss      | 3.78     |
|    critic_loss     | 1.12     |
|    learning_rate   | 0.000615 |
|    n_updates       | 172378   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-2.28 +/- 9.70
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.28    |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 3.8      |
|    critic_loss     | 0.988    |
|    learning_rate   | 0.000611 |
|    n_updates       | 178381   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -151     |
| time/              |          |
|    episodes        | 104      |
|    fps             | 9        |
|    time_elapsed    | 20439    |
|    total timesteps | 202378   |
| train/             |          |
|    actor_loss      | 3.82     |
|    critic_loss     | 1.01     |
|    learning_rate   | 0.00061  |
|    n_updates       | 180382   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -151     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 9        |
|    time_elapsed    | 21406    |
|    total timesteps | 210382   |
| train/             |          |
|    actor_loss      | 3.86     |
|    critic_loss     | 0.979    |
|    learning_rate   | 0.000604 |
|    n_updates       | 188386   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -151     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 9        |
|    time_elapsed    | 22372    |
|    total timesteps | 218386   |
| train/             |          |
|    actor_loss      | 3.86     |
|    critic_loss     | 1.03     |
|    learning_rate   | 0.000599 |
|    n_updates       | 196390   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-58.99 +/- 45.42
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -59      |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 3.85     |
|    critic_loss     | 0.866    |
|    learning_rate   | 0.000597 |
|    n_updates       | 198391   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -150     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 9        |
|    time_elapsed    | 23571    |
|    total timesteps | 226390   |
| train/             |          |
|    actor_loss      | 3.84     |
|    critic_loss     | 0.921    |
|    learning_rate   | 0.000593 |
|    n_updates       | 204394   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -152     |
| time/              |          |
|    episodes        | 120      |
|    fps             | 9        |
|    time_elapsed    | 24540    |
|    total timesteps | 234394   |
| train/             |          |
|    actor_loss      | 3.86     |
|    critic_loss     | 0.991    |
|    learning_rate   | 0.000587 |
|    n_updates       | 212398   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-31.43 +/- 50.05
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -31.4    |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 3.84     |
|    critic_loss     | 0.75     |
|    learning_rate   | 0.000583 |
|    n_updates       | 218100   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -150     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 9        |
|    time_elapsed    | 25693    |
|    total timesteps | 242097   |
| train/             |          |
|    actor_loss      | 3.82     |
|    critic_loss     | 0.932    |
|    learning_rate   | 0.000582 |
|    n_updates       | 220101   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -148     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 9        |
|    time_elapsed    | 26665    |
|    total timesteps | 250101   |
| train/             |          |
|    actor_loss      | 3.83     |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.000576 |
|    n_updates       | 228105   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -151     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 9        |
|    time_elapsed    | 27636    |
|    total timesteps | 258105   |
| train/             |          |
|    actor_loss      | 3.85     |
|    critic_loss     | 0.964    |
|    learning_rate   | 0.000571 |
|    n_updates       | 236109   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-23.06 +/- 29.52
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.1    |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 3.83     |
|    critic_loss     | 0.838    |
|    learning_rate   | 0.000569 |
|    n_updates       | 238110   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -152     |
| time/              |          |
|    episodes        | 136      |
|    fps             | 9        |
|    time_elapsed    | 28826    |
|    total timesteps | 266109   |
| train/             |          |
|    actor_loss      | 3.86     |
|    critic_loss     | 0.97     |
|    learning_rate   | 0.000565 |
|    n_updates       | 244113   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -152     |
| time/              |          |
|    episodes        | 140      |
|    fps             | 9        |
|    time_elapsed    | 29797    |
|    total timesteps | 274113   |
| train/             |          |
|    actor_loss      | 3.87     |
|    critic_loss     | 0.986    |
|    learning_rate   | 0.00056  |
|    n_updates       | 252117   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-21.29 +/- 38.85
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -21.3    |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 3.89     |
|    critic_loss     | 1.04     |
|    learning_rate   | 0.000554 |
|    n_updates       | 259972   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -150     |
| time/              |          |
|    episodes        | 144      |
|    fps             | 9        |
|    time_elapsed    | 30972    |
|    total timesteps | 281968   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -150     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 9        |
|    time_elapsed    | 31940    |
|    total timesteps | 289972   |
| train/             |          |
|    actor_loss      | 3.9      |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.000548 |
|    n_updates       | 267976   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -151     |
| time/              |          |
|    episodes        | 152      |
|    fps             | 9        |
|    time_elapsed    | 32909    |
|    total timesteps | 297976   |
| train/             |          |
|    actor_loss      | 3.93     |
|    critic_loss     | 0.923    |
|    learning_rate   | 0.000543 |
|    n_updates       | 275980   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-16.10 +/- 28.03
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -16.1    |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 3.87     |
|    critic_loss     | 0.895    |
|    learning_rate   | 0.00054  |
|    n_updates       | 279982   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -152     |
| time/              |          |
|    episodes        | 156      |
|    fps             | 8        |
|    time_elapsed    | 34104    |
|    total timesteps | 305980   |
| train/             |          |
|    actor_loss      | 3.84     |
|    critic_loss     | 0.89     |
|    learning_rate   | 0.000537 |
|    n_updates       | 283984   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -154     |
| time/              |          |
|    episodes        | 160      |
|    fps             | 8        |
|    time_elapsed    | 35082    |
|    total timesteps | 313984   |
| train/             |          |
|    actor_loss      | 3.78     |
|    critic_loss     | 0.812    |
|    learning_rate   | 0.000532 |
|    n_updates       | 291988   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-10.77 +/- 26.52
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.8    |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 3.74     |
|    critic_loss     | 0.739    |
|    learning_rate   | 0.000526 |
|    n_updates       | 299992   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -155     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 8        |
|    time_elapsed    | 36276    |
|    total timesteps | 321988   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -152     |
| time/              |          |
|    episodes        | 168      |
|    fps             | 8        |
|    time_elapsed    | 37242    |
|    total timesteps | 329992   |
| train/             |          |
|    actor_loss      | 3.67     |
|    critic_loss     | 0.516    |
|    learning_rate   | 0.00052  |
|    n_updates       | 307996   |
---------------------------------
Terminated
2021-12-13 12:17:01.535305: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-13 12:17:01.535411: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_8
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 68.1     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 45       |
|    time_elapsed    | 175      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 75.7     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 50       |
|    time_elapsed    | 316      |
|    total timesteps | 16008    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-273.70 +/- 197.13
Episode length: 1874.80 +/- 252.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.87e+03 |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 40.9     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 25       |
|    time_elapsed    | 937      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | -0.717   |
|    critic_loss     | 0.944    |
|    learning_rate   | 0.000735 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 45.9     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 20       |
|    time_elapsed    | 1597     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | -0.409   |
|    critic_loss     | 0.775    |
|    learning_rate   | 0.000729 |
|    n_updates       | 12006    |
---------------------------------
Eval num_timesteps=40000, episode_reward=23.61 +/- 41.16
Episode length: 1824.80 +/- 352.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.82e+03 |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -0.604   |
|    critic_loss     | 0.834    |
|    learning_rate   | 0.000723 |
|    n_updates       | 20010    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 57.7     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 16       |
|    time_elapsed    | 2490     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 58.3     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 15       |
|    time_elapsed    | 3161     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | -0.851   |
|    critic_loss     | 0.698    |
|    learning_rate   | 0.000718 |
|    n_updates       | 28014    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 56.5     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 14       |
|    time_elapsed    | 3836     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | -1.06    |
|    critic_loss     | 0.693    |
|    learning_rate   | 0.000712 |
|    n_updates       | 36018    |
---------------------------------
Terminated
2021-12-13 14:17:36.475053: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-13 14:17:36.475093: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_9
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 22.3     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 62       |
|    time_elapsed    | 128      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 38.7     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 62       |
|    time_elapsed    | 255      |
|    total timesteps | 16008    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-33.43 +/- 46.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -33.4    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 64.2     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 28       |
|    time_elapsed    | 849      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 11.9     |
|    critic_loss     | 2.85     |
|    learning_rate   | 0.000978 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 66.5     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 21       |
|    time_elapsed    | 1500     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 7.71     |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.00097  |
|    n_updates       | 12006    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-36.49 +/- 24.86
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -36.5    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 4.77     |
|    critic_loss     | 1        |
|    learning_rate   | 0.000962 |
|    n_updates       | 20010    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 55       |
| time/              |          |
|    episodes        | 20       |
|    fps             | 16       |
|    time_elapsed    | 2355     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 52.8     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 15       |
|    time_elapsed    | 3018     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 2.74     |
|    critic_loss     | 0.902    |
|    learning_rate   | 0.000954 |
|    n_updates       | 28014    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 40       |
| time/              |          |
|    episodes        | 28       |
|    fps             | 15       |
|    time_elapsed    | 3668     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 1.56     |
|    critic_loss     | 0.812    |
|    learning_rate   | 0.000947 |
|    n_updates       | 36018    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-38.38 +/- 29.47
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -38.4    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 1.36     |
|    critic_loss     | 0.909    |
|    learning_rate   | 0.000943 |
|    n_updates       | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 46.5     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 14       |
|    time_elapsed    | 4539     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 1.12     |
|    critic_loss     | 0.889    |
|    learning_rate   | 0.000939 |
|    n_updates       | 44022    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 43.3     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 13       |
|    time_elapsed    | 5210     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 0.908    |
|    critic_loss     | 0.876    |
|    learning_rate   | 0.000931 |
|    n_updates       | 52026    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-60.39 +/- 54.88
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -60.4    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.761    |
|    critic_loss     | 0.7      |
|    learning_rate   | 0.000923 |
|    n_updates       | 60030    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 37.6     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 13       |
|    time_elapsed    | 6096     |
|    total timesteps | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 34.3     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 12       |
|    time_elapsed    | 6797     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 0.424    |
|    critic_loss     | 0.908    |
|    learning_rate   | 0.000915 |
|    n_updates       | 68034    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 34.9     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 12       |
|    time_elapsed    | 7528     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 0.506    |
|    critic_loss     | 1.07     |
|    learning_rate   | 0.000907 |
|    n_updates       | 76038    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-28.47 +/- 8.12
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -28.5    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.37     |
|    critic_loss     | 0.891    |
|    learning_rate   | 0.000903 |
|    n_updates       | 80040    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 38.2     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 12       |
|    time_elapsed    | 8519     |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | 0.149    |
|    critic_loss     | 0.768    |
|    learning_rate   | 0.000899 |
|    n_updates       | 84042    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 35.7     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 11       |
|    time_elapsed    | 9419     |
|    total timesteps | 112056   |
| train/             |          |
|    actor_loss      | -0.165   |
|    critic_loss     | 0.883    |
|    learning_rate   | 0.000891 |
|    n_updates       | 92046    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-18.52 +/- 22.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -18.5    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -0.383   |
|    critic_loss     | 0.811    |
|    learning_rate   | 0.000883 |
|    n_updates       | 100050   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 34.8     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 11       |
|    time_elapsed    | 10563    |
|    total timesteps | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 36.2     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 11       |
|    time_elapsed    | 11505    |
|    total timesteps | 128064   |
| train/             |          |
|    actor_loss      | -0.514   |
|    critic_loss     | 0.771    |
|    learning_rate   | 0.000875 |
|    n_updates       | 108054   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 37.4     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 10       |
|    time_elapsed    | 12457    |
|    total timesteps | 136068   |
| train/             |          |
|    actor_loss      | -0.612   |
|    critic_loss     | 0.894    |
|    learning_rate   | 0.000867 |
|    n_updates       | 116058   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-10.54 +/- 11.45
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.5    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -0.669   |
|    critic_loss     | 0.792    |
|    learning_rate   | 0.000863 |
|    n_updates       | 120060   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 43.5     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 10       |
|    time_elapsed    | 13639    |
|    total timesteps | 144072   |
| train/             |          |
|    actor_loss      | -0.688   |
|    critic_loss     | 0.911    |
|    learning_rate   | 0.000859 |
|    n_updates       | 124062   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 45       |
| time/              |          |
|    episodes        | 76       |
|    fps             | 10       |
|    time_elapsed    | 14631    |
|    total timesteps | 152076   |
| train/             |          |
|    actor_loss      | -0.756   |
|    critic_loss     | 0.751    |
|    learning_rate   | 0.000851 |
|    n_updates       | 132066   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-26.87 +/- 15.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -26.9    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -0.879   |
|    critic_loss     | 0.691    |
|    learning_rate   | 0.000844 |
|    n_updates       | 140009   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 45.7     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 10       |
|    time_elapsed    | 15827    |
|    total timesteps | 160019   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 48.6     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 9        |
|    time_elapsed    | 16789    |
|    total timesteps | 167787   |
| train/             |          |
|    actor_loss      | -0.999   |
|    critic_loss     | 0.709    |
|    learning_rate   | 0.000836 |
|    n_updates       | 147777   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 46.7     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 9        |
|    time_elapsed    | 17770    |
|    total timesteps | 175791   |
| train/             |          |
|    actor_loss      | -0.988   |
|    critic_loss     | 0.777    |
|    learning_rate   | 0.000828 |
|    n_updates       | 155781   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-59.77 +/- 32.51
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -59.8    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.745    |
|    learning_rate   | 0.000822 |
|    n_updates       | 161784   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 46.1     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 9        |
|    time_elapsed    | 18969    |
|    total timesteps | 183795   |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.749    |
|    learning_rate   | 0.00082  |
|    n_updates       | 163785   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 45.4     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 9        |
|    time_elapsed    | 19950    |
|    total timesteps | 191799   |
| train/             |          |
|    actor_loss      | -1.07    |
|    critic_loss     | 0.706    |
|    learning_rate   | 0.000812 |
|    n_updates       | 171789   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 46.4     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 9        |
|    time_elapsed    | 20937    |
|    total timesteps | 199803   |
| train/             |          |
|    actor_loss      | -1.09    |
|    critic_loss     | 0.737    |
|    learning_rate   | 0.000804 |
|    n_updates       | 179793   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-45.14 +/- 46.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -45.1    |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -1.1     |
|    critic_loss     | 0.62     |
|    learning_rate   | 0.000802 |
|    n_updates       | 181794   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 45       |
| time/              |          |
|    episodes        | 104      |
|    fps             | 9        |
|    time_elapsed    | 22124    |
|    total timesteps | 207807   |
| train/             |          |
|    actor_loss      | -1.12    |
|    critic_loss     | 0.694    |
|    learning_rate   | 0.000796 |
|    n_updates       | 187797   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 45.2     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 9        |
|    time_elapsed    | 23117    |
|    total timesteps | 215811   |
| train/             |          |
|    actor_loss      | -1.04    |
|    critic_loss     | 0.76     |
|    learning_rate   | 0.000788 |
|    n_updates       | 195801   |
---------------------------------
Eval num_timesteps=220000, episode_reward=5.75 +/- 11.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 5.75     |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.766    |
|    learning_rate   | 0.000782 |
|    n_updates       | 201804   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 43.1     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 9        |
|    time_elapsed    | 24316    |
|    total timesteps | 223815   |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.712    |
|    learning_rate   | 0.00078  |
|    n_updates       | 203805   |
---------------------------------
Terminated
2021-12-13 21:29:43.663341: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-13 21:29:43.663388: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_10
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 94.3     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 97       |
|    time_elapsed    | 82       |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 98.9     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 98       |
|    time_elapsed    | 161      |
|    total timesteps | 16008    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-14.88 +/- 37.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.9    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 79       |
| time/              |          |
|    episodes        | 12       |
|    fps             | 34       |
|    time_elapsed    | 691      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 6.2      |
|    critic_loss     | 0.683    |
|    learning_rate   | 0.000978 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 70.6     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 25       |
|    time_elapsed    | 1264     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 3.48     |
|    critic_loss     | 0.086    |
|    learning_rate   | 0.00097  |
|    n_updates       | 12006    |
---------------------------------
Eval num_timesteps=40000, episode_reward=1.40 +/- 33.56
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 1.94     |
|    critic_loss     | 0.03     |
|    learning_rate   | 0.000962 |
|    n_updates       | 20010    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 68.3     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 19       |
|    time_elapsed    | 2051     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 67.7     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 18       |
|    time_elapsed    | 2625     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.9      |
|    critic_loss     | 0.0213   |
|    learning_rate   | 0.000954 |
|    n_updates       | 28014    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 73.1     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 17       |
|    time_elapsed    | 3197     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 0.33     |
|    critic_loss     | 0.0227   |
|    learning_rate   | 0.000947 |
|    n_updates       | 36018    |
---------------------------------
Eval num_timesteps=60000, episode_reward=7.12 +/- 31.44
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 7.12     |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.0822   |
|    critic_loss     | 0.0213   |
|    learning_rate   | 0.000943 |
|    n_updates       | 40020    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 66.2     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 16       |
|    time_elapsed    | 3981     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | -0.195   |
|    critic_loss     | 0.0227   |
|    learning_rate   | 0.000939 |
|    n_updates       | 44022    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 67.1     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 15       |
|    time_elapsed    | 4568     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | -0.595   |
|    critic_loss     | 0.0202   |
|    learning_rate   | 0.000931 |
|    n_updates       | 52026    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-7.31 +/- 32.26
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -7.31    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -0.872   |
|    critic_loss     | 0.0185   |
|    learning_rate   | 0.000923 |
|    n_updates       | 60030    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 65.3     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 14       |
|    time_elapsed    | 5370     |
|    total timesteps | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 66.4     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 14       |
|    time_elapsed    | 5992     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | -1.07    |
|    critic_loss     | 0.0184   |
|    learning_rate   | 0.000915 |
|    n_updates       | 68034    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 65.6     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 14       |
|    time_elapsed    | 6645     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | -1.22    |
|    critic_loss     | 0.0181   |
|    learning_rate   | 0.000907 |
|    n_updates       | 76038    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-23.62 +/- 21.16
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.6    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.0171   |
|    learning_rate   | 0.000903 |
|    n_updates       | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 69.6     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 13       |
|    time_elapsed    | 7543     |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.0216   |
|    learning_rate   | 0.000899 |
|    n_updates       | 84042    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 71.6     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 13       |
|    time_elapsed    | 8347     |
|    total timesteps | 112056   |
| train/             |          |
|    actor_loss      | -1.44    |
|    critic_loss     | 0.0199   |
|    learning_rate   | 0.000891 |
|    n_updates       | 92046    |
---------------------------------
Eval num_timesteps=120000, episode_reward=10.68 +/- 34.13
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -1.52    |
|    critic_loss     | 0.0207   |
|    learning_rate   | 0.000883 |
|    n_updates       | 100050   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 69.7     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 12       |
|    time_elapsed    | 9393     |
|    total timesteps | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 69.9     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 12       |
|    time_elapsed    | 10253    |
|    total timesteps | 128064   |
| train/             |          |
|    actor_loss      | -1.57    |
|    critic_loss     | 0.0198   |
|    learning_rate   | 0.000875 |
|    n_updates       | 108054   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 69.6     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 12       |
|    time_elapsed    | 11121    |
|    total timesteps | 136068   |
| train/             |          |
|    actor_loss      | -1.59    |
|    critic_loss     | 0.0205   |
|    learning_rate   | 0.000867 |
|    n_updates       | 116058   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-4.68 +/- 27.00
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -4.68    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -1.6     |
|    critic_loss     | 0.0211   |
|    learning_rate   | 0.000863 |
|    n_updates       | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 68.7     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 11       |
|    time_elapsed    | 12211    |
|    total timesteps | 144072   |
| train/             |          |
|    actor_loss      | -1.61    |
|    critic_loss     | 0.0203   |
|    learning_rate   | 0.000859 |
|    n_updates       | 124062   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 68.6     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 11       |
|    time_elapsed    | 13117    |
|    total timesteps | 152076   |
| train/             |          |
|    actor_loss      | -1.62    |
|    critic_loss     | 0.0196   |
|    learning_rate   | 0.000851 |
|    n_updates       | 132066   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-1.57 +/- 23.50
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.57    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -1.61    |
|    critic_loss     | 0.0191   |
|    learning_rate   | 0.000844 |
|    n_updates       | 140070   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 68.1     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 11       |
|    time_elapsed    | 14228    |
|    total timesteps | 160080   |
---------------------------------
Terminated
2021-12-14 01:56:21.924416: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-14 01:56:21.924453: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_12
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 147      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 97       |
|    time_elapsed    | 82       |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 112      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 97       |
|    time_elapsed    | 163      |
|    total timesteps | 16008    |
---------------------------------
Eval num_timesteps=20000, episode_reward=0.82 +/- 53.78
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 0.822    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
Traceback (most recent call last):
  File "train.py", line 75, in <module>
    model.learn(args.timesteps)
  File "/home/shandilya/Desktop/Projects/NavigationController/learning/explore.py", line 181, in learn
    callback = self.rl_callback
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 211, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 369, in learn
    self.train(batch_size=self.batch_size, gradient_steps=gradient_steps)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 145, in train
    replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/buffers.py", line 583, in sample
    return super(ReplayBuffer, self).sample(batch_size=batch_size, env=env)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/buffers.py", line 110, in sample
    return self._get_samples(batch_inds, env=env)
  File "/home/shandilya/Desktop/Projects/NavigationController/utils/td3_utils.py", line 950, in _get_samples
    actions = self.actions[batch_inds, 0, :]
IndexError: too many indices for array: array is 2-dimensional, but 3 were indexed
2021-12-14 02:32:16.401816: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-14 02:32:16.401853: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_16
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 72.9     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 18       |
|    time_elapsed    | 442      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | -0.29    |
|    critic_loss     | 0.387    |
|    learning_rate   | 0.000994 |
|    n_updates       | 6003     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 67       |
| time/              |          |
|    episodes        | 8        |
|    fps             | 15       |
|    time_elapsed    | 1010     |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | -6.09    |
|    critic_loss     | 0.911    |
|    learning_rate   | 0.000986 |
|    n_updates       | 14007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=18.75 +/- 32.77
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -7.69    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.000982 |
|    n_updates       | 18009    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 66.2     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 13       |
|    time_elapsed    | 1793     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | -8.5     |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.000978 |
|    n_updates       | 22011    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 61.1     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 13       |
|    time_elapsed    | 2368     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | -9.73    |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.00097  |
|    n_updates       | 30015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=36.90 +/- 46.52
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 36.9     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 1.81     |
|    learning_rate   | 0.000962 |
|    n_updates       | 38019    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 63.5     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 12       |
|    time_elapsed    | 3157     |
|    total timesteps | 40020    |
---------------------------------
Terminated
2021-12-14 03:28:45.521642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-14 03:28:45.521679: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_17
Terminated
running build_ext
building 'mujoco_py.cymj' extension
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o -fopenmp -w
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py
gcc -pthread -shared /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mjpro150/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mjpro150/bin -lmujoco150 -lglewegl -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp
2021-12-14 12:52:21.103726: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-14 12:52:21.103815: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp14/TD3_18
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 34.9     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 78       |
|    time_elapsed    | 102      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 32.1     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 80       |
|    time_elapsed    | 198      |
|    total timesteps | 16008    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=20000, episode_reward=-295.55 +/- 97.22
Episode length: 1777.80 +/- 446.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.78e+03 |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 24.9     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 38       |
|    time_elapsed    | 621      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 4.71     |
|    critic_loss     | 22.3     |
|    learning_rate   | 0.000978 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 22.8     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 27       |
|    time_elapsed    | 1143     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 6        |
|    critic_loss     | 114      |
|    learning_rate   | 0.00097  |
|    n_updates       | 12006    |
---------------------------------
Terminated
2021-12-14 13:21:09.565993: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-14 13:21:09.566066: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Env Type: maze
Task: CustomGoalReward4Rooms
Using cuda device
Logging to assets/out/models/exp14/TD3_19
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -102     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 78       |
|    time_elapsed    | 101      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -64.6    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 79       |
|    time_elapsed    | 201      |
|    total timesteps | 16008    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=20000, episode_reward=3.61 +/- 16.87
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 3.61     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -46.4    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 37       |
|    time_elapsed    | 645      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 11.8     |
|    critic_loss     | 28.7     |
|    learning_rate   | 0.000978 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -34.7    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 27       |
|    time_elapsed    | 1171     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 8.48     |
|    critic_loss     | 104      |
|    learning_rate   | 0.00097  |
|    n_updates       | 12006    |
---------------------------------
Terminated
2021-12-14 13:49:18.139320: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-14 13:49:18.139391: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Env Type: maze
Task: CustomGoalReward4Rooms
Using cuda device
Logging to assets/out/models/exp14/TD3_20
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 79       |
|    time_elapsed    | 101      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -29      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 80       |
|    time_elapsed    | 199      |
|    total timesteps | 16008    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=20000, episode_reward=15.64 +/- 49.80
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.68    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 37       |
|    time_elapsed    | 648      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 5.09     |
|    critic_loss     | 26.6     |
|    learning_rate   | 0.000978 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 27       |
|    time_elapsed    | 1179     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 5.31     |
|    critic_loss     | 18.6     |
|    learning_rate   | 0.00097  |
|    n_updates       | 12006    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-30.65 +/- 31.06
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -30.6    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 3.74     |
|    critic_loss     | 17.8     |
|    learning_rate   | 0.000962 |
|    n_updates       | 20010    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.8     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 21       |
|    time_elapsed    | 1844     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -3.93    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 20       |
|    time_elapsed    | 2327     |
|    total timesteps | 47407    |
| train/             |          |
|    actor_loss      | 3.13     |
|    critic_loss     | 18       |
|    learning_rate   | 0.000955 |
|    n_updates       | 27397    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -2.66    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 19       |
|    time_elapsed    | 2844     |
|    total timesteps | 55411    |
| train/             |          |
|    actor_loss      | 2.63     |
|    critic_loss     | 16.8     |
|    learning_rate   | 0.000947 |
|    n_updates       | 35401    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-5.87 +/- 21.48
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -5.87    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 2.05     |
|    critic_loss     | 16.5     |
|    learning_rate   | 0.000941 |
|    n_updates       | 41404    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 4.83     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 18       |
|    time_elapsed    | 3494     |
|    total timesteps | 63415    |
| train/             |          |
|    actor_loss      | 1.87     |
|    critic_loss     | 16.5     |
|    learning_rate   | 0.000939 |
|    n_updates       | 43405    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 3.84     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 17       |
|    time_elapsed    | 4011     |
|    total timesteps | 71419    |
| train/             |          |
|    actor_loss      | 1.22     |
|    critic_loss     | 16.3     |
|    learning_rate   | 0.000931 |
|    n_updates       | 51409    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 17       |
|    time_elapsed    | 4494     |
|    total timesteps | 78852    |
| train/             |          |
|    actor_loss      | 1.19     |
|    critic_loss     | 16.9     |
|    learning_rate   | 0.000924 |
|    n_updates       | 58842    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-24.99 +/- 40.04
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -25      |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 1.21     |
|    critic_loss     | 16.8     |
|    learning_rate   | 0.000922 |
|    n_updates       | 60843    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 9.2      |
| time/              |          |
|    episodes        | 44       |
|    fps             | 16       |
|    time_elapsed    | 5150     |
|    total timesteps | 86856    |
| train/             |          |
|    actor_loss      | 1.8      |
|    critic_loss     | 16.7     |
|    learning_rate   | 0.000916 |
|    n_updates       | 66846    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 8.66     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 16       |
|    time_elapsed    | 5673     |
|    total timesteps | 94860    |
| train/             |          |
|    actor_loss      | 2.27     |
|    critic_loss     | 15.7     |
|    learning_rate   | 0.000908 |
|    n_updates       | 74850    |
---------------------------------
