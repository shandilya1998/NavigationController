2021-12-02 22:57:37.159931: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-02 22:57:37.160005: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp7/TD3_5
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -5.18    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 8        |
|    time_elapsed     | 738      |
|    total timesteps  | 6004     |
| train/              |          |
|    actor_loss       | -0.00636 |
|    critic_loss      | 0.00166  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1101     |
|    total_actor_loss | -0.00589 |
|    value_loss       | 0.000469 |
----------------------------------
Eval num_timesteps=9000, episode_reward=-4.12 +/- 6.96
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -4.12    |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | 0.00504  |
|    critic_loss      | 0.00489  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1698     |
|    total_actor_loss | 0.00702  |
|    value_loss       | 0.00197  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -6.8     |
| time/               |          |
|    episodes         | 8        |
|    fps              | 6        |
|    time_elapsed     | 1917     |
|    total timesteps  | 12008    |
| train/              |          |
|    actor_loss       | -0.00589 |
|    critic_loss      | 0.00249  |
|    learning_rate    | 0.0007   |
|    n_updates        | 2301     |
|    total_actor_loss | -0.00478 |
|    value_loss       | 0.00111  |
----------------------------------
Eval num_timesteps=18000, episode_reward=-1.02 +/- 4.61
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -1.02    |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | 0.009    |
|    critic_loss      | 0.0051   |
|    learning_rate    | 0.0007   |
|    n_updates        | 3498     |
|    total_actor_loss | 0.0123   |
|    value_loss       | 0.00334  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -4.97    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 5        |
|    time_elapsed     | 3109     |
|    total timesteps  | 18012    |
| train/              |          |
|    actor_loss       | 0.0111   |
|    critic_loss      | 0.00224  |
|    learning_rate    | 0.0007   |
|    n_updates        | 3501     |
|    total_actor_loss | 0.0119   |
|    value_loss       | 0.000809 |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -6.01    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 6        |
|    time_elapsed     | 3888     |
|    total timesteps  | 24016    |
| train/              |          |
|    actor_loss       | 0.0173   |
|    critic_loss      | 0.00179  |
|    learning_rate    | 0.0007   |
|    n_updates        | 4704     |
|    total_actor_loss | 0.0183   |
|    value_loss       | 0.00109  |
----------------------------------
Eval num_timesteps=27000, episode_reward=-0.56 +/- 4.64
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -0.56    |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | 0.0263   |
|    critic_loss      | 0.0011   |
|    learning_rate    | 0.0007   |
|    n_updates        | 5298     |
|    total_actor_loss | 0.0269   |
|    value_loss       | 0.000644 |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -6.14    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 5        |
|    time_elapsed     | 5071     |
|    total timesteps  | 30020    |
| train/              |          |
|    actor_loss       | 0.0266   |
|    critic_loss      | 0.00144  |
|    learning_rate    | 0.0007   |
|    n_updates        | 5904     |
|    total_actor_loss | 0.0274   |
|    value_loss       | 0.000825 |
----------------------------------
Eval num_timesteps=36000, episode_reward=-9.12 +/- 6.48
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -9.12    |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | 0.0268   |
|    critic_loss      | 0.00118  |
|    learning_rate    | 0.0007   |
|    n_updates        | 7098     |
|    total_actor_loss | 0.0275   |
|    value_loss       | 0.000645 |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -5.38    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 5        |
|    time_elapsed     | 6257     |
|    total timesteps  | 36024    |
| train/              |          |
|    actor_loss       | 0.0226   |
|    critic_loss      | 0.00367  |
|    learning_rate    | 0.0007   |
|    n_updates        | 7104     |
|    total_actor_loss | 0.0243   |
|    value_loss       | 0.00172  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -4.01    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 5        |
|    time_elapsed     | 7028     |
|    total timesteps  | 42028    |
| train/              |          |
|    actor_loss       | 0.0198   |
|    critic_loss      | 0.00411  |
|    learning_rate    | 0.0007   |
|    n_updates        | 8304     |
|    total_actor_loss | 0.0218   |
|    value_loss       | 0.00194  |
----------------------------------
Eval num_timesteps=45000, episode_reward=-1.38 +/- 13.12
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -1.38    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | 0.0164   |
|    critic_loss      | 0.00146  |
|    learning_rate    | 0.0007   |
|    n_updates        | 8898     |
|    total_actor_loss | 0.0171   |
|    value_loss       | 0.000727 |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -4.55    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 5        |
|    time_elapsed     | 8211     |
|    total timesteps  | 48032    |
| train/              |          |
|    actor_loss       | 0.0199   |
|    critic_loss      | 0.00155  |
|    learning_rate    | 0.0007   |
|    n_updates        | 9507     |
|    total_actor_loss | 0.021    |
|    value_loss       | 0.00109  |
----------------------------------
Eval num_timesteps=54000, episode_reward=-3.86 +/- 8.77
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -3.86    |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | 0.0213   |
|    critic_loss      | 0.00281  |
|    learning_rate    | 0.0007   |
|    n_updates        | 10698    |
|    total_actor_loss | 0.0224   |
|    value_loss       | 0.00108  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -4.01    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 5        |
|    time_elapsed     | 9385     |
|    total timesteps  | 54036    |
| train/              |          |
|    actor_loss       | 0.0231   |
|    critic_loss      | 0.00331  |
|    learning_rate    | 0.0007   |
|    n_updates        | 10707    |
|    total_actor_loss | 0.0244   |
|    value_loss       | 0.00123  |
----------------------------------
Terminated
2021-12-03 01:38:54.875688: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-03 01:38:54.875757: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp7/TD3_6
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.4e+03  |
|    ep_rew_mean      | 23       |
| time/               |          |
|    episodes         | 4        |
|    fps              | 8        |
|    time_elapsed     | 684      |
|    total timesteps  | 5599     |
| train/              |          |
|    actor_loss       | 0.00141  |
|    critic_loss      | 0.00943  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1020     |
|    total_actor_loss | 0.0108   |
|    value_loss       | 0.00936  |
----------------------------------
Eval num_timesteps=9000, episode_reward=11.25 +/- 7.87
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 11.2     |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | -0.0381  |
|    critic_loss      | 0.00533  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1698     |
|    total_actor_loss | -0.0361  |
|    value_loss       | 0.00201  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.45e+03 |
|    ep_rew_mean      | 17       |
| time/               |          |
|    episodes         | 8        |
|    fps              | 6        |
|    time_elapsed     | 1882     |
|    total timesteps  | 11603    |
| train/              |          |
|    actor_loss       | -0.0126  |
|    critic_loss      | 0.018    |
|    learning_rate    | 0.0007   |
|    n_updates        | 2220     |
|    total_actor_loss | 0.000826 |
|    value_loss       | 0.0134   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.47e+03 |
|    ep_rew_mean      | 15.5     |
| time/               |          |
|    episodes         | 12       |
|    fps              | 6        |
|    time_elapsed     | 2654     |
|    total timesteps  | 17607    |
| train/              |          |
|    actor_loss       | -0.0598  |
|    critic_loss      | 0.00524  |
|    learning_rate    | 0.0007   |
|    n_updates        | 3420     |
|    total_actor_loss | -0.0497  |
|    value_loss       | 0.0101   |
----------------------------------
Eval num_timesteps=18000, episode_reward=8.33 +/- 3.54
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 8.33     |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | -0.00421 |
|    critic_loss      | 0.0153   |
|    learning_rate    | 0.0007   |
|    n_updates        | 3498     |
|    total_actor_loss | 0.00188  |
|    value_loss       | 0.00609  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.48e+03 |
|    ep_rew_mean      | 12.6     |
| time/               |          |
|    episodes         | 16       |
|    fps              | 6        |
|    time_elapsed     | 3839     |
|    total timesteps  | 23611    |
| train/              |          |
|    actor_loss       | 0.0136   |
|    critic_loss      | 0.0127   |
|    learning_rate    | 0.0007   |
|    n_updates        | 4623     |
|    total_actor_loss | 0.0245   |
|    value_loss       | 0.0109   |
----------------------------------
Eval num_timesteps=27000, episode_reward=8.52 +/- 10.98
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 8.52     |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | -0.0071  |
|    critic_loss      | 0.00848  |
|    learning_rate    | 0.0007   |
|    n_updates        | 5298     |
|    total_actor_loss | -0.00072 |
|    value_loss       | 0.00638  |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 1.48e+03  |
|    ep_rew_mean      | 12.1      |
| time/               |           |
|    episodes         | 20        |
|    fps              | 5         |
|    time_elapsed     | 5017      |
|    total timesteps  | 29615     |
| train/              |           |
|    actor_loss       | -0.000189 |
|    critic_loss      | 0.00406   |
|    learning_rate    | 0.0007    |
|    n_updates        | 5823      |
|    total_actor_loss | 0.000898  |
|    value_loss       | 0.00109   |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.48e+03 |
|    ep_rew_mean      | 18.6     |
| time/               |          |
|    episodes         | 24       |
|    fps              | 6        |
|    time_elapsed     | 5794     |
|    total timesteps  | 35619    |
| train/              |          |
|    actor_loss       | -0.0979  |
|    critic_loss      | 0.0179   |
|    learning_rate    | 0.0007   |
|    n_updates        | 7023     |
|    total_actor_loss | -0.0833  |
|    value_loss       | 0.0146   |
----------------------------------
Eval num_timesteps=36000, episode_reward=9.09 +/- 7.04
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 9.09     |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | -0.152   |
|    critic_loss      | 0.00822  |
|    learning_rate    | 0.0007   |
|    n_updates        | 7098     |
|    total_actor_loss | -0.135   |
|    value_loss       | 0.0167   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 16.8     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 5        |
|    time_elapsed     | 6958     |
|    total timesteps  | 41623    |
| train/              |          |
|    actor_loss       | -0.122   |
|    critic_loss      | 0.0215   |
|    learning_rate    | 0.0007   |
|    n_updates        | 8223     |
|    total_actor_loss | -0.107   |
|    value_loss       | 0.0146   |
----------------------------------
Eval num_timesteps=45000, episode_reward=16.31 +/- 6.48
Episode length: 1457.80 +/- 86.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.46e+03 |
|    mean_reward      | 16.3     |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | -0.024   |
|    critic_loss      | 0.0168   |
|    learning_rate    | 0.0007   |
|    n_updates        | 8898     |
|    total_actor_loss | -0.0146  |
|    value_loss       | 0.0094   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 19.2     |
| time/               |          |
|    episodes         | 32       |
|    fps              | 5        |
|    time_elapsed     | 8136     |
|    total timesteps  | 47627    |
| train/              |          |
|    actor_loss       | -0.0879  |
|    critic_loss      | 0.0199   |
|    learning_rate    | 0.0007   |
|    n_updates        | 9426     |
|    total_actor_loss | -0.0763  |
|    value_loss       | 0.0116   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 17.9     |
| time/               |          |
|    episodes         | 36       |
|    fps              | 6        |
|    time_elapsed     | 8924     |
|    total timesteps  | 53624    |
| train/              |          |
|    actor_loss       | -0.103   |
|    critic_loss      | 0.0164   |
|    learning_rate    | 0.0007   |
|    n_updates        | 10623    |
|    total_actor_loss | -0.0822  |
|    value_loss       | 0.0211   |
----------------------------------
Eval num_timesteps=54000, episode_reward=28.07 +/- 18.67
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 28.1     |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | -0.157   |
|    critic_loss      | 0.0186   |
|    learning_rate    | 0.0007   |
|    n_updates        | 10698    |
|    total_actor_loss | -0.142   |
|    value_loss       | 0.0146   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 17.5     |
| time/               |          |
|    episodes         | 40       |
|    fps              | 5        |
|    time_elapsed     | 10101    |
|    total timesteps  | 59628    |
| train/              |          |
|    actor_loss       | -0.00979 |
|    critic_loss      | 0.0063   |
|    learning_rate    | 0.0007   |
|    n_updates        | 11826    |
|    total_actor_loss | -0.0055  |
|    value_loss       | 0.00429  |
----------------------------------
Eval num_timesteps=63000, episode_reward=26.58 +/- 13.20
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 26.6     |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    actor_loss       | 0.00546  |
|    critic_loss      | 0.00348  |
|    learning_rate    | 0.0007   |
|    n_updates        | 12498    |
|    total_actor_loss | 0.0132   |
|    value_loss       | 0.00778  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 17.5     |
| time/               |          |
|    episodes         | 44       |
|    fps              | 5        |
|    time_elapsed     | 11259    |
|    total timesteps  | 65632    |
| train/              |          |
|    actor_loss       | -0.00475 |
|    critic_loss      | 0.011    |
|    learning_rate    | 0.0007   |
|    n_updates        | 13026    |
|    total_actor_loss | 0.0172   |
|    value_loss       | 0.022    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 17.6     |
| time/               |          |
|    episodes         | 48       |
|    fps              | 5        |
|    time_elapsed     | 12033    |
|    total timesteps  | 71636    |
| train/              |          |
|    actor_loss       | -0.0527  |
|    critic_loss      | 0.026    |
|    learning_rate    | 0.0007   |
|    n_updates        | 14226    |
|    total_actor_loss | -0.0393  |
|    value_loss       | 0.0134   |
----------------------------------
Eval num_timesteps=72000, episode_reward=28.52 +/- 15.77
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 28.5     |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    actor_loss       | -0.0318  |
|    critic_loss      | 0.0124   |
|    learning_rate    | 0.0007   |
|    n_updates        | 14298    |
|    total_actor_loss | -0.0244  |
|    value_loss       | 0.00742  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 17.5     |
| time/               |          |
|    episodes         | 52       |
|    fps              | 5        |
|    time_elapsed     | 13206    |
|    total timesteps  | 77640    |
| train/              |          |
|    actor_loss       | -0.0162  |
|    critic_loss      | 0.00503  |
|    learning_rate    | 0.0007   |
|    n_updates        | 15426    |
|    total_actor_loss | -0.0104  |
|    value_loss       | 0.00582  |
----------------------------------
Eval num_timesteps=81000, episode_reward=14.07 +/- 15.99
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 14.1     |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    actor_loss       | -0.00953 |
|    critic_loss      | 0.00728  |
|    learning_rate    | 0.0007   |
|    n_updates        | 16098    |
|    total_actor_loss | -0.00278 |
|    value_loss       | 0.00675  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 16.9     |
| time/               |          |
|    episodes         | 56       |
|    fps              | 5        |
|    time_elapsed     | 14393    |
|    total timesteps  | 83644    |
| train/              |          |
|    actor_loss       | -0.06    |
|    critic_loss      | 0.0069   |
|    learning_rate    | 0.0007   |
|    n_updates        | 16629    |
|    total_actor_loss | -0.0565  |
|    value_loss       | 0.0035   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 16.4     |
| time/               |          |
|    episodes         | 60       |
|    fps              | 5        |
|    time_elapsed     | 15168    |
|    total timesteps  | 89648    |
| train/              |          |
|    actor_loss       | -0.00182 |
|    critic_loss      | 0.00442  |
|    learning_rate    | 0.0007   |
|    n_updates        | 17829    |
|    total_actor_loss | 0.00267  |
|    value_loss       | 0.0045   |
----------------------------------
Eval num_timesteps=90000, episode_reward=21.81 +/- 15.56
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 21.8     |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    actor_loss       | -0.0121  |
|    critic_loss      | 0.00972  |
|    learning_rate    | 0.0007   |
|    n_updates        | 17898    |
|    total_actor_loss | -0.00138 |
|    value_loss       | 0.0107   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 16.1     |
| time/               |          |
|    episodes         | 64       |
|    fps              | 5        |
|    time_elapsed     | 16350    |
|    total timesteps  | 95652    |
| train/              |          |
|    actor_loss       | -0.0639  |
|    critic_loss      | 0.00122  |
|    learning_rate    | 0.0007   |
|    n_updates        | 19029    |
|    total_actor_loss | -0.0627  |
|    value_loss       | 0.00123  |
----------------------------------
Eval num_timesteps=99000, episode_reward=36.68 +/- 23.76
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 36.7     |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    actor_loss       | -0.068   |
|    critic_loss      | 0.00622  |
|    learning_rate    | 0.0007   |
|    n_updates        | 19698    |
|    total_actor_loss | -0.0641  |
|    value_loss       | 0.00392  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 17.9     |
| time/               |          |
|    episodes         | 68       |
|    fps              | 5        |
|    time_elapsed     | 17538    |
|    total timesteps  | 101656   |
| train/              |          |
|    actor_loss       | -0.127   |
|    critic_loss      | 0.00968  |
|    learning_rate    | 0.0007   |
|    n_updates        | 20232    |
|    total_actor_loss | -0.114   |
|    value_loss       | 0.0126   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 19       |
| time/               |          |
|    episodes         | 72       |
|    fps              | 5        |
|    time_elapsed     | 18311    |
|    total timesteps  | 107660   |
| train/              |          |
|    actor_loss       | -0.233   |
|    critic_loss      | 0.0188   |
|    learning_rate    | 0.0007   |
|    n_updates        | 21432    |
|    total_actor_loss | -0.214   |
|    value_loss       | 0.0196   |
----------------------------------
Eval num_timesteps=108000, episode_reward=11.51 +/- 4.41
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 11.5     |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    actor_loss       | -0.226   |
|    critic_loss      | 0.0374   |
|    learning_rate    | 0.0007   |
|    n_updates        | 21498    |
|    total_actor_loss | -0.197   |
|    value_loss       | 0.0291   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 18.5     |
| time/               |          |
|    episodes         | 76       |
|    fps              | 5        |
|    time_elapsed     | 19497    |
|    total timesteps  | 113664   |
| train/              |          |
|    actor_loss       | -0.196   |
|    critic_loss      | 0.0151   |
|    learning_rate    | 0.0007   |
|    n_updates        | 22632    |
|    total_actor_loss | -0.187   |
|    value_loss       | 0.00933  |
----------------------------------
Eval num_timesteps=117000, episode_reward=8.16 +/- 4.92
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 8.16     |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    actor_loss       | -0.119   |
|    critic_loss      | 0.00225  |
|    learning_rate    | 0.0007   |
|    n_updates        | 23298    |
|    total_actor_loss | -0.116   |
|    value_loss       | 0.0027   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 18       |
| time/               |          |
|    episodes         | 80       |
|    fps              | 5        |
|    time_elapsed     | 20668    |
|    total timesteps  | 119668   |
| train/              |          |
|    actor_loss       | -0.111   |
|    critic_loss      | 0.00301  |
|    learning_rate    | 0.0007   |
|    n_updates        | 23832    |
|    total_actor_loss | -0.109   |
|    value_loss       | 0.00229  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.6     |
| time/               |          |
|    episodes         | 84       |
|    fps              | 5        |
|    time_elapsed     | 21451    |
|    total timesteps  | 125672   |
| train/              |          |
|    actor_loss       | -0.097   |
|    critic_loss      | 0.00198  |
|    learning_rate    | 0.0007   |
|    n_updates        | 25035    |
|    total_actor_loss | -0.0955  |
|    value_loss       | 0.00149  |
----------------------------------
Eval num_timesteps=126000, episode_reward=11.12 +/- 8.04
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 11.1     |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    actor_loss       | -0.0969  |
|    critic_loss      | 0.00413  |
|    learning_rate    | 0.0007   |
|    n_updates        | 25098    |
|    total_actor_loss | -0.0929  |
|    value_loss       | 0.00392  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.4     |
| time/               |          |
|    episodes         | 88       |
|    fps              | 5        |
|    time_elapsed     | 22630    |
|    total timesteps  | 131676   |
| train/              |          |
|    actor_loss       | -0.111   |
|    critic_loss      | 0.00245  |
|    learning_rate    | 0.0007   |
|    n_updates        | 26235    |
|    total_actor_loss | -0.107   |
|    value_loss       | 0.0032   |
----------------------------------
Eval num_timesteps=135000, episode_reward=17.61 +/- 19.29
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 17.6     |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    actor_loss       | -0.129   |
|    critic_loss      | 0.00577  |
|    learning_rate    | 0.0007   |
|    n_updates        | 26898    |
|    total_actor_loss | -0.124   |
|    value_loss       | 0.00428  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 18.1     |
| time/               |          |
|    episodes         | 92       |
|    fps              | 5        |
|    time_elapsed     | 23824    |
|    total timesteps  | 137680   |
| train/              |          |
|    actor_loss       | -0.168   |
|    critic_loss      | 0.0107   |
|    learning_rate    | 0.0007   |
|    n_updates        | 27435    |
|    total_actor_loss | -0.157   |
|    value_loss       | 0.0104   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.8     |
| time/               |          |
|    episodes         | 96       |
|    fps              | 5        |
|    time_elapsed     | 24603    |
|    total timesteps  | 143684   |
| train/              |          |
|    actor_loss       | -0.121   |
|    critic_loss      | 0.00683  |
|    learning_rate    | 0.0007   |
|    n_updates        | 28635    |
|    total_actor_loss | -0.119   |
|    value_loss       | 0.00224  |
----------------------------------
Eval num_timesteps=144000, episode_reward=16.23 +/- 12.71
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 16.2     |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    actor_loss       | -0.146   |
|    critic_loss      | 0.00941  |
|    learning_rate    | 0.0007   |
|    n_updates        | 28698    |
|    total_actor_loss | -0.142   |
|    value_loss       | 0.00416  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.7     |
| time/               |          |
|    episodes         | 100      |
|    fps              | 5        |
|    time_elapsed     | 25799    |
|    total timesteps  | 149688   |
| train/              |          |
|    actor_loss       | -0.134   |
|    critic_loss      | 0.00631  |
|    learning_rate    | 0.0007   |
|    n_updates        | 29838    |
|    total_actor_loss | -0.129   |
|    value_loss       | 0.00547  |
----------------------------------
Eval num_timesteps=153000, episode_reward=6.81 +/- 5.29
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 6.81     |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    actor_loss       | -0.111   |
|    critic_loss      | 0.00757  |
|    learning_rate    | 0.0007   |
|    n_updates        | 30498    |
|    total_actor_loss | -0.107   |
|    value_loss       | 0.00367  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.1     |
| time/               |          |
|    episodes         | 104      |
|    fps              | 5        |
|    time_elapsed     | 26978    |
|    total timesteps  | 155692   |
| train/              |          |
|    actor_loss       | -0.15    |
|    critic_loss      | 0.00991  |
|    learning_rate    | 0.0007   |
|    n_updates        | 31038    |
|    total_actor_loss | -0.144   |
|    value_loss       | 0.0058   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 18       |
| time/               |          |
|    episodes         | 108      |
|    fps              | 5        |
|    time_elapsed     | 27758    |
|    total timesteps  | 161696   |
| train/              |          |
|    actor_loss       | -0.13    |
|    critic_loss      | 0.00804  |
|    learning_rate    | 0.0007   |
|    n_updates        | 32238    |
|    total_actor_loss | -0.127   |
|    value_loss       | 0.00336  |
----------------------------------
Eval num_timesteps=162000, episode_reward=19.61 +/- 14.95
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 19.6     |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    actor_loss       | -0.133   |
|    critic_loss      | 0.0131   |
|    learning_rate    | 0.0007   |
|    n_updates        | 32298    |
|    total_actor_loss | -0.126   |
|    value_loss       | 0.00668  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.8     |
| time/               |          |
|    episodes         | 112      |
|    fps              | 5        |
|    time_elapsed     | 28937    |
|    total timesteps  | 167700   |
| train/              |          |
|    actor_loss       | -0.167   |
|    critic_loss      | 0.01     |
|    learning_rate    | 0.0007   |
|    n_updates        | 33438    |
|    total_actor_loss | -0.162   |
|    value_loss       | 0.00551  |
----------------------------------
Eval num_timesteps=171000, episode_reward=27.48 +/- 15.50
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 27.5     |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    actor_loss       | -0.136   |
|    critic_loss      | 0.00464  |
|    learning_rate    | 0.0007   |
|    n_updates        | 34098    |
|    total_actor_loss | -0.133   |
|    value_loss       | 0.00313  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 18       |
| time/               |          |
|    episodes         | 116      |
|    fps              | 5        |
|    time_elapsed     | 30129    |
|    total timesteps  | 173704   |
| train/              |          |
|    actor_loss       | -0.144   |
|    critic_loss      | 0.0034   |
|    learning_rate    | 0.0007   |
|    n_updates        | 34641    |
|    total_actor_loss | -0.142   |
|    value_loss       | 0.00188  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 18.4     |
| time/               |          |
|    episodes         | 120      |
|    fps              | 5        |
|    time_elapsed     | 30896    |
|    total timesteps  | 179708   |
| train/              |          |
|    actor_loss       | -0.144   |
|    critic_loss      | 0.0108   |
|    learning_rate    | 0.0007   |
|    n_updates        | 35841    |
|    total_actor_loss | -0.14    |
|    value_loss       | 0.00355  |
----------------------------------
Eval num_timesteps=180000, episode_reward=19.27 +/- 13.49
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 19.3     |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    actor_loss       | -0.132   |
|    critic_loss      | 0.00497  |
|    learning_rate    | 0.0007   |
|    n_updates        | 35898    |
|    total_actor_loss | -0.128   |
|    value_loss       | 0.00368  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17       |
| time/               |          |
|    episodes         | 124      |
|    fps              | 5        |
|    time_elapsed     | 32079    |
|    total timesteps  | 185712   |
| train/              |          |
|    actor_loss       | -0.19    |
|    critic_loss      | 0.0044   |
|    learning_rate    | 0.0007   |
|    n_updates        | 37041    |
|    total_actor_loss | -0.186   |
|    value_loss       | 0.00367  |
----------------------------------
Eval num_timesteps=189000, episode_reward=18.69 +/- 12.91
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 18.7     |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    actor_loss       | -0.15    |
|    critic_loss      | 0.00505  |
|    learning_rate    | 0.0007   |
|    n_updates        | 37698    |
|    total_actor_loss | -0.146   |
|    value_loss       | 0.00397  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.4     |
| time/               |          |
|    episodes         | 128      |
|    fps              | 5        |
|    time_elapsed     | 33266    |
|    total timesteps  | 191716   |
| train/              |          |
|    actor_loss       | -0.164   |
|    critic_loss      | 0.00582  |
|    learning_rate    | 0.0007   |
|    n_updates        | 38244    |
|    total_actor_loss | -0.158   |
|    value_loss       | 0.00619  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17       |
| time/               |          |
|    episodes         | 132      |
|    fps              | 5        |
|    time_elapsed     | 34059    |
|    total timesteps  | 197720   |
| train/              |          |
|    actor_loss       | -0.201   |
|    critic_loss      | 0.0131   |
|    learning_rate    | 0.0007   |
|    n_updates        | 39444    |
|    total_actor_loss | -0.192   |
|    value_loss       | 0.00844  |
----------------------------------
Eval num_timesteps=198000, episode_reward=17.19 +/- 12.62
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 17.2     |
| time/               |          |
|    total_timesteps  | 198000   |
| train/              |          |
|    actor_loss       | -0.199   |
|    critic_loss      | 0.00662  |
|    learning_rate    | 0.0007   |
|    n_updates        | 39498    |
|    total_actor_loss | -0.195   |
|    value_loss       | 0.00401  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.4     |
| time/               |          |
|    episodes         | 136      |
|    fps              | 5        |
|    time_elapsed     | 35250    |
|    total timesteps  | 203724   |
| train/              |          |
|    actor_loss       | -0.245   |
|    critic_loss      | 0.00919  |
|    learning_rate    | 0.0007   |
|    n_updates        | 40644    |
|    total_actor_loss | -0.242   |
|    value_loss       | 0.00289  |
----------------------------------
Eval num_timesteps=207000, episode_reward=12.71 +/- 23.54
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 12.7     |
| time/               |          |
|    total_timesteps  | 207000   |
| train/              |          |
|    actor_loss       | -0.243   |
|    critic_loss      | 0.00915  |
|    learning_rate    | 0.0007   |
|    n_updates        | 41298    |
|    total_actor_loss | -0.234   |
|    value_loss       | 0.00849  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.5     |
| time/               |          |
|    episodes         | 140      |
|    fps              | 5        |
|    time_elapsed     | 36439    |
|    total timesteps  | 209728   |
| train/              |          |
|    actor_loss       | -0.237   |
|    critic_loss      | 0.00974  |
|    learning_rate    | 0.0007   |
|    n_updates        | 41844    |
|    total_actor_loss | -0.225   |
|    value_loss       | 0.0118   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 19.2     |
| time/               |          |
|    episodes         | 144      |
|    fps              | 5        |
|    time_elapsed     | 37219    |
|    total timesteps  | 215732   |
| train/              |          |
|    actor_loss       | -0.293   |
|    critic_loss      | 0.018    |
|    learning_rate    | 0.0007   |
|    n_updates        | 43047    |
|    total_actor_loss | -0.259   |
|    value_loss       | 0.0335   |
----------------------------------
Eval num_timesteps=216000, episode_reward=59.78 +/- 64.39
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 59.8     |
| time/               |          |
|    total_timesteps  | 216000   |
| train/              |          |
|    actor_loss       | -0.304   |
|    critic_loss      | 0.0207   |
|    learning_rate    | 0.0007   |
|    n_updates        | 43098    |
|    total_actor_loss | -0.296   |
|    value_loss       | 0.00798  |
----------------------------------
Terminated
2021-12-03 12:15:15.865827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-03 12:15:15.865896: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp7/TD3_8
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 49.9     |
| time/               |          |
|    episodes         | 4        |
|    fps              | 8        |
|    time_elapsed     | 729      |
|    total timesteps  | 6004     |
| train/              |          |
|    actor_loss       | -0.117   |
|    critic_loss      | 0.0378   |
|    learning_rate    | 0.0007   |
|    n_updates        | 1101     |
|    total_actor_loss | -0.095   |
|    value_loss       | 0.0224   |
----------------------------------
Eval num_timesteps=9000, episode_reward=15.59 +/- 10.00
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 15.6     |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | -0.0292  |
|    critic_loss      | 0.0152   |
|    learning_rate    | 0.0007   |
|    n_updates        | 1698     |
|    total_actor_loss | -0.0175  |
|    value_loss       | 0.0117   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 34       |
| time/               |          |
|    episodes         | 8        |
|    fps              | 6        |
|    time_elapsed     | 1924     |
|    total timesteps  | 12008    |
| train/              |          |
|    actor_loss       | -0.072   |
|    critic_loss      | 0.0119   |
|    learning_rate    | 0.0007   |
|    n_updates        | 2301     |
|    total_actor_loss | -0.0649  |
|    value_loss       | 0.00701  |
----------------------------------
Eval num_timesteps=18000, episode_reward=33.41 +/- 38.00
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 33.4     |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | -0.0639  |
|    critic_loss      | 0.025    |
|    learning_rate    | 0.0007   |
|    n_updates        | 3498     |
|    total_actor_loss | -0.037   |
|    value_loss       | 0.0269   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 28.2     |
| time/               |          |
|    episodes         | 12       |
|    fps              | 5        |
|    time_elapsed     | 3118     |
|    total timesteps  | 18012    |
| train/              |          |
|    actor_loss       | -0.0717  |
|    critic_loss      | 0.0219   |
|    learning_rate    | 0.0007   |
|    n_updates        | 3501     |
|    total_actor_loss | -0.0654  |
|    value_loss       | 0.00632  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 29.2     |
| time/               |          |
|    episodes         | 16       |
|    fps              | 6        |
|    time_elapsed     | 3903     |
|    total timesteps  | 24016    |
| train/              |          |
|    actor_loss       | -0.147   |
|    critic_loss      | 0.0146   |
|    learning_rate    | 0.0007   |
|    n_updates        | 4704     |
|    total_actor_loss | -0.137   |
|    value_loss       | 0.0101   |
----------------------------------
Eval num_timesteps=27000, episode_reward=24.82 +/- 26.00
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 24.8     |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | -0.146   |
|    critic_loss      | 0.0268   |
|    learning_rate    | 0.0007   |
|    n_updates        | 5298     |
|    total_actor_loss | -0.134   |
|    value_loss       | 0.012    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 32.1     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 5        |
|    time_elapsed     | 5094     |
|    total timesteps  | 30020    |
| train/              |          |
|    actor_loss       | -0.191   |
|    critic_loss      | 0.0132   |
|    learning_rate    | 0.0007   |
|    n_updates        | 5904     |
|    total_actor_loss | -0.179   |
|    value_loss       | 0.0117   |
----------------------------------
Eval num_timesteps=36000, episode_reward=261.79 +/- 60.16
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 262      |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | -0.326   |
|    critic_loss      | 0.0725   |
|    learning_rate    | 0.0007   |
|    n_updates        | 7098     |
|    total_actor_loss | -0.279   |
|    value_loss       | 0.047    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 40.4     |
| time/               |          |
|    episodes         | 24       |
|    fps              | 5        |
|    time_elapsed     | 6287     |
|    total timesteps  | 36024    |
| train/              |          |
|    actor_loss       | -0.329   |
|    critic_loss      | 0.0348   |
|    learning_rate    | 0.0007   |
|    n_updates        | 7104     |
|    total_actor_loss | -0.309   |
|    value_loss       | 0.0191   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 42.2     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 5        |
|    time_elapsed     | 7068     |
|    total timesteps  | 42028    |
| train/              |          |
|    actor_loss       | -0.43    |
|    critic_loss      | 0.0221   |
|    learning_rate    | 0.0007   |
|    n_updates        | 8304     |
|    total_actor_loss | -0.412   |
|    value_loss       | 0.0173   |
----------------------------------
Eval num_timesteps=45000, episode_reward=52.28 +/- 18.60
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 52.3     |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | -0.411   |
|    critic_loss      | 0.0156   |
|    learning_rate    | 0.0007   |
|    n_updates        | 8898     |
|    total_actor_loss | -0.405   |
|    value_loss       | 0.00628  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 47       |
| time/               |          |
|    episodes         | 32       |
|    fps              | 5        |
|    time_elapsed     | 8267     |
|    total timesteps  | 48032    |
| train/              |          |
|    actor_loss       | -0.548   |
|    critic_loss      | 0.0303   |
|    learning_rate    | 0.0007   |
|    n_updates        | 9507     |
|    total_actor_loss | -0.532   |
|    value_loss       | 0.016    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.47e+03 |
|    ep_rew_mean      | 60.7     |
| time/               |          |
|    episodes         | 36       |
|    fps              | 5        |
|    time_elapsed     | 8884     |
|    total timesteps  | 52750    |
| train/              |          |
|    actor_loss       | -1.08    |
|    critic_loss      | 0.0861   |
|    learning_rate    | 0.0007   |
|    n_updates        | 10449    |
|    total_actor_loss | -1.05    |
|    value_loss       | 0.0254   |
----------------------------------
