running build_ext
building 'mujoco_py.cymj' extension
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o -fopenmp -w
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py
gcc -pthread -shared /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mjpro150/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mjpro150/bin -lmujoco150 -lglewegl -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp
2021-12-10 12:44:46.309827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-10 12:44:46.309897: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp12/TD3_6
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -60.8    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 36       |
|    time_elapsed    | 217      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 1.3      |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 3003     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-7.53 +/- 28.57
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -7.53    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.38     |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.0005   |
|    n_updates       | 4999     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -16.4    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 20       |
|    time_elapsed    | 766      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 1.36     |
|    critic_loss     | 0.453    |
|    learning_rate   | 0.0005   |
|    n_updates       | 11007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=12.79 +/- 26.09
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 1.09     |
|    critic_loss     | 0.791    |
|    learning_rate   | 0.0005   |
|    n_updates       | 14999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.03    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 18       |
|    time_elapsed    | 1319     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 1.06     |
|    critic_loss     | 0.817    |
|    learning_rate   | 0.0005   |
|    n_updates       | 19011    |
---------------------------------
Eval num_timesteps=30000, episode_reward=10.64 +/- 19.60
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 0.754    |
|    critic_loss     | 2.36     |
|    learning_rate   | 0.0005   |
|    n_updates       | 24999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -12.8    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 17       |
|    time_elapsed    | 1869     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 0.702    |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.0005   |
|    n_updates       | 27015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=39.09 +/- 8.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 39.1     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.61     |
|    critic_loss     | 1.24     |
|    learning_rate   | 0.0005   |
|    n_updates       | 34999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.174   |
| time/              |          |
|    episodes        | 20       |
|    fps             | 16       |
|    time_elapsed    | 2418     |
|    total timesteps | 40020    |
| train/             |          |
|    actor_loss      | 0.595    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 35019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 16       |
|    time_elapsed    | 2853     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.444    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 43023    |
---------------------------------
Eval num_timesteps=50000, episode_reward=4.66 +/- 21.49
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 4.66     |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.434    |
|    critic_loss     | 0.0914   |
|    learning_rate   | 0.0005   |
|    n_updates       | 44999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 16       |
|    time_elapsed    | 3416     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 0.336    |
|    critic_loss     | 0.0682   |
|    learning_rate   | 0.0005   |
|    n_updates       | 51027    |
---------------------------------
Eval num_timesteps=60000, episode_reward=16.64 +/- 21.97
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.296    |
|    critic_loss     | 0.0819   |
|    learning_rate   | 0.0005   |
|    n_updates       | 54999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.51    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 16       |
|    time_elapsed    | 3963     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 0.177    |
|    critic_loss     | 1.14     |
|    learning_rate   | 0.0005   |
|    n_updates       | 59031    |
---------------------------------
Eval num_timesteps=70000, episode_reward=23.46 +/- 45.85
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 0.401    |
|    critic_loss     | 0.543    |
|    learning_rate   | 0.0005   |
|    n_updates       | 64999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.77    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 15       |
|    time_elapsed    | 4504     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 0.261    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 67035    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-12.04 +/- 46.12
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -12      |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.287    |
|    critic_loss     | 0.841    |
|    learning_rate   | 0.0005   |
|    n_updates       | 74999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.15    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 15       |
|    time_elapsed    | 5047     |
|    total timesteps | 80040    |
| train/             |          |
|    actor_loss      | 0.0988   |
|    critic_loss     | 1.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 75039    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.57    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 16       |
|    time_elapsed    | 5477     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 0.313    |
|    critic_loss     | 0.0833   |
|    learning_rate   | 0.0005   |
|    n_updates       | 83043    |
---------------------------------
Eval num_timesteps=90000, episode_reward=12.72 +/- 17.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 0.0842   |
|    critic_loss     | 0.848    |
|    learning_rate   | 0.0005   |
|    n_updates       | 84999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.43    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 15       |
|    time_elapsed    | 6028     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 0.251    |
|    critic_loss     | 1.56     |
|    learning_rate   | 0.0005   |
|    n_updates       | 91047    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-14.20 +/- 25.45
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.2    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.259    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 94999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.95    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 15       |
|    time_elapsed    | 6567     |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | 0.156    |
|    critic_loss     | 0.432    |
|    learning_rate   | 0.0005   |
|    n_updates       | 99051    |
---------------------------------
Eval num_timesteps=110000, episode_reward=12.22 +/- 25.37
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -0.0837  |
|    critic_loss     | 0.979    |
|    learning_rate   | 0.0005   |
|    n_updates       | 104999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.12    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 15       |
|    time_elapsed    | 7107     |
|    total timesteps | 112056   |
| train/             |          |
|    actor_loss      | 0.353    |
|    critic_loss     | 0.822    |
|    learning_rate   | 0.0005   |
|    n_updates       | 107055   |
---------------------------------
Eval num_timesteps=120000, episode_reward=7.41 +/- 33.57
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 7.41     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 0.104    |
|    critic_loss     | 0.443    |
|    learning_rate   | 0.0005   |
|    n_updates       | 114999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.54    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 15       |
|    time_elapsed    | 7645     |
|    total timesteps | 120060   |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 115059   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.66    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 15       |
|    time_elapsed    | 8074     |
|    total timesteps | 128064   |
| train/             |          |
|    actor_loss      | 0.265    |
|    critic_loss     | 1.15     |
|    learning_rate   | 0.0005   |
|    n_updates       | 123063   |
---------------------------------
Eval num_timesteps=130000, episode_reward=5.56 +/- 26.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 5.56     |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 0.407    |
|    critic_loss     | 0.865    |
|    learning_rate   | 0.0005   |
|    n_updates       | 124999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.71    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 15       |
|    time_elapsed    | 8611     |
|    total timesteps | 136068   |
| train/             |          |
|    actor_loss      | 0.204    |
|    critic_loss     | 0.786    |
|    learning_rate   | 0.0005   |
|    n_updates       | 131067   |
---------------------------------
Eval num_timesteps=140000, episode_reward=12.87 +/- 29.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 0.294    |
|    critic_loss     | 0.425    |
|    learning_rate   | 0.0005   |
|    n_updates       | 134999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.1     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 15       |
|    time_elapsed    | 9150     |
|    total timesteps | 144072   |
| train/             |          |
|    actor_loss      | 0.304    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 139071   |
---------------------------------
Eval num_timesteps=150000, episode_reward=11.68 +/- 45.69
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 0.182    |
|    critic_loss     | 0.817    |
|    learning_rate   | 0.0005   |
|    n_updates       | 144999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.23    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 15       |
|    time_elapsed    | 9690     |
|    total timesteps | 152076   |
| train/             |          |
|    actor_loss      | 0.239    |
|    critic_loss     | 0.792    |
|    learning_rate   | 0.0005   |
|    n_updates       | 147075   |
---------------------------------
Eval num_timesteps=160000, episode_reward=41.53 +/- 9.87
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 41.5     |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 0.168    |
|    critic_loss     | 2.81     |
|    learning_rate   | 0.0005   |
|    n_updates       | 154999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.24    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 15       |
|    time_elapsed    | 10236    |
|    total timesteps | 160080   |
| train/             |          |
|    actor_loss      | 0.135    |
|    critic_loss     | 0.0318   |
|    learning_rate   | 0.0005   |
|    n_updates       | 155079   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 84       |
|    fps             | 15       |
|    time_elapsed    | 10668    |
|    total timesteps | 168084   |
| train/             |          |
|    actor_loss      | 0.122    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 163083   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-1.32 +/- 16.36
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.32    |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 0.203    |
|    critic_loss     | 1.19     |
|    learning_rate   | 0.0005   |
|    n_updates       | 164999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.05    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 15       |
|    time_elapsed    | 11213    |
|    total timesteps | 176088   |
| train/             |          |
|    actor_loss      | 0.0774   |
|    critic_loss     | 0.83     |
|    learning_rate   | 0.0005   |
|    n_updates       | 171087   |
---------------------------------
Eval num_timesteps=180000, episode_reward=1.79 +/- 39.62
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 1.79     |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 0.262    |
|    critic_loss     | 0.832    |
|    learning_rate   | 0.0005   |
|    n_updates       | 174999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.96    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 15       |
|    time_elapsed    | 11755    |
|    total timesteps | 184092   |
| train/             |          |
|    actor_loss      | 0.124    |
|    critic_loss     | 0.419    |
|    learning_rate   | 0.0005   |
|    n_updates       | 179091   |
---------------------------------
Eval num_timesteps=190000, episode_reward=18.36 +/- 32.43
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.0005   |
|    n_updates       | 184999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.87    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 15       |
|    time_elapsed    | 12299    |
|    total timesteps | 192096   |
| train/             |          |
|    actor_loss      | 0.177    |
|    critic_loss     | 0.0267   |
|    learning_rate   | 0.0005   |
|    n_updates       | 187095   |
---------------------------------
Eval num_timesteps=200000, episode_reward=2.23 +/- 21.25
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.23     |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 0.154    |
|    critic_loss     | 0.368    |
|    learning_rate   | 0.0005   |
|    n_updates       | 194999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.58    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 15       |
|    time_elapsed    | 12847    |
|    total timesteps | 200100   |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 0.819    |
|    learning_rate   | 0.0005   |
|    n_updates       | 195099   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.98    |
| time/              |          |
|    episodes        | 104      |
|    fps             | 15       |
|    time_elapsed    | 13277    |
|    total timesteps | 208104   |
| train/             |          |
|    actor_loss      | 0.161    |
|    critic_loss     | 1.92     |
|    learning_rate   | 0.0005   |
|    n_updates       | 203103   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-3.41 +/- 14.25
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -3.41    |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | 0.0874   |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.0005   |
|    n_updates       | 204999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.22    |
| time/              |          |
|    episodes        | 108      |
|    fps             | 15       |
|    time_elapsed    | 13817    |
|    total timesteps | 216108   |
| train/             |          |
|    actor_loss      | 0.305    |
|    critic_loss     | 1.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 211107   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-10.53 +/- 26.86
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.5    |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 0.228    |
|    critic_loss     | 0.421    |
|    learning_rate   | 0.0005   |
|    n_updates       | 214999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.09    |
| time/              |          |
|    episodes        | 112      |
|    fps             | 15       |
|    time_elapsed    | 14365    |
|    total timesteps | 224112   |
| train/             |          |
|    actor_loss      | 0.281    |
|    critic_loss     | 0.81     |
|    learning_rate   | 0.0005   |
|    n_updates       | 219111   |
---------------------------------
Eval num_timesteps=230000, episode_reward=-3.04 +/- 34.71
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -3.04    |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 0.182    |
|    critic_loss     | 0.418    |
|    learning_rate   | 0.0005   |
|    n_updates       | 224999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.71    |
| time/              |          |
|    episodes        | 116      |
|    fps             | 15       |
|    time_elapsed    | 14915    |
|    total timesteps | 232116   |
| train/             |          |
|    actor_loss      | 0.117    |
|    critic_loss     | 1.68     |
|    learning_rate   | 0.0005   |
|    n_updates       | 227115   |
---------------------------------
Eval num_timesteps=240000, episode_reward=27.70 +/- 48.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 27.7     |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.211    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 234999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5       |
| time/              |          |
|    episodes        | 120      |
|    fps             | 15       |
|    time_elapsed    | 15460    |
|    total timesteps | 240120   |
| train/             |          |
|    actor_loss      | 0.23     |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 235119   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.26    |
| time/              |          |
|    episodes        | 124      |
|    fps             | 15       |
|    time_elapsed    | 15889    |
|    total timesteps | 248124   |
| train/             |          |
|    actor_loss      | 0.405    |
|    critic_loss     | 0.502    |
|    learning_rate   | 0.0005   |
|    n_updates       | 243123   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-8.25 +/- 18.65
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -8.25    |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | 0.245    |
|    critic_loss     | 1.16     |
|    learning_rate   | 0.0005   |
|    n_updates       | 244999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.63    |
| time/              |          |
|    episodes        | 128      |
|    fps             | 15       |
|    time_elapsed    | 16431    |
|    total timesteps | 256128   |
| train/             |          |
|    actor_loss      | 0.173    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 251127   |
---------------------------------
Eval num_timesteps=260000, episode_reward=11.06 +/- 37.20
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 0.0595   |
|    critic_loss     | 0.0595   |
|    learning_rate   | 0.0005   |
|    n_updates       | 254999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.29    |
| time/              |          |
|    episodes        | 132      |
|    fps             | 15       |
|    time_elapsed    | 16977    |
|    total timesteps | 264132   |
| train/             |          |
|    actor_loss      | 0.355    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 259131   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-5.53 +/- 23.55
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -5.53    |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | 0.158    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 264999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.33    |
| time/              |          |
|    episodes        | 136      |
|    fps             | 15       |
|    time_elapsed    | 17517    |
|    total timesteps | 272136   |
| train/             |          |
|    actor_loss      | 0.273    |
|    critic_loss     | 2.74     |
|    learning_rate   | 0.0005   |
|    n_updates       | 267135   |
---------------------------------
Eval num_timesteps=280000, episode_reward=24.80 +/- 28.97
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 24.8     |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 0.213    |
|    critic_loss     | 0.422    |
|    learning_rate   | 0.0005   |
|    n_updates       | 274999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.58    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 15       |
|    time_elapsed    | 18054    |
|    total timesteps | 280140   |
| train/             |          |
|    actor_loss      | 0.0186   |
|    critic_loss     | 0.059    |
|    learning_rate   | 0.0005   |
|    n_updates       | 275139   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.25    |
| time/              |          |
|    episodes        | 144      |
|    fps             | 15       |
|    time_elapsed    | 18481    |
|    total timesteps | 288144   |
| train/             |          |
|    actor_loss      | 0.216    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 283143   |
---------------------------------
Eval num_timesteps=290000, episode_reward=15.64 +/- 19.14
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | 0.288    |
|    critic_loss     | 1.19     |
|    learning_rate   | 0.0005   |
|    n_updates       | 284999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.52    |
| time/              |          |
|    episodes        | 148      |
|    fps             | 15       |
|    time_elapsed    | 19019    |
|    total timesteps | 296148   |
| train/             |          |
|    actor_loss      | 0.125    |
|    critic_loss     | 0.434    |
|    learning_rate   | 0.0005   |
|    n_updates       | 291147   |
---------------------------------
Eval num_timesteps=300000, episode_reward=2.52 +/- 13.41
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 0.176    |
|    critic_loss     | 0.809    |
|    learning_rate   | 0.0005   |
|    n_updates       | 294999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.74    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 15       |
|    time_elapsed    | 19564    |
|    total timesteps | 304152   |
| train/             |          |
|    actor_loss      | 0.338    |
|    critic_loss     | 1.62     |
|    learning_rate   | 0.0005   |
|    n_updates       | 299151   |
---------------------------------
Eval num_timesteps=310000, episode_reward=11.91 +/- 18.83
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | 0.254    |
|    critic_loss     | 0.807    |
|    learning_rate   | 0.0005   |
|    n_updates       | 304999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.18    |
| time/              |          |
|    episodes        | 156      |
|    fps             | 15       |
|    time_elapsed    | 20110    |
|    total timesteps | 312156   |
| train/             |          |
|    actor_loss      | 0.386    |
|    critic_loss     | 0.505    |
|    learning_rate   | 0.0005   |
|    n_updates       | 307155   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-2.69 +/- 21.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.69    |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 0.251    |
|    critic_loss     | 1.18     |
|    learning_rate   | 0.0005   |
|    n_updates       | 314999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.12    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 15       |
|    time_elapsed    | 20655    |
|    total timesteps | 320160   |
| train/             |          |
|    actor_loss      | 0.226    |
|    critic_loss     | 0.416    |
|    learning_rate   | 0.0005   |
|    n_updates       | 315159   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.6     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 15       |
|    time_elapsed    | 21085    |
|    total timesteps | 328164   |
| train/             |          |
|    actor_loss      | 0.165    |
|    critic_loss     | 0.775    |
|    learning_rate   | 0.0005   |
|    n_updates       | 323163   |
---------------------------------
Eval num_timesteps=330000, episode_reward=2.94 +/- 48.37
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | 0.319    |
|    critic_loss     | 2.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 324999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.45    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 15       |
|    time_elapsed    | 21627    |
|    total timesteps | 336168   |
| train/             |          |
|    actor_loss      | 0.293    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 331167   |
---------------------------------
Eval num_timesteps=340000, episode_reward=21.44 +/- 11.68
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 21.4     |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | 0.173    |
|    critic_loss     | 1.98     |
|    learning_rate   | 0.0005   |
|    n_updates       | 334999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.29    |
| time/              |          |
|    episodes        | 172      |
|    fps             | 15       |
|    time_elapsed    | 22163    |
|    total timesteps | 344013   |
| train/             |          |
|    actor_loss      | 0.253    |
|    critic_loss     | 0.472    |
|    learning_rate   | 0.0005   |
|    n_updates       | 339012   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-12.57 +/- 7.20
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -12.6    |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | 0.155    |
|    critic_loss     | 0.417    |
|    learning_rate   | 0.0005   |
|    n_updates       | 344999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -8.84    |
| time/              |          |
|    episodes        | 176      |
|    fps             | 15       |
|    time_elapsed    | 22638    |
|    total timesteps | 350790   |
| train/             |          |
|    actor_loss      | 0.253    |
|    critic_loss     | 1.17     |
|    learning_rate   | 0.0005   |
|    n_updates       | 345789   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -9.39    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 15       |
|    time_elapsed    | 23070    |
|    total timesteps | 358794   |
| train/             |          |
|    actor_loss      | 0.316    |
|    critic_loss     | 0.409    |
|    learning_rate   | 0.0005   |
|    n_updates       | 353793   |
---------------------------------
Eval num_timesteps=360000, episode_reward=4.75 +/- 26.92
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 4.75     |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | 0.17     |
|    critic_loss     | 0.854    |
|    learning_rate   | 0.0005   |
|    n_updates       | 354999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -12.1    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 15       |
|    time_elapsed    | 23611    |
|    total timesteps | 366798   |
| train/             |          |
|    actor_loss      | 0.232    |
|    critic_loss     | 0.833    |
|    learning_rate   | 0.0005   |
|    n_updates       | 361797   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-14.27 +/- 19.63
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.3    |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | 0.301    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 364999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -13.7    |
| time/              |          |
|    episodes        | 188      |
|    fps             | 15       |
|    time_elapsed    | 24152    |
|    total timesteps | 374802   |
| train/             |          |
|    actor_loss      | 0.308    |
|    critic_loss     | 0.0458   |
|    learning_rate   | 0.0005   |
|    n_updates       | 369801   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-17.33 +/- 27.10
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -17.3    |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | 0.318    |
|    critic_loss     | 1.96     |
|    learning_rate   | 0.0005   |
|    n_updates       | 374999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    episodes        | 192      |
|    fps             | 15       |
|    time_elapsed    | 24699    |
|    total timesteps | 382806   |
| train/             |          |
|    actor_loss      | 0.256    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 377805   |
---------------------------------
Eval num_timesteps=390000, episode_reward=-25.28 +/- 43.91
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -25.3    |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | 0.289    |
|    critic_loss     | 0.422    |
|    learning_rate   | 0.0005   |
|    n_updates       | 384999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -11      |
| time/              |          |
|    episodes        | 196      |
|    fps             | 15       |
|    time_elapsed    | 25254    |
|    total timesteps | 390810   |
| train/             |          |
|    actor_loss      | 0.282    |
|    critic_loss     | 0.0353   |
|    learning_rate   | 0.0005   |
|    n_updates       | 385809   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    episodes        | 200      |
|    fps             | 15       |
|    time_elapsed    | 25683    |
|    total timesteps | 398814   |
| train/             |          |
|    actor_loss      | 0.138    |
|    critic_loss     | 0.465    |
|    learning_rate   | 0.0005   |
|    n_updates       | 393813   |
---------------------------------
Eval num_timesteps=400000, episode_reward=21.54 +/- 17.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 21.5     |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | 0.314    |
|    critic_loss     | 0.446    |
|    learning_rate   | 0.0005   |
|    n_updates       | 394999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    episodes        | 204      |
|    fps             | 15       |
|    time_elapsed    | 26222    |
|    total timesteps | 406818   |
| train/             |          |
|    actor_loss      | 0.34     |
|    critic_loss     | 1.18     |
|    learning_rate   | 0.0005   |
|    n_updates       | 401817   |
---------------------------------
Eval num_timesteps=410000, episode_reward=4.81 +/- 16.76
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 4.81     |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | 0.301    |
|    critic_loss     | 1.16     |
|    learning_rate   | 0.0005   |
|    n_updates       | 404999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -12.3    |
| time/              |          |
|    episodes        | 208      |
|    fps             | 15       |
|    time_elapsed    | 26772    |
|    total timesteps | 414822   |
| train/             |          |
|    actor_loss      | 0.255    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 409821   |
---------------------------------
Eval num_timesteps=420000, episode_reward=5.04 +/- 20.54
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 5.04     |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | 0.349    |
|    critic_loss     | 0.427    |
|    learning_rate   | 0.0005   |
|    n_updates       | 414999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 15       |
|    time_elapsed    | 27313    |
|    total timesteps | 422826   |
| train/             |          |
|    actor_loss      | 0.264    |
|    critic_loss     | 0.406    |
|    learning_rate   | 0.0005   |
|    n_updates       | 417825   |
---------------------------------
Eval num_timesteps=430000, episode_reward=5.13 +/- 27.14
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 5.13     |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | 0.163    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 424999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -8.79    |
| time/              |          |
|    episodes        | 216      |
|    fps             | 15       |
|    time_elapsed    | 27855    |
|    total timesteps | 430830   |
| train/             |          |
|    actor_loss      | 0.343    |
|    critic_loss     | 1.15     |
|    learning_rate   | 0.0005   |
|    n_updates       | 425829   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 15       |
|    time_elapsed    | 28283    |
|    total timesteps | 438834   |
| train/             |          |
|    actor_loss      | 0.393    |
|    critic_loss     | 0.0812   |
|    learning_rate   | 0.0005   |
|    n_updates       | 433833   |
---------------------------------
Eval num_timesteps=440000, episode_reward=33.16 +/- 30.72
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 33.2     |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | 0.296    |
|    critic_loss     | 1.61     |
|    learning_rate   | 0.0005   |
|    n_updates       | 434999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    episodes        | 224      |
|    fps             | 15       |
|    time_elapsed    | 28827    |
|    total timesteps | 446838   |
| train/             |          |
|    actor_loss      | 0.378    |
|    critic_loss     | 0.844    |
|    learning_rate   | 0.0005   |
|    n_updates       | 441837   |
---------------------------------
Terminated
2021-12-10 20:48:30.280480: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-10 20:48:30.280542: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp12/TD3_7
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -98.7    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 47       |
|    time_elapsed    | 168      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 1.48     |
|    critic_loss     | 2.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 2001     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-26.38 +/- 29.78
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -26.4    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.64     |
|    critic_loss     | 2.1      |
|    learning_rate   | 0.0005   |
|    n_updates       | 4002     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -67.3    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 23       |
|    time_elapsed    | 690      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 1.49     |
|    critic_loss     | 1.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 10005    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-52.96 +/- 32.83
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -53      |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 1.39     |
|    critic_loss     | 1.71     |
|    learning_rate   | 0.0005   |
|    n_updates       | 14007    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -37.2    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 19       |
|    time_elapsed    | 1219     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 1.27     |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.0005   |
|    n_updates       | 18009    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-32.46 +/- 44.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -32.5    |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 1.07     |
|    critic_loss     | 1.4      |
|    learning_rate   | 0.0005   |
|    n_updates       | 24012    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -19      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 18       |
|    time_elapsed    | 1748     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 1        |
|    critic_loss     | 1.31     |
|    learning_rate   | 0.0005   |
|    n_updates       | 26013    |
---------------------------------
Eval num_timesteps=40000, episode_reward=2.98 +/- 32.15
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.732    |
|    critic_loss     | 1.17     |
|    learning_rate   | 0.0005   |
|    n_updates       | 34017    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.13    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 17       |
|    time_elapsed    | 2278     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.22    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 17       |
|    time_elapsed    | 2692     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.522    |
|    critic_loss     | 1.09     |
|    learning_rate   | 0.0005   |
|    n_updates       | 42021    |
---------------------------------
Eval num_timesteps=50000, episode_reward=1.74 +/- 36.94
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.489    |
|    critic_loss     | 1.11     |
|    learning_rate   | 0.0005   |
|    n_updates       | 44022    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.3     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 17       |
|    time_elapsed    | 3215     |
|    total timesteps | 55914    |
| train/             |          |
|    actor_loss      | 0.392    |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.0005   |
|    n_updates       | 49911    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-43.01 +/- 30.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -43      |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.343    |
|    critic_loss     | 1.11     |
|    learning_rate   | 0.0005   |
|    n_updates       | 55914    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.96    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 17       |
|    time_elapsed    | 3745     |
|    total timesteps | 63918    |
| train/             |          |
|    actor_loss      | 0.335    |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.0005   |
|    n_updates       | 57915    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-10.24 +/- 29.63
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.2    |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 0.283    |
|    critic_loss     | 1.07     |
|    learning_rate   | 0.0005   |
|    n_updates       | 65743    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -4.71    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 16       |
|    time_elapsed    | 4264     |
|    total timesteps | 71746    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -0.948   |
| time/              |          |
|    episodes        | 40       |
|    fps             | 17       |
|    time_elapsed    | 4676     |
|    total timesteps | 79750    |
| train/             |          |
|    actor_loss      | 0.271    |
|    critic_loss     | 1.07     |
|    learning_rate   | 0.0005   |
|    n_updates       | 73747    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-43.43 +/- 33.51
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -43.4    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.263    |
|    critic_loss     | 1.05     |
|    learning_rate   | 0.0005   |
|    n_updates       | 75748    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.12    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 16       |
|    time_elapsed    | 5199     |
|    total timesteps | 87754    |
| train/             |          |
|    actor_loss      | 0.226    |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.0005   |
|    n_updates       | 81751    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-29.27 +/- 13.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29.3    |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 0.213    |
|    critic_loss     | 1.02     |
|    learning_rate   | 0.0005   |
|    n_updates       | 85753    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.58     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 16       |
|    time_elapsed    | 5720     |
|    total timesteps | 95758    |
| train/             |          |
|    actor_loss      | 0.185    |
|    critic_loss     | 0.984    |
|    learning_rate   | 0.0005   |
|    n_updates       | 89755    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-12.53 +/- 14.10
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -12.5    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.154    |
|    critic_loss     | 0.981    |
|    learning_rate   | 0.0005   |
|    n_updates       | 95758    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 16       |
|    time_elapsed    | 6248     |
|    total timesteps | 103762   |
| train/             |          |
|    actor_loss      | 0.146    |
|    critic_loss     | 0.997    |
|    learning_rate   | 0.0005   |
|    n_updates       | 97759    |
---------------------------------
Eval num_timesteps=110000, episode_reward=-23.67 +/- 11.69
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.7    |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 0.136    |
|    critic_loss     | 0.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 105763   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.31     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 16       |
|    time_elapsed    | 6770     |
|    total timesteps | 111766   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.46     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 16       |
|    time_elapsed    | 7164     |
|    total timesteps | 119402   |
| train/             |          |
|    actor_loss      | 0.115    |
|    critic_loss     | 0.984    |
|    learning_rate   | 0.0005   |
|    n_updates       | 113399   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-33.51 +/- 30.50
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -33.5    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 0.105    |
|    critic_loss     | 0.973    |
|    learning_rate   | 0.0005   |
|    n_updates       | 115400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.2      |
| time/              |          |
|    episodes        | 64       |
|    fps             | 16       |
|    time_elapsed    | 7689     |
|    total timesteps | 127406   |
| train/             |          |
|    actor_loss      | 0.0933   |
|    critic_loss     | 0.987    |
|    learning_rate   | 0.0005   |
|    n_updates       | 121403   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-13.17 +/- 38.38
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -13.2    |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 0.0832   |
|    critic_loss     | 0.959    |
|    learning_rate   | 0.0005   |
|    n_updates       | 125405   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.1      |
| time/              |          |
|    episodes        | 68       |
|    fps             | 16       |
|    time_elapsed    | 8216     |
|    total timesteps | 135410   |
| train/             |          |
|    actor_loss      | 0.0639   |
|    critic_loss     | 0.959    |
|    learning_rate   | 0.0005   |
|    n_updates       | 129407   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-23.83 +/- 21.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.8    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 0.0522   |
|    critic_loss     | 0.964    |
|    learning_rate   | 0.0005   |
|    n_updates       | 135410   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.75     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 16       |
|    time_elapsed    | 8744     |
|    total timesteps | 143414   |
| train/             |          |
|    actor_loss      | 0.0547   |
|    critic_loss     | 0.981    |
|    learning_rate   | 0.0005   |
|    n_updates       | 137411   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-44.46 +/- 27.99
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -44.5    |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 0.0584   |
|    critic_loss     | 0.974    |
|    learning_rate   | 0.0005   |
|    n_updates       | 145415   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.84     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 16       |
|    time_elapsed    | 9273     |
|    total timesteps | 151418   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 0.416    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 16       |
|    time_elapsed    | 9686     |
|    total timesteps | 159422   |
| train/             |          |
|    actor_loss      | 0.0884   |
|    critic_loss     | 0.96     |
|    learning_rate   | 0.0005   |
|    n_updates       | 153419   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-8.92 +/- 16.88
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -8.92    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 0.0977   |
|    critic_loss     | 0.989    |
|    learning_rate   | 0.0005   |
|    n_updates       | 155420   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.23     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 16       |
|    time_elapsed    | 10212    |
|    total timesteps | 167426   |
| train/             |          |
|    actor_loss      | 0.11     |
|    critic_loss     | 0.987    |
|    learning_rate   | 0.0005   |
|    n_updates       | 161423   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-28.36 +/- 9.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -28.4    |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 0.117    |
|    critic_loss     | 0.968    |
|    learning_rate   | 0.0005   |
|    n_updates       | 165425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 16       |
|    time_elapsed    | 10736    |
|    total timesteps | 175430   |
| train/             |          |
|    actor_loss      | 0.109    |
|    critic_loss     | 0.945    |
|    learning_rate   | 0.0005   |
|    n_updates       | 169427   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-47.40 +/- 32.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -47.4    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 0.121    |
|    critic_loss     | 0.971    |
|    learning_rate   | 0.0005   |
|    n_updates       | 175430   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.14     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 16       |
|    time_elapsed    | 11261    |
|    total timesteps | 183434   |
| train/             |          |
|    actor_loss      | 0.124    |
|    critic_loss     | 0.939    |
|    learning_rate   | 0.0005   |
|    n_updates       | 177431   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-39.76 +/- 29.93
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -39.8    |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 0.107    |
|    critic_loss     | 0.954    |
|    learning_rate   | 0.0005   |
|    n_updates       | 185435   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.4      |
| time/              |          |
|    episodes        | 96       |
|    fps             | 16       |
|    time_elapsed    | 11788    |
|    total timesteps | 191438   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.34     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 16       |
|    time_elapsed    | 12201    |
|    total timesteps | 199442   |
| train/             |          |
|    actor_loss      | 0.114    |
|    critic_loss     | 0.964    |
|    learning_rate   | 0.0005   |
|    n_updates       | 193439   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-42.47 +/- 29.25
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -42.5    |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 0.114    |
|    critic_loss     | 0.941    |
|    learning_rate   | 0.0005   |
|    n_updates       | 195440   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.28     |
| time/              |          |
|    episodes        | 104      |
|    fps             | 16       |
|    time_elapsed    | 12729    |
|    total timesteps | 207446   |
| train/             |          |
|    actor_loss      | 0.121    |
|    critic_loss     | 0.966    |
|    learning_rate   | 0.0005   |
|    n_updates       | 201443   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-66.04 +/- 48.88
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -66      |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | 0.124    |
|    critic_loss     | 0.983    |
|    learning_rate   | 0.0005   |
|    n_updates       | 205445   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.41     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 16       |
|    time_elapsed    | 13256    |
|    total timesteps | 215450   |
| train/             |          |
|    actor_loss      | 0.125    |
|    critic_loss     | 0.965    |
|    learning_rate   | 0.0005   |
|    n_updates       | 209447   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-19.07 +/- 18.77
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19.1    |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 0.123    |
|    critic_loss     | 0.953    |
|    learning_rate   | 0.0005   |
|    n_updates       | 215450   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.08     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 16       |
|    time_elapsed    | 13784    |
|    total timesteps | 223454   |
| train/             |          |
|    actor_loss      | 0.122    |
|    critic_loss     | 0.936    |
|    learning_rate   | 0.0005   |
|    n_updates       | 217451   |
---------------------------------
Eval num_timesteps=230000, episode_reward=-23.82 +/- 19.96
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.8    |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 0.133    |
|    critic_loss     | 0.961    |
|    learning_rate   | 0.0005   |
|    n_updates       | 225455   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 16       |
|    time_elapsed    | 14313    |
|    total timesteps | 231458   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -0.311   |
| time/              |          |
|    episodes        | 120      |
|    fps             | 16       |
|    time_elapsed    | 14729    |
|    total timesteps | 239462   |
| train/             |          |
|    actor_loss      | 0.157    |
|    critic_loss     | 0.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 233459   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-24.19 +/- 30.92
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -24.2    |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.169    |
|    critic_loss     | 0.974    |
|    learning_rate   | 0.0005   |
|    n_updates       | 235460   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -0.368   |
| time/              |          |
|    episodes        | 124      |
|    fps             | 16       |
|    time_elapsed    | 15252    |
|    total timesteps | 247466   |
| train/             |          |
|    actor_loss      | 0.173    |
|    critic_loss     | 0.993    |
|    learning_rate   | 0.0005   |
|    n_updates       | 241463   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-19.14 +/- 12.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19.1    |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | 0.17     |
|    critic_loss     | 0.963    |
|    learning_rate   | 0.0005   |
|    n_updates       | 245465   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.25     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 16       |
|    time_elapsed    | 15780    |
|    total timesteps | 255470   |
| train/             |          |
|    actor_loss      | 0.174    |
|    critic_loss     | 0.994    |
|    learning_rate   | 0.0005   |
|    n_updates       | 249467   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-17.86 +/- 34.81
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -17.9    |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 0.189    |
|    critic_loss     | 0.984    |
|    learning_rate   | 0.0005   |
|    n_updates       | 255470   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.022   |
| time/              |          |
|    episodes        | 132      |
|    fps             | 16       |
|    time_elapsed    | 16308    |
|    total timesteps | 263474   |
| train/             |          |
|    actor_loss      | 0.186    |
|    critic_loss     | 0.96     |
|    learning_rate   | 0.0005   |
|    n_updates       | 257471   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-20.61 +/- 30.86
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -20.6    |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | 0.177    |
|    critic_loss     | 0.958    |
|    learning_rate   | 0.0005   |
|    n_updates       | 265475   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.816   |
| time/              |          |
|    episodes        | 136      |
|    fps             | 16       |
|    time_elapsed    | 16837    |
|    total timesteps | 271478   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.09    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 16       |
|    time_elapsed    | 17252    |
|    total timesteps | 279482   |
| train/             |          |
|    actor_loss      | 0.185    |
|    critic_loss     | 0.966    |
|    learning_rate   | 0.0005   |
|    n_updates       | 273479   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-21.44 +/- 28.90
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -21.4    |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 0.19     |
|    critic_loss     | 0.963    |
|    learning_rate   | 0.0005   |
|    n_updates       | 275480   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.705   |
| time/              |          |
|    episodes        | 144      |
|    fps             | 16       |
|    time_elapsed    | 17798    |
|    total timesteps | 287486   |
| train/             |          |
|    actor_loss      | 0.188    |
|    critic_loss     | 0.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 281483   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-10.86 +/- 18.83
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.9    |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | 0.18     |
|    critic_loss     | 0.944    |
|    learning_rate   | 0.0005   |
|    n_updates       | 285485   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.82    |
| time/              |          |
|    episodes        | 148      |
|    fps             | 16       |
|    time_elapsed    | 18331    |
|    total timesteps | 295490   |
| train/             |          |
|    actor_loss      | 0.181    |
|    critic_loss     | 0.957    |
|    learning_rate   | 0.0005   |
|    n_updates       | 289487   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-15.29 +/- 35.70
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -15.3    |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 0.179    |
|    critic_loss     | 0.966    |
|    learning_rate   | 0.0005   |
|    n_updates       | 295490   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.39    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 16       |
|    time_elapsed    | 18860    |
|    total timesteps | 303494   |
| train/             |          |
|    actor_loss      | 0.183    |
|    critic_loss     | 0.951    |
|    learning_rate   | 0.0005   |
|    n_updates       | 297491   |
---------------------------------
Eval num_timesteps=310000, episode_reward=-46.92 +/- 16.87
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -46.9    |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | 0.172    |
|    critic_loss     | 0.941    |
|    learning_rate   | 0.0005   |
|    n_updates       | 305495   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.75    |
| time/              |          |
|    episodes        | 156      |
|    fps             | 16       |
|    time_elapsed    | 19385    |
|    total timesteps | 311498   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.19    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 16       |
|    time_elapsed    | 19797    |
|    total timesteps | 319502   |
| train/             |          |
|    actor_loss      | 0.187    |
|    critic_loss     | 0.938    |
|    learning_rate   | 0.0005   |
|    n_updates       | 313499   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-44.73 +/- 45.77
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -44.7    |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 0.184    |
|    critic_loss     | 0.947    |
|    learning_rate   | 0.0005   |
|    n_updates       | 315500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.46    |
| time/              |          |
|    episodes        | 164      |
|    fps             | 16       |
|    time_elapsed    | 20327    |
|    total timesteps | 327506   |
| train/             |          |
|    actor_loss      | 0.189    |
|    critic_loss     | 0.955    |
|    learning_rate   | 0.0005   |
|    n_updates       | 321503   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-4.51 +/- 13.40
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -4.51    |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | 0.193    |
|    critic_loss     | 0.934    |
|    learning_rate   | 0.0005   |
|    n_updates       | 325505   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.14    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 16       |
|    time_elapsed    | 20851    |
|    total timesteps | 335510   |
| train/             |          |
|    actor_loss      | 0.18     |
|    critic_loss     | 0.939    |
|    learning_rate   | 0.0005   |
|    n_updates       | 329507   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-28.16 +/- 29.89
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -28.2    |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | 0.184    |
|    critic_loss     | 0.942    |
|    learning_rate   | 0.0005   |
|    n_updates       | 335510   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.72    |
| time/              |          |
|    episodes        | 172      |
|    fps             | 16       |
|    time_elapsed    | 21375    |
|    total timesteps | 343514   |
| train/             |          |
|    actor_loss      | 0.187    |
|    critic_loss     | 0.976    |
|    learning_rate   | 0.0005   |
|    n_updates       | 337511   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-24.34 +/- 19.03
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -24.3    |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | 0.195    |
|    critic_loss     | 0.942    |
|    learning_rate   | 0.0005   |
|    n_updates       | 345515   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.57    |
| time/              |          |
|    episodes        | 176      |
|    fps             | 16       |
|    time_elapsed    | 21899    |
|    total timesteps | 351518   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.08    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 16       |
|    time_elapsed    | 22310    |
|    total timesteps | 359522   |
| train/             |          |
|    actor_loss      | 0.21     |
|    critic_loss     | 0.946    |
|    learning_rate   | 0.0005   |
|    n_updates       | 353519   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-29.39 +/- 25.42
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29.4    |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | 0.205    |
|    critic_loss     | 0.928    |
|    learning_rate   | 0.0005   |
|    n_updates       | 355520   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.19    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 16       |
|    time_elapsed    | 22837    |
|    total timesteps | 367526   |
| train/             |          |
|    actor_loss      | 0.214    |
|    critic_loss     | 0.947    |
|    learning_rate   | 0.0005   |
|    n_updates       | 361523   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-22.85 +/- 40.12
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -22.9    |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | 0.218    |
|    critic_loss     | 0.945    |
|    learning_rate   | 0.0005   |
|    n_updates       | 365525   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.5     |
| time/              |          |
|    episodes        | 188      |
|    fps             | 16       |
|    time_elapsed    | 23360    |
|    total timesteps | 375530   |
| train/             |          |
|    actor_loss      | 0.202    |
|    critic_loss     | 0.947    |
|    learning_rate   | 0.0005   |
|    n_updates       | 369527   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-0.73 +/- 18.04
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -0.727   |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | 0.204    |
|    critic_loss     | 0.928    |
|    learning_rate   | 0.0005   |
|    n_updates       | 375530   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.21    |
| time/              |          |
|    episodes        | 192      |
|    fps             | 16       |
|    time_elapsed    | 23883    |
|    total timesteps | 383534   |
| train/             |          |
|    actor_loss      | 0.201    |
|    critic_loss     | 0.926    |
|    learning_rate   | 0.0005   |
|    n_updates       | 377531   |
---------------------------------
Eval num_timesteps=390000, episode_reward=0.89 +/- 36.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 0.893    |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | 0.213    |
|    critic_loss     | 0.929    |
|    learning_rate   | 0.0005   |
|    n_updates       | 385535   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.66    |
| time/              |          |
|    episodes        | 196      |
|    fps             | 16       |
|    time_elapsed    | 24407    |
|    total timesteps | 391538   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.37    |
| time/              |          |
|    episodes        | 200      |
|    fps             | 16       |
|    time_elapsed    | 24819    |
|    total timesteps | 399542   |
| train/             |          |
|    actor_loss      | 0.196    |
|    critic_loss     | 0.942    |
|    learning_rate   | 0.0005   |
|    n_updates       | 393539   |
---------------------------------
Eval num_timesteps=400000, episode_reward=-38.58 +/- 16.99
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -38.6    |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | 0.206    |
|    critic_loss     | 0.961    |
|    learning_rate   | 0.0005   |
|    n_updates       | 395540   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.25    |
| time/              |          |
|    episodes        | 204      |
|    fps             | 16       |
|    time_elapsed    | 25344    |
|    total timesteps | 407546   |
| train/             |          |
|    actor_loss      | 0.208    |
|    critic_loss     | 0.925    |
|    learning_rate   | 0.0005   |
|    n_updates       | 401543   |
---------------------------------
Eval num_timesteps=410000, episode_reward=-28.67 +/- 36.63
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -28.7    |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | 0.212    |
|    critic_loss     | 0.963    |
|    learning_rate   | 0.0005   |
|    n_updates       | 405545   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.05    |
| time/              |          |
|    episodes        | 208      |
|    fps             | 16       |
|    time_elapsed    | 25868    |
|    total timesteps | 415550   |
| train/             |          |
|    actor_loss      | 0.221    |
|    critic_loss     | 0.952    |
|    learning_rate   | 0.0005   |
|    n_updates       | 409547   |
---------------------------------
Eval num_timesteps=420000, episode_reward=-22.52 +/- 34.54
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -22.5    |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | 0.231    |
|    critic_loss     | 0.969    |
|    learning_rate   | 0.0005   |
|    n_updates       | 415550   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.17    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 16       |
|    time_elapsed    | 26391    |
|    total timesteps | 423554   |
| train/             |          |
|    actor_loss      | 0.232    |
|    critic_loss     | 0.929    |
|    learning_rate   | 0.0005   |
|    n_updates       | 417551   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-3.89 +/- 17.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -3.89    |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | 0.252    |
|    critic_loss     | 0.952    |
|    learning_rate   | 0.0005   |
|    n_updates       | 425555   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.14    |
| time/              |          |
|    episodes        | 216      |
|    fps             | 16       |
|    time_elapsed    | 26915    |
|    total timesteps | 431558   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.64    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 16       |
|    time_elapsed    | 27327    |
|    total timesteps | 439562   |
| train/             |          |
|    actor_loss      | 0.251    |
|    critic_loss     | 0.936    |
|    learning_rate   | 0.0005   |
|    n_updates       | 433559   |
---------------------------------
Eval num_timesteps=440000, episode_reward=-10.67 +/- 29.17
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.7    |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | 0.246    |
|    critic_loss     | 0.956    |
|    learning_rate   | 0.0005   |
|    n_updates       | 435560   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.99    |
| time/              |          |
|    episodes        | 224      |
|    fps             | 16       |
|    time_elapsed    | 27851    |
|    total timesteps | 447566   |
| train/             |          |
|    actor_loss      | 0.242    |
|    critic_loss     | 0.942    |
|    learning_rate   | 0.0005   |
|    n_updates       | 441563   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-19.23 +/- 20.44
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19.2    |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | 0.238    |
|    critic_loss     | 0.921    |
|    learning_rate   | 0.0005   |
|    n_updates       | 445565   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -5.68    |
| time/              |          |
|    episodes        | 228      |
|    fps             | 16       |
|    time_elapsed    | 28339    |
|    total timesteps | 454831   |
| train/             |          |
|    actor_loss      | 0.232    |
|    critic_loss     | 0.938    |
|    learning_rate   | 0.0005   |
|    n_updates       | 448828   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-6.93 +/- 15.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -6.93    |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | 0.231    |
|    critic_loss     | 0.959    |
|    learning_rate   | 0.0005   |
|    n_updates       | 454831   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -3.7     |
| time/              |          |
|    episodes        | 232      |
|    fps             | 16       |
|    time_elapsed    | 28865    |
|    total timesteps | 462835   |
| train/             |          |
|    actor_loss      | 0.237    |
|    critic_loss     | 0.931    |
|    learning_rate   | 0.0005   |
|    n_updates       | 456832   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-29.26 +/- 27.20
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29.3    |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | 0.224    |
|    critic_loss     | 0.95     |
|    learning_rate   | 0.0005   |
|    n_updates       | 464836   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -0.779   |
| time/              |          |
|    episodes        | 236      |
|    fps             | 16       |
|    time_elapsed    | 29388    |
|    total timesteps | 470839   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -2.05    |
| time/              |          |
|    episodes        | 240      |
|    fps             | 16       |
|    time_elapsed    | 29800    |
|    total timesteps | 478843   |
| train/             |          |
|    actor_loss      | 0.213    |
|    critic_loss     | 0.942    |
|    learning_rate   | 0.0005   |
|    n_updates       | 472840   |
---------------------------------
Eval num_timesteps=480000, episode_reward=-30.07 +/- 19.37
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -30.1    |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | 0.214    |
|    critic_loss     | 0.936    |
|    learning_rate   | 0.0005   |
|    n_updates       | 474841   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -2.11    |
| time/              |          |
|    episodes        | 244      |
|    fps             | 16       |
|    time_elapsed    | 30323    |
|    total timesteps | 486847   |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 0.953    |
|    learning_rate   | 0.0005   |
|    n_updates       | 480844   |
---------------------------------
Eval num_timesteps=490000, episode_reward=-16.42 +/- 14.03
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -16.4    |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | 0.203    |
|    critic_loss     | 0.933    |
|    learning_rate   | 0.0005   |
|    n_updates       | 484846   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -0.915   |
| time/              |          |
|    episodes        | 248      |
|    fps             | 16       |
|    time_elapsed    | 30846    |
|    total timesteps | 494851   |
| train/             |          |
|    actor_loss      | 0.196    |
|    critic_loss     | 0.94     |
|    learning_rate   | 0.0005   |
|    n_updates       | 488848   |
---------------------------------
Eval num_timesteps=500000, episode_reward=-28.38 +/- 36.81
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -28.4    |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | 0.203    |
|    critic_loss     | 0.938    |
|    learning_rate   | 0.0005   |
|    n_updates       | 494851   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.31    |
| time/              |          |
|    episodes        | 252      |
|    fps             | 16       |
|    time_elapsed    | 31373    |
|    total timesteps | 502855   |
| train/             |          |
|    actor_loss      | 0.198    |
|    critic_loss     | 0.941    |
|    learning_rate   | 0.0005   |
|    n_updates       | 496852   |
---------------------------------
Eval num_timesteps=510000, episode_reward=-9.11 +/- 29.45
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -9.11    |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | 0.2      |
|    critic_loss     | 0.921    |
|    learning_rate   | 0.0005   |
|    n_updates       | 504856   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.11    |
| time/              |          |
|    episodes        | 256      |
|    fps             | 16       |
|    time_elapsed    | 31895    |
|    total timesteps | 510859   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 0.655    |
| time/              |          |
|    episodes        | 260      |
|    fps             | 16       |
|    time_elapsed    | 32305    |
|    total timesteps | 518863   |
| train/             |          |
|    actor_loss      | 0.197    |
|    critic_loss     | 0.919    |
|    learning_rate   | 0.0005   |
|    n_updates       | 512860   |
---------------------------------
Eval num_timesteps=520000, episode_reward=5.28 +/- 29.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 5.28     |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | 0.198    |
|    critic_loss     | 0.923    |
|    learning_rate   | 0.0005   |
|    n_updates       | 514861   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 0.698    |
| time/              |          |
|    episodes        | 264      |
|    fps             | 16       |
|    time_elapsed    | 32830    |
|    total timesteps | 526867   |
| train/             |          |
|    actor_loss      | 0.206    |
|    critic_loss     | 0.922    |
|    learning_rate   | 0.0005   |
|    n_updates       | 520864   |
---------------------------------
Eval num_timesteps=530000, episode_reward=-44.95 +/- 48.61
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -44.9    |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | 0.201    |
|    critic_loss     | 0.913    |
|    learning_rate   | 0.0005   |
|    n_updates       | 524866   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.63     |
| time/              |          |
|    episodes        | 268      |
|    fps             | 16       |
|    time_elapsed    | 33354    |
|    total timesteps | 534871   |
| train/             |          |
|    actor_loss      | 0.189    |
|    critic_loss     | 0.939    |
|    learning_rate   | 0.0005   |
|    n_updates       | 528868   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-17.58 +/- 20.92
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -17.6    |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | 0.187    |
|    critic_loss     | 0.912    |
|    learning_rate   | 0.0005   |
|    n_updates       | 534871   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.04     |
| time/              |          |
|    episodes        | 272      |
|    fps             | 16       |
|    time_elapsed    | 33880    |
|    total timesteps | 542875   |
| train/             |          |
|    actor_loss      | 0.186    |
|    critic_loss     | 0.905    |
|    learning_rate   | 0.0005   |
|    n_updates       | 536872   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-31.29 +/- 28.70
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -31.3    |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | 0.176    |
|    critic_loss     | 0.933    |
|    learning_rate   | 0.0005   |
|    n_updates       | 544876   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.33     |
| time/              |          |
|    episodes        | 276      |
|    fps             | 16       |
|    time_elapsed    | 34404    |
|    total timesteps | 550879   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.52     |
| time/              |          |
|    episodes        | 280      |
|    fps             | 16       |
|    time_elapsed    | 34816    |
|    total timesteps | 558883   |
| train/             |          |
|    actor_loss      | 0.171    |
|    critic_loss     | 0.913    |
|    learning_rate   | 0.0005   |
|    n_updates       | 552880   |
---------------------------------
Eval num_timesteps=560000, episode_reward=-23.97 +/- 35.89
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -24      |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | 0.174    |
|    critic_loss     | 0.916    |
|    learning_rate   | 0.0005   |
|    n_updates       | 554881   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.63     |
| time/              |          |
|    episodes        | 284      |
|    fps             | 16       |
|    time_elapsed    | 35300    |
|    total timesteps | 566117   |
| train/             |          |
|    actor_loss      | 0.175    |
|    critic_loss     | 0.919    |
|    learning_rate   | 0.0005   |
|    n_updates       | 560114   |
---------------------------------
Eval num_timesteps=570000, episode_reward=-23.84 +/- 37.93
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.8    |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | 0.18     |
|    critic_loss     | 0.901    |
|    learning_rate   | 0.0005   |
|    n_updates       | 564116   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.94     |
| time/              |          |
|    episodes        | 288      |
|    fps             | 16       |
|    time_elapsed    | 35825    |
|    total timesteps | 574121   |
| train/             |          |
|    actor_loss      | 0.189    |
|    critic_loss     | 0.932    |
|    learning_rate   | 0.0005   |
|    n_updates       | 568118   |
---------------------------------
Eval num_timesteps=580000, episode_reward=-29.02 +/- 30.28
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29      |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | 0.176    |
|    critic_loss     | 0.921    |
|    learning_rate   | 0.0005   |
|    n_updates       | 574121   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.97     |
| time/              |          |
|    episodes        | 292      |
|    fps             | 16       |
|    time_elapsed    | 36356    |
|    total timesteps | 582125   |
| train/             |          |
|    actor_loss      | 0.18     |
|    critic_loss     | 0.909    |
|    learning_rate   | 0.0005   |
|    n_updates       | 576122   |
---------------------------------
Eval num_timesteps=590000, episode_reward=-18.28 +/- 26.72
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -18.3    |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | 0.2      |
|    critic_loss     | 0.918    |
|    learning_rate   | 0.0005   |
|    n_updates       | 584126   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.56     |
| time/              |          |
|    episodes        | 296      |
|    fps             | 15       |
|    time_elapsed    | 36883    |
|    total timesteps | 590129   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.83     |
| time/              |          |
|    episodes        | 300      |
|    fps             | 16       |
|    time_elapsed    | 37299    |
|    total timesteps | 598133   |
| train/             |          |
|    actor_loss      | 0.191    |
|    critic_loss     | 0.932    |
|    learning_rate   | 0.0005   |
|    n_updates       | 592130   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-34.36 +/- 36.94
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -34.4    |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | 0.19     |
|    critic_loss     | 0.912    |
|    learning_rate   | 0.0005   |
|    n_updates       | 594131   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 9.17     |
| time/              |          |
|    episodes        | 304      |
|    fps             | 16       |
|    time_elapsed    | 37824    |
|    total timesteps | 606137   |
| train/             |          |
|    actor_loss      | 0.188    |
|    critic_loss     | 0.897    |
|    learning_rate   | 0.0005   |
|    n_updates       | 600134   |
---------------------------------
Eval num_timesteps=610000, episode_reward=3.10 +/- 17.92
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | 0.18     |
|    critic_loss     | 0.902    |
|    learning_rate   | 0.0005   |
|    n_updates       | 604136   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 10.1     |
| time/              |          |
|    episodes        | 308      |
|    fps             | 16       |
|    time_elapsed    | 38346    |
|    total timesteps | 614141   |
| train/             |          |
|    actor_loss      | 0.178    |
|    critic_loss     | 0.916    |
|    learning_rate   | 0.0005   |
|    n_updates       | 608138   |
---------------------------------
Eval num_timesteps=620000, episode_reward=-2.35 +/- 28.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.35    |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | 0.167    |
|    critic_loss     | 0.896    |
|    learning_rate   | 0.0005   |
|    n_updates       | 614141   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 9.81     |
| time/              |          |
|    episodes        | 312      |
|    fps             | 16       |
|    time_elapsed    | 38874    |
|    total timesteps | 622145   |
| train/             |          |
|    actor_loss      | 0.16     |
|    critic_loss     | 0.896    |
|    learning_rate   | 0.0005   |
|    n_updates       | 616142   |
---------------------------------
Eval num_timesteps=630000, episode_reward=-17.11 +/- 17.07
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -17.1    |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | 0.161    |
|    critic_loss     | 0.92     |
|    learning_rate   | 0.0005   |
|    n_updates       | 624146   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 11.5     |
| time/              |          |
|    episodes        | 316      |
|    fps             | 15       |
|    time_elapsed    | 39411    |
|    total timesteps | 630149   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    episodes        | 320      |
|    fps             | 16       |
|    time_elapsed    | 39830    |
|    total timesteps | 638153   |
| train/             |          |
|    actor_loss      | 0.148    |
|    critic_loss     | 0.91     |
|    learning_rate   | 0.0005   |
|    n_updates       | 632150   |
---------------------------------
Eval num_timesteps=640000, episode_reward=-63.00 +/- 35.54
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -63      |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | 0.151    |
|    critic_loss     | 0.889    |
|    learning_rate   | 0.0005   |
|    n_updates       | 634151   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 9.22     |
| time/              |          |
|    episodes        | 324      |
|    fps             | 16       |
|    time_elapsed    | 40364    |
|    total timesteps | 646157   |
| train/             |          |
|    actor_loss      | 0.143    |
|    critic_loss     | 0.914    |
|    learning_rate   | 0.0005   |
|    n_updates       | 640154   |
---------------------------------
Eval num_timesteps=650000, episode_reward=-27.06 +/- 14.83
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27.1    |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | 0.147    |
|    critic_loss     | 0.903    |
|    learning_rate   | 0.0005   |
|    n_updates       | 644156   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 9.32     |
| time/              |          |
|    episodes        | 328      |
|    fps             | 15       |
|    time_elapsed    | 40896    |
|    total timesteps | 654161   |
| train/             |          |
|    actor_loss      | 0.154    |
|    critic_loss     | 0.898    |
|    learning_rate   | 0.0005   |
|    n_updates       | 648158   |
---------------------------------
Eval num_timesteps=660000, episode_reward=-28.99 +/- 36.17
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29      |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | 0.154    |
|    critic_loss     | 0.914    |
|    learning_rate   | 0.0005   |
|    n_updates       | 654161   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 7.12     |
| time/              |          |
|    episodes        | 332      |
|    fps             | 15       |
|    time_elapsed    | 41424    |
|    total timesteps | 662165   |
| train/             |          |
|    actor_loss      | 0.152    |
|    critic_loss     | 0.889    |
|    learning_rate   | 0.0005   |
|    n_updates       | 656162   |
---------------------------------
Eval num_timesteps=670000, episode_reward=-17.25 +/- 6.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -17.2    |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | 0.152    |
|    critic_loss     | 0.913    |
|    learning_rate   | 0.0005   |
|    n_updates       | 664166   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.1      |
| time/              |          |
|    episodes        | 336      |
|    fps             | 15       |
|    time_elapsed    | 41950    |
|    total timesteps | 670169   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.82     |
| time/              |          |
|    episodes        | 340      |
|    fps             | 16       |
|    time_elapsed    | 42368    |
|    total timesteps | 678173   |
| train/             |          |
|    actor_loss      | 0.153    |
|    critic_loss     | 0.925    |
|    learning_rate   | 0.0005   |
|    n_updates       | 672170   |
---------------------------------
Eval num_timesteps=680000, episode_reward=-30.54 +/- 19.63
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -30.5    |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | 0.152    |
|    critic_loss     | 0.938    |
|    learning_rate   | 0.0005   |
|    n_updates       | 675902   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.78     |
| time/              |          |
|    episodes        | 344      |
|    fps             | 15       |
|    time_elapsed    | 42882    |
|    total timesteps | 685907   |
| train/             |          |
|    actor_loss      | 0.162    |
|    critic_loss     | 0.923    |
|    learning_rate   | 0.0005   |
|    n_updates       | 679904   |
---------------------------------
Eval num_timesteps=690000, episode_reward=-49.01 +/- 32.69
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -49      |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | 0.159    |
|    critic_loss     | 0.899    |
|    learning_rate   | 0.0005   |
|    n_updates       | 685907   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.24     |
| time/              |          |
|    episodes        | 348      |
|    fps             | 15       |
|    time_elapsed    | 43408    |
|    total timesteps | 693911   |
| train/             |          |
|    actor_loss      | 0.155    |
|    critic_loss     | 0.912    |
|    learning_rate   | 0.0005   |
|    n_updates       | 687908   |
---------------------------------
Eval num_timesteps=700000, episode_reward=-19.13 +/- 27.24
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19.1    |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | 0.147    |
|    critic_loss     | 0.885    |
|    learning_rate   | 0.0005   |
|    n_updates       | 695912   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.93     |
| time/              |          |
|    episodes        | 352      |
|    fps             | 15       |
|    time_elapsed    | 43940    |
|    total timesteps | 701915   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.32     |
| time/              |          |
|    episodes        | 356      |
|    fps             | 16       |
|    time_elapsed    | 44354    |
|    total timesteps | 709919   |
| train/             |          |
|    actor_loss      | 0.131    |
|    critic_loss     | 0.89     |
|    learning_rate   | 0.0005   |
|    n_updates       | 703916   |
---------------------------------
Eval num_timesteps=710000, episode_reward=-44.22 +/- 34.08
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -44.2    |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | 0.13     |
|    critic_loss     | 0.896    |
|    learning_rate   | 0.0005   |
|    n_updates       | 705917   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 7.95     |
| time/              |          |
|    episodes        | 360      |
|    fps             | 15       |
|    time_elapsed    | 44876    |
|    total timesteps | 717923   |
| train/             |          |
|    actor_loss      | 0.125    |
|    critic_loss     | 0.92     |
|    learning_rate   | 0.0005   |
|    n_updates       | 711920   |
---------------------------------
Eval num_timesteps=720000, episode_reward=-20.58 +/- 24.08
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -20.6    |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | 0.119    |
|    critic_loss     | 0.882    |
|    learning_rate   | 0.0005   |
|    n_updates       | 715922   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    episodes        | 364      |
|    fps             | 15       |
|    time_elapsed    | 45399    |
|    total timesteps | 725927   |
| train/             |          |
|    actor_loss      | 0.118    |
|    critic_loss     | 0.898    |
|    learning_rate   | 0.0005   |
|    n_updates       | 719924   |
---------------------------------
Eval num_timesteps=730000, episode_reward=-26.38 +/- 26.06
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -26.4    |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | 0.131    |
|    critic_loss     | 0.882    |
|    learning_rate   | 0.0005   |
|    n_updates       | 725927   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.97     |
| time/              |          |
|    episodes        | 368      |
|    fps             | 15       |
|    time_elapsed    | 45923    |
|    total timesteps | 733931   |
| train/             |          |
|    actor_loss      | 0.132    |
|    critic_loss     | 0.917    |
|    learning_rate   | 0.0005   |
|    n_updates       | 727928   |
---------------------------------
Eval num_timesteps=740000, episode_reward=-17.94 +/- 26.79
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -17.9    |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | 0.126    |
|    critic_loss     | 0.902    |
|    learning_rate   | 0.0005   |
|    n_updates       | 735932   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.79     |
| time/              |          |
|    episodes        | 372      |
|    fps             | 15       |
|    time_elapsed    | 46447    |
|    total timesteps | 741935   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.62     |
| time/              |          |
|    episodes        | 376      |
|    fps             | 16       |
|    time_elapsed    | 46861    |
|    total timesteps | 749939   |
| train/             |          |
|    actor_loss      | 0.124    |
|    critic_loss     | 0.907    |
|    learning_rate   | 0.0005   |
|    n_updates       | 743936   |
---------------------------------
Eval num_timesteps=750000, episode_reward=-15.24 +/- 32.45
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -15.2    |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | 0.127    |
|    critic_loss     | 0.896    |
|    learning_rate   | 0.0005   |
|    n_updates       | 745937   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.18     |
| time/              |          |
|    episodes        | 380      |
|    fps             | 15       |
|    time_elapsed    | 47387    |
|    total timesteps | 757943   |
| train/             |          |
|    actor_loss      | 0.11     |
|    critic_loss     | 0.905    |
|    learning_rate   | 0.0005   |
|    n_updates       | 751940   |
---------------------------------
Eval num_timesteps=760000, episode_reward=-21.53 +/- 17.51
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -21.5    |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | 0.111    |
|    critic_loss     | 0.918    |
|    learning_rate   | 0.0005   |
|    n_updates       | 755942   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 6.3      |
| time/              |          |
|    episodes        | 384      |
|    fps             | 15       |
|    time_elapsed    | 47915    |
|    total timesteps | 765947   |
| train/             |          |
|    actor_loss      | 0.114    |
|    critic_loss     | 0.905    |
|    learning_rate   | 0.0005   |
|    n_updates       | 759944   |
---------------------------------
Eval num_timesteps=770000, episode_reward=-45.27 +/- 34.99
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -45.3    |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | 0.114    |
|    critic_loss     | 0.911    |
|    learning_rate   | 0.0005   |
|    n_updates       | 765947   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.95     |
| time/              |          |
|    episodes        | 388      |
|    fps             | 15       |
|    time_elapsed    | 48447    |
|    total timesteps | 773951   |
| train/             |          |
|    actor_loss      | 0.121    |
|    critic_loss     | 0.92     |
|    learning_rate   | 0.0005   |
|    n_updates       | 767948   |
---------------------------------
Eval num_timesteps=780000, episode_reward=-1.42 +/- 34.08
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.42    |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | 0.132    |
|    critic_loss     | 0.891    |
|    learning_rate   | 0.0005   |
|    n_updates       | 775952   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.83     |
| time/              |          |
|    episodes        | 392      |
|    fps             | 15       |
|    time_elapsed    | 48974    |
|    total timesteps | 781955   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.67     |
| time/              |          |
|    episodes        | 396      |
|    fps             | 15       |
|    time_elapsed    | 49388    |
|    total timesteps | 789959   |
| train/             |          |
|    actor_loss      | 0.138    |
|    critic_loss     | 0.924    |
|    learning_rate   | 0.0005   |
|    n_updates       | 783956   |
---------------------------------
Eval num_timesteps=790000, episode_reward=-24.83 +/- 34.12
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -24.8    |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | 0.138    |
|    critic_loss     | 0.891    |
|    learning_rate   | 0.0005   |
|    n_updates       | 785957   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.27     |
| time/              |          |
|    episodes        | 400      |
|    fps             | 15       |
|    time_elapsed    | 49914    |
|    total timesteps | 797963   |
| train/             |          |
|    actor_loss      | 0.138    |
|    critic_loss     | 0.922    |
|    learning_rate   | 0.0005   |
|    n_updates       | 791960   |
---------------------------------
Eval num_timesteps=800000, episode_reward=-27.52 +/- 40.78
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27.5    |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | 0.138    |
|    critic_loss     | 0.903    |
|    learning_rate   | 0.0005   |
|    n_updates       | 795962   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.07     |
| time/              |          |
|    episodes        | 404      |
|    fps             | 15       |
|    time_elapsed    | 50441    |
|    total timesteps | 805967   |
| train/             |          |
|    actor_loss      | 0.139    |
|    critic_loss     | 0.916    |
|    learning_rate   | 0.0005   |
|    n_updates       | 799964   |
---------------------------------
Eval num_timesteps=810000, episode_reward=-23.68 +/- 28.76
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.7    |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | 0.13     |
|    critic_loss     | 0.907    |
|    learning_rate   | 0.0005   |
|    n_updates       | 805967   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.732    |
| time/              |          |
|    episodes        | 408      |
|    fps             | 15       |
|    time_elapsed    | 50966    |
|    total timesteps | 813971   |
| train/             |          |
|    actor_loss      | 0.134    |
|    critic_loss     | 0.915    |
|    learning_rate   | 0.0005   |
|    n_updates       | 807968   |
---------------------------------
Eval num_timesteps=820000, episode_reward=10.31 +/- 23.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | 0.112    |
|    critic_loss     | 0.912    |
|    learning_rate   | 0.0005   |
|    n_updates       | 815972   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.16     |
| time/              |          |
|    episodes        | 412      |
|    fps             | 15       |
|    time_elapsed    | 51491    |
|    total timesteps | 821975   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.982    |
| time/              |          |
|    episodes        | 416      |
|    fps             | 15       |
|    time_elapsed    | 51902    |
|    total timesteps | 829979   |
| train/             |          |
|    actor_loss      | 0.125    |
|    critic_loss     | 0.897    |
|    learning_rate   | 0.0005   |
|    n_updates       | 823976   |
---------------------------------
Eval num_timesteps=830000, episode_reward=-7.31 +/- 9.38
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -7.31    |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | 0.124    |
|    critic_loss     | 0.915    |
|    learning_rate   | 0.0005   |
|    n_updates       | 825977   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.198    |
| time/              |          |
|    episodes        | 420      |
|    fps             | 15       |
|    time_elapsed    | 52430    |
|    total timesteps | 837983   |
| train/             |          |
|    actor_loss      | 0.128    |
|    critic_loss     | 0.915    |
|    learning_rate   | 0.0005   |
|    n_updates       | 831980   |
---------------------------------
Eval num_timesteps=840000, episode_reward=-30.28 +/- 45.16
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -30.3    |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | 0.121    |
|    critic_loss     | 0.898    |
|    learning_rate   | 0.0005   |
|    n_updates       | 835982   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.0172  |
| time/              |          |
|    episodes        | 424      |
|    fps             | 15       |
|    time_elapsed    | 52959    |
|    total timesteps | 845987   |
| train/             |          |
|    actor_loss      | 0.116    |
|    critic_loss     | 0.906    |
|    learning_rate   | 0.0005   |
|    n_updates       | 839984   |
---------------------------------
Eval num_timesteps=850000, episode_reward=-51.68 +/- 26.12
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -51.7    |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | 0.112    |
|    critic_loss     | 0.917    |
|    learning_rate   | 0.0005   |
|    n_updates       | 845987   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.557   |
| time/              |          |
|    episodes        | 428      |
|    fps             | 15       |
|    time_elapsed    | 53484    |
|    total timesteps | 853991   |
| train/             |          |
|    actor_loss      | 0.106    |
|    critic_loss     | 0.907    |
|    learning_rate   | 0.0005   |
|    n_updates       | 847988   |
---------------------------------
Eval num_timesteps=860000, episode_reward=-13.57 +/- 17.00
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -13.6    |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | 0.115    |
|    critic_loss     | 0.926    |
|    learning_rate   | 0.0005   |
|    n_updates       | 855992   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.628    |
| time/              |          |
|    episodes        | 432      |
|    fps             | 15       |
|    time_elapsed    | 54008    |
|    total timesteps | 861995   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.08     |
| time/              |          |
|    episodes        | 436      |
|    fps             | 15       |
|    time_elapsed    | 54394    |
|    total timesteps | 869407   |
| train/             |          |
|    actor_loss      | 0.109    |
|    critic_loss     | 0.9      |
|    learning_rate   | 0.0005   |
|    n_updates       | 863404   |
---------------------------------
Eval num_timesteps=870000, episode_reward=-17.28 +/- 26.37
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -17.3    |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | 0.109    |
|    critic_loss     | 0.909    |
|    learning_rate   | 0.0005   |
|    n_updates       | 865405   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.93     |
| time/              |          |
|    episodes        | 440      |
|    fps             | 15       |
|    time_elapsed    | 54922    |
|    total timesteps | 877411   |
| train/             |          |
|    actor_loss      | 0.12     |
|    critic_loss     | 0.906    |
|    learning_rate   | 0.0005   |
|    n_updates       | 871408   |
---------------------------------
Eval num_timesteps=880000, episode_reward=-25.35 +/- 32.90
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -25.4    |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | 0.117    |
|    critic_loss     | 0.904    |
|    learning_rate   | 0.0005   |
|    n_updates       | 875410   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    episodes        | 444      |
|    fps             | 15       |
|    time_elapsed    | 55446    |
|    total timesteps | 885415   |
| train/             |          |
|    actor_loss      | 0.115    |
|    critic_loss     | 0.904    |
|    learning_rate   | 0.0005   |
|    n_updates       | 879412   |
---------------------------------
Eval num_timesteps=890000, episode_reward=-22.58 +/- 25.08
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -22.6    |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | 0.121    |
|    critic_loss     | 0.922    |
|    learning_rate   | 0.0005   |
|    n_updates       | 885415   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.41     |
| time/              |          |
|    episodes        | 448      |
|    fps             | 15       |
|    time_elapsed    | 55976    |
|    total timesteps | 893419   |
| train/             |          |
|    actor_loss      | 0.123    |
|    critic_loss     | 0.93     |
|    learning_rate   | 0.0005   |
|    n_updates       | 887416   |
---------------------------------
Eval num_timesteps=900000, episode_reward=-9.87 +/- 22.74
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -9.87    |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | 0.115    |
|    critic_loss     | 0.924    |
|    learning_rate   | 0.0005   |
|    n_updates       | 895420   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.0752  |
| time/              |          |
|    episodes        | 452      |
|    fps             | 15       |
|    time_elapsed    | 56509    |
|    total timesteps | 901423   |
---------------------------------
