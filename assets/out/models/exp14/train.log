2021-12-13 01:36:02.971210: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-13 01:36:02.971254: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_6
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 9.53     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 58       |
|    time_elapsed    | 136      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.93e+03 |
|    ep_rew_mean     | 0.0176   |
| time/              |          |
|    episodes        | 8        |
|    fps             | 59       |
|    time_elapsed    | 260      |
|    total timesteps | 15450    |
---------------------------------
Terminated
2021-12-13 01:42:24.842419: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-13 01:42:24.842455: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_7
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -35.4    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 59       |
|    time_elapsed    | 135      |
|    total timesteps | 7989     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -53.8    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 59       |
|    time_elapsed    | 268      |
|    total timesteps | 15993    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-100.22 +/- 74.05
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -100     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -90.3    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 31       |
|    time_elapsed    | 768      |
|    total timesteps | 23997    |
| train/             |          |
|    actor_loss      | 2.35     |
|    critic_loss     | 4.92     |
|    learning_rate   | 0.000735 |
|    n_updates       | 2001     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -118     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 23       |
|    time_elapsed    | 1372     |
|    total timesteps | 32001    |
| train/             |          |
|    actor_loss      | 2.84     |
|    critic_loss     | 0.791    |
|    learning_rate   | 0.000729 |
|    n_updates       | 10005    |
---------------------------------
Eval num_timesteps=40000, episode_reward=11.00 +/- 49.18
Episode length: 1669.00 +/- 536.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.67e+03 |
|    mean_reward     | 11       |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 3.05     |
|    critic_loss     | 0.994    |
|    learning_rate   | 0.000723 |
|    n_updates       | 18009    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -122     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 18       |
|    time_elapsed    | 2161     |
|    total timesteps | 40005    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -122     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 17       |
|    time_elapsed    | 2774     |
|    total timesteps | 48009    |
| train/             |          |
|    actor_loss      | 7.13     |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.000718 |
|    n_updates       | 26013    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -127     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 16       |
|    time_elapsed    | 3393     |
|    total timesteps | 56013    |
| train/             |          |
|    actor_loss      | 6.37     |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.000712 |
|    n_updates       | 34017    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-64.17 +/- 67.94
Episode length: 1981.40 +/- 39.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.98e+03 |
|    mean_reward     | -64.2    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 5.99     |
|    critic_loss     | 1        |
|    learning_rate   | 0.000709 |
|    n_updates       | 38019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -124     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 15       |
|    time_elapsed    | 4177     |
|    total timesteps | 63104    |
| train/             |          |
|    actor_loss      | 5.61     |
|    critic_loss     | 0.915    |
|    learning_rate   | 0.000707 |
|    n_updates       | 41344    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -122     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 14       |
|    time_elapsed    | 4793     |
|    total timesteps | 71069    |
| train/             |          |
|    actor_loss      | 4.97     |
|    critic_loss     | 0.773    |
|    learning_rate   | 0.000702 |
|    n_updates       | 49112    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -126     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 14       |
|    time_elapsed    | 5419     |
|    total timesteps | 79073    |
| train/             |          |
|    actor_loss      | 4.31     |
|    critic_loss     | 0.775    |
|    learning_rate   | 0.000696 |
|    n_updates       | 57077    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-67.67 +/- 74.81
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -67.7    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 4.11     |
|    critic_loss     | 0.673    |
|    learning_rate   | 0.000695 |
|    n_updates       | 59078    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -130     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 13       |
|    time_elapsed    | 6293     |
|    total timesteps | 87077    |
| train/             |          |
|    actor_loss      | 3.84     |
|    critic_loss     | 0.689    |
|    learning_rate   | 0.00069  |
|    n_updates       | 65081    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -131     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 13       |
|    time_elapsed    | 6985     |
|    total timesteps | 95081    |
| train/             |          |
|    actor_loss      | 3.6      |
|    critic_loss     | 0.745    |
|    learning_rate   | 0.000685 |
|    n_updates       | 73085    |
---------------------------------
Eval num_timesteps=100000, episode_reward=35.80 +/- 39.18
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 35.8     |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 3.49     |
|    critic_loss     | 0.667    |
|    learning_rate   | 0.000681 |
|    n_updates       | 79088    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -133     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 13       |
|    time_elapsed    | 7926     |
|    total timesteps | 103085   |
| train/             |          |
|    actor_loss      | 3.48     |
|    critic_loss     | 0.811    |
|    learning_rate   | 0.000679 |
|    n_updates       | 81089    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -134     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 12       |
|    time_elapsed    | 8719     |
|    total timesteps | 111089   |
| train/             |          |
|    actor_loss      | 3.39     |
|    critic_loss     | 0.705    |
|    learning_rate   | 0.000674 |
|    n_updates       | 89093    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -132     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 12       |
|    time_elapsed    | 9472     |
|    total timesteps | 118047   |
| train/             |          |
|    actor_loss      | 3.28     |
|    critic_loss     | 0.703    |
|    learning_rate   | 0.000669 |
|    n_updates       | 96051    |
---------------------------------
Eval num_timesteps=120000, episode_reward=38.85 +/- 35.28
Episode length: 1895.60 +/- 210.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.9e+03  |
|    mean_reward     | 38.8     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 3.27     |
|    critic_loss     | 0.671    |
|    learning_rate   | 0.000667 |
|    n_updates       | 98052    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -132     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 11       |
|    time_elapsed    | 10563    |
|    total timesteps | 126051   |
| train/             |          |
|    actor_loss      | 3.2      |
|    critic_loss     | 0.54     |
|    learning_rate   | 0.000663 |
|    n_updates       | 104055   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -136     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 11       |
|    time_elapsed    | 11457    |
|    total timesteps | 134055   |
| train/             |          |
|    actor_loss      | 3.12     |
|    critic_loss     | 0.76     |
|    learning_rate   | 0.000658 |
|    n_updates       | 112059   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-6.16 +/- 23.77
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -6.16    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 3.15     |
|    critic_loss     | 0.796    |
|    learning_rate   | 0.000653 |
|    n_updates       | 118062   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -138     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 11       |
|    time_elapsed    | 12592    |
|    total timesteps | 142059   |
| train/             |          |
|    actor_loss      | 3.19     |
|    critic_loss     | 0.886    |
|    learning_rate   | 0.000652 |
|    n_updates       | 120063   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -138     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 11       |
|    time_elapsed    | 13519    |
|    total timesteps | 150063   |
| train/             |          |
|    actor_loss      | 3.3      |
|    critic_loss     | 0.794    |
|    learning_rate   | 0.000646 |
|    n_updates       | 128067   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -145     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 10       |
|    time_elapsed    | 14466    |
|    total timesteps | 158067   |
| train/             |          |
|    actor_loss      | 3.41     |
|    critic_loss     | 0.781    |
|    learning_rate   | 0.000641 |
|    n_updates       | 136071   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-14.29 +/- 23.84
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.3    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 3.44     |
|    critic_loss     | 0.909    |
|    learning_rate   | 0.000639 |
|    n_updates       | 138072   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -144     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 10       |
|    time_elapsed    | 15437    |
|    total timesteps | 164348   |
| train/             |          |
|    actor_loss      | 3.48     |
|    critic_loss     | 0.832    |
|    learning_rate   | 0.000636 |
|    n_updates       | 142352   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -148     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 10       |
|    time_elapsed    | 16392    |
|    total timesteps | 172352   |
| train/             |          |
|    actor_loss      | 3.57     |
|    critic_loss     | 0.856    |
|    learning_rate   | 0.000631 |
|    n_updates       | 150356   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-22.75 +/- 86.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -22.7    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 3.63     |
|    critic_loss     | 0.972    |
|    learning_rate   | 0.000625 |
|    n_updates       | 158360   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -147     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 10       |
|    time_elapsed    | 17573    |
|    total timesteps | 180356   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -145     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 10       |
|    time_elapsed    | 18416    |
|    total timesteps | 187368   |
| train/             |          |
|    actor_loss      | 3.73     |
|    critic_loss     | 0.892    |
|    learning_rate   | 0.00062  |
|    n_updates       | 165372   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -145     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 10       |
|    time_elapsed    | 19258    |
|    total timesteps | 194374   |
| train/             |          |
|    actor_loss      | 3.78     |
|    critic_loss     | 1.12     |
|    learning_rate   | 0.000615 |
|    n_updates       | 172378   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-2.28 +/- 9.70
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.28    |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 3.8      |
|    critic_loss     | 0.988    |
|    learning_rate   | 0.000611 |
|    n_updates       | 178381   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -151     |
| time/              |          |
|    episodes        | 104      |
|    fps             | 9        |
|    time_elapsed    | 20439    |
|    total timesteps | 202378   |
| train/             |          |
|    actor_loss      | 3.82     |
|    critic_loss     | 1.01     |
|    learning_rate   | 0.00061  |
|    n_updates       | 180382   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -151     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 9        |
|    time_elapsed    | 21406    |
|    total timesteps | 210382   |
| train/             |          |
|    actor_loss      | 3.86     |
|    critic_loss     | 0.979    |
|    learning_rate   | 0.000604 |
|    n_updates       | 188386   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -151     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 9        |
|    time_elapsed    | 22372    |
|    total timesteps | 218386   |
| train/             |          |
|    actor_loss      | 3.86     |
|    critic_loss     | 1.03     |
|    learning_rate   | 0.000599 |
|    n_updates       | 196390   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-58.99 +/- 45.42
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -59      |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 3.85     |
|    critic_loss     | 0.866    |
|    learning_rate   | 0.000597 |
|    n_updates       | 198391   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -150     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 9        |
|    time_elapsed    | 23571    |
|    total timesteps | 226390   |
| train/             |          |
|    actor_loss      | 3.84     |
|    critic_loss     | 0.921    |
|    learning_rate   | 0.000593 |
|    n_updates       | 204394   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -152     |
| time/              |          |
|    episodes        | 120      |
|    fps             | 9        |
|    time_elapsed    | 24540    |
|    total timesteps | 234394   |
| train/             |          |
|    actor_loss      | 3.86     |
|    critic_loss     | 0.991    |
|    learning_rate   | 0.000587 |
|    n_updates       | 212398   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-31.43 +/- 50.05
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -31.4    |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 3.84     |
|    critic_loss     | 0.75     |
|    learning_rate   | 0.000583 |
|    n_updates       | 218100   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -150     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 9        |
|    time_elapsed    | 25693    |
|    total timesteps | 242097   |
| train/             |          |
|    actor_loss      | 3.82     |
|    critic_loss     | 0.932    |
|    learning_rate   | 0.000582 |
|    n_updates       | 220101   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -148     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 9        |
|    time_elapsed    | 26665    |
|    total timesteps | 250101   |
| train/             |          |
|    actor_loss      | 3.83     |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.000576 |
|    n_updates       | 228105   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -151     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 9        |
|    time_elapsed    | 27636    |
|    total timesteps | 258105   |
| train/             |          |
|    actor_loss      | 3.85     |
|    critic_loss     | 0.964    |
|    learning_rate   | 0.000571 |
|    n_updates       | 236109   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-23.06 +/- 29.52
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.1    |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 3.83     |
|    critic_loss     | 0.838    |
|    learning_rate   | 0.000569 |
|    n_updates       | 238110   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -152     |
| time/              |          |
|    episodes        | 136      |
|    fps             | 9        |
|    time_elapsed    | 28826    |
|    total timesteps | 266109   |
| train/             |          |
|    actor_loss      | 3.86     |
|    critic_loss     | 0.97     |
|    learning_rate   | 0.000565 |
|    n_updates       | 244113   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -152     |
| time/              |          |
|    episodes        | 140      |
|    fps             | 9        |
|    time_elapsed    | 29797    |
|    total timesteps | 274113   |
| train/             |          |
|    actor_loss      | 3.87     |
|    critic_loss     | 0.986    |
|    learning_rate   | 0.00056  |
|    n_updates       | 252117   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-21.29 +/- 38.85
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -21.3    |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 3.89     |
|    critic_loss     | 1.04     |
|    learning_rate   | 0.000554 |
|    n_updates       | 259972   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -150     |
| time/              |          |
|    episodes        | 144      |
|    fps             | 9        |
|    time_elapsed    | 30972    |
|    total timesteps | 281968   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -150     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 9        |
|    time_elapsed    | 31940    |
|    total timesteps | 289972   |
| train/             |          |
|    actor_loss      | 3.9      |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.000548 |
|    n_updates       | 267976   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -151     |
| time/              |          |
|    episodes        | 152      |
|    fps             | 9        |
|    time_elapsed    | 32909    |
|    total timesteps | 297976   |
| train/             |          |
|    actor_loss      | 3.93     |
|    critic_loss     | 0.923    |
|    learning_rate   | 0.000543 |
|    n_updates       | 275980   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-16.10 +/- 28.03
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -16.1    |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 3.87     |
|    critic_loss     | 0.895    |
|    learning_rate   | 0.00054  |
|    n_updates       | 279982   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -152     |
| time/              |          |
|    episodes        | 156      |
|    fps             | 8        |
|    time_elapsed    | 34104    |
|    total timesteps | 305980   |
| train/             |          |
|    actor_loss      | 3.84     |
|    critic_loss     | 0.89     |
|    learning_rate   | 0.000537 |
|    n_updates       | 283984   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -154     |
| time/              |          |
|    episodes        | 160      |
|    fps             | 8        |
|    time_elapsed    | 35082    |
|    total timesteps | 313984   |
| train/             |          |
|    actor_loss      | 3.78     |
|    critic_loss     | 0.812    |
|    learning_rate   | 0.000532 |
|    n_updates       | 291988   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-10.77 +/- 26.52
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.8    |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 3.74     |
|    critic_loss     | 0.739    |
|    learning_rate   | 0.000526 |
|    n_updates       | 299992   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -155     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 8        |
|    time_elapsed    | 36276    |
|    total timesteps | 321988   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -152     |
| time/              |          |
|    episodes        | 168      |
|    fps             | 8        |
|    time_elapsed    | 37242    |
|    total timesteps | 329992   |
| train/             |          |
|    actor_loss      | 3.67     |
|    critic_loss     | 0.516    |
|    learning_rate   | 0.00052  |
|    n_updates       | 307996   |
---------------------------------
Terminated
2021-12-13 12:17:01.535305: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-13 12:17:01.535411: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_8
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 68.1     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 45       |
|    time_elapsed    | 175      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 75.7     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 50       |
|    time_elapsed    | 316      |
|    total timesteps | 16008    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-273.70 +/- 197.13
Episode length: 1874.80 +/- 252.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.87e+03 |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 40.9     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 25       |
|    time_elapsed    | 937      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | -0.717   |
|    critic_loss     | 0.944    |
|    learning_rate   | 0.000735 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 45.9     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 20       |
|    time_elapsed    | 1597     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | -0.409   |
|    critic_loss     | 0.775    |
|    learning_rate   | 0.000729 |
|    n_updates       | 12006    |
---------------------------------
Eval num_timesteps=40000, episode_reward=23.61 +/- 41.16
Episode length: 1824.80 +/- 352.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.82e+03 |
|    mean_reward     | 23.6     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -0.604   |
|    critic_loss     | 0.834    |
|    learning_rate   | 0.000723 |
|    n_updates       | 20010    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 57.7     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 16       |
|    time_elapsed    | 2490     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 58.3     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 15       |
|    time_elapsed    | 3161     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | -0.851   |
|    critic_loss     | 0.698    |
|    learning_rate   | 0.000718 |
|    n_updates       | 28014    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 56.5     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 14       |
|    time_elapsed    | 3836     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | -1.06    |
|    critic_loss     | 0.693    |
|    learning_rate   | 0.000712 |
|    n_updates       | 36018    |
---------------------------------
Terminated
2021-12-13 14:17:36.475053: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-13 14:17:36.475093: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_9
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 22.3     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 62       |
|    time_elapsed    | 128      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 38.7     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 62       |
|    time_elapsed    | 255      |
|    total timesteps | 16008    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-33.43 +/- 46.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -33.4    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 64.2     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 28       |
|    time_elapsed    | 849      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 11.9     |
|    critic_loss     | 2.85     |
|    learning_rate   | 0.000978 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 66.5     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 21       |
|    time_elapsed    | 1500     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 7.71     |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.00097  |
|    n_updates       | 12006    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-36.49 +/- 24.86
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -36.5    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 4.77     |
|    critic_loss     | 1        |
|    learning_rate   | 0.000962 |
|    n_updates       | 20010    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 55       |
| time/              |          |
|    episodes        | 20       |
|    fps             | 16       |
|    time_elapsed    | 2355     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 52.8     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 15       |
|    time_elapsed    | 3018     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 2.74     |
|    critic_loss     | 0.902    |
|    learning_rate   | 0.000954 |
|    n_updates       | 28014    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 40       |
| time/              |          |
|    episodes        | 28       |
|    fps             | 15       |
|    time_elapsed    | 3668     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 1.56     |
|    critic_loss     | 0.812    |
|    learning_rate   | 0.000947 |
|    n_updates       | 36018    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-38.38 +/- 29.47
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -38.4    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 1.36     |
|    critic_loss     | 0.909    |
|    learning_rate   | 0.000943 |
|    n_updates       | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 46.5     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 14       |
|    time_elapsed    | 4539     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 1.12     |
|    critic_loss     | 0.889    |
|    learning_rate   | 0.000939 |
|    n_updates       | 44022    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 43.3     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 13       |
|    time_elapsed    | 5210     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 0.908    |
|    critic_loss     | 0.876    |
|    learning_rate   | 0.000931 |
|    n_updates       | 52026    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-60.39 +/- 54.88
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -60.4    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.761    |
|    critic_loss     | 0.7      |
|    learning_rate   | 0.000923 |
|    n_updates       | 60030    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 37.6     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 13       |
|    time_elapsed    | 6096     |
|    total timesteps | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 34.3     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 12       |
|    time_elapsed    | 6797     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 0.424    |
|    critic_loss     | 0.908    |
|    learning_rate   | 0.000915 |
|    n_updates       | 68034    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 34.9     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 12       |
|    time_elapsed    | 7528     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 0.506    |
|    critic_loss     | 1.07     |
|    learning_rate   | 0.000907 |
|    n_updates       | 76038    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-28.47 +/- 8.12
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -28.5    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.37     |
|    critic_loss     | 0.891    |
|    learning_rate   | 0.000903 |
|    n_updates       | 80040    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 38.2     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 12       |
|    time_elapsed    | 8519     |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | 0.149    |
|    critic_loss     | 0.768    |
|    learning_rate   | 0.000899 |
|    n_updates       | 84042    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 35.7     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 11       |
|    time_elapsed    | 9419     |
|    total timesteps | 112056   |
| train/             |          |
|    actor_loss      | -0.165   |
|    critic_loss     | 0.883    |
|    learning_rate   | 0.000891 |
|    n_updates       | 92046    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-18.52 +/- 22.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -18.5    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -0.383   |
|    critic_loss     | 0.811    |
|    learning_rate   | 0.000883 |
|    n_updates       | 100050   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 34.8     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 11       |
|    time_elapsed    | 10563    |
|    total timesteps | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 36.2     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 11       |
|    time_elapsed    | 11505    |
|    total timesteps | 128064   |
| train/             |          |
|    actor_loss      | -0.514   |
|    critic_loss     | 0.771    |
|    learning_rate   | 0.000875 |
|    n_updates       | 108054   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 37.4     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 10       |
|    time_elapsed    | 12457    |
|    total timesteps | 136068   |
| train/             |          |
|    actor_loss      | -0.612   |
|    critic_loss     | 0.894    |
|    learning_rate   | 0.000867 |
|    n_updates       | 116058   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-10.54 +/- 11.45
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.5    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -0.669   |
|    critic_loss     | 0.792    |
|    learning_rate   | 0.000863 |
|    n_updates       | 120060   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 43.5     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 10       |
|    time_elapsed    | 13639    |
|    total timesteps | 144072   |
| train/             |          |
|    actor_loss      | -0.688   |
|    critic_loss     | 0.911    |
|    learning_rate   | 0.000859 |
|    n_updates       | 124062   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 45       |
| time/              |          |
|    episodes        | 76       |
|    fps             | 10       |
|    time_elapsed    | 14631    |
|    total timesteps | 152076   |
| train/             |          |
|    actor_loss      | -0.756   |
|    critic_loss     | 0.751    |
|    learning_rate   | 0.000851 |
|    n_updates       | 132066   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-26.87 +/- 15.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -26.9    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -0.879   |
|    critic_loss     | 0.691    |
|    learning_rate   | 0.000844 |
|    n_updates       | 140009   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 45.7     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 10       |
|    time_elapsed    | 15827    |
|    total timesteps | 160019   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 48.6     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 9        |
|    time_elapsed    | 16789    |
|    total timesteps | 167787   |
| train/             |          |
|    actor_loss      | -0.999   |
|    critic_loss     | 0.709    |
|    learning_rate   | 0.000836 |
|    n_updates       | 147777   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 46.7     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 9        |
|    time_elapsed    | 17770    |
|    total timesteps | 175791   |
| train/             |          |
|    actor_loss      | -0.988   |
|    critic_loss     | 0.777    |
|    learning_rate   | 0.000828 |
|    n_updates       | 155781   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-59.77 +/- 32.51
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -59.8    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.745    |
|    learning_rate   | 0.000822 |
|    n_updates       | 161784   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 46.1     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 9        |
|    time_elapsed    | 18969    |
|    total timesteps | 183795   |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.749    |
|    learning_rate   | 0.00082  |
|    n_updates       | 163785   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 45.4     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 9        |
|    time_elapsed    | 19950    |
|    total timesteps | 191799   |
| train/             |          |
|    actor_loss      | -1.07    |
|    critic_loss     | 0.706    |
|    learning_rate   | 0.000812 |
|    n_updates       | 171789   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 46.4     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 9        |
|    time_elapsed    | 20937    |
|    total timesteps | 199803   |
| train/             |          |
|    actor_loss      | -1.09    |
|    critic_loss     | 0.737    |
|    learning_rate   | 0.000804 |
|    n_updates       | 179793   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-45.14 +/- 46.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -45.1    |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -1.1     |
|    critic_loss     | 0.62     |
|    learning_rate   | 0.000802 |
|    n_updates       | 181794   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 45       |
| time/              |          |
|    episodes        | 104      |
|    fps             | 9        |
|    time_elapsed    | 22124    |
|    total timesteps | 207807   |
| train/             |          |
|    actor_loss      | -1.12    |
|    critic_loss     | 0.694    |
|    learning_rate   | 0.000796 |
|    n_updates       | 187797   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 45.2     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 9        |
|    time_elapsed    | 23117    |
|    total timesteps | 215811   |
| train/             |          |
|    actor_loss      | -1.04    |
|    critic_loss     | 0.76     |
|    learning_rate   | 0.000788 |
|    n_updates       | 195801   |
---------------------------------
Eval num_timesteps=220000, episode_reward=5.75 +/- 11.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 5.75     |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.766    |
|    learning_rate   | 0.000782 |
|    n_updates       | 201804   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 43.1     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 9        |
|    time_elapsed    | 24316    |
|    total timesteps | 223815   |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.712    |
|    learning_rate   | 0.00078  |
|    n_updates       | 203805   |
---------------------------------
Terminated
2021-12-13 21:29:43.663341: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-13 21:29:43.663388: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_10
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 94.3     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 97       |
|    time_elapsed    | 82       |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 98.9     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 98       |
|    time_elapsed    | 161      |
|    total timesteps | 16008    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-14.88 +/- 37.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.9    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 79       |
| time/              |          |
|    episodes        | 12       |
|    fps             | 34       |
|    time_elapsed    | 691      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 6.2      |
|    critic_loss     | 0.683    |
|    learning_rate   | 0.000978 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 70.6     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 25       |
|    time_elapsed    | 1264     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 3.48     |
|    critic_loss     | 0.086    |
|    learning_rate   | 0.00097  |
|    n_updates       | 12006    |
---------------------------------
Eval num_timesteps=40000, episode_reward=1.40 +/- 33.56
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 1.94     |
|    critic_loss     | 0.03     |
|    learning_rate   | 0.000962 |
|    n_updates       | 20010    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 68.3     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 19       |
|    time_elapsed    | 2051     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 67.7     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 18       |
|    time_elapsed    | 2625     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.9      |
|    critic_loss     | 0.0213   |
|    learning_rate   | 0.000954 |
|    n_updates       | 28014    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 73.1     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 17       |
|    time_elapsed    | 3197     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 0.33     |
|    critic_loss     | 0.0227   |
|    learning_rate   | 0.000947 |
|    n_updates       | 36018    |
---------------------------------
Eval num_timesteps=60000, episode_reward=7.12 +/- 31.44
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 7.12     |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.0822   |
|    critic_loss     | 0.0213   |
|    learning_rate   | 0.000943 |
|    n_updates       | 40020    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 66.2     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 16       |
|    time_elapsed    | 3981     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | -0.195   |
|    critic_loss     | 0.0227   |
|    learning_rate   | 0.000939 |
|    n_updates       | 44022    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 67.1     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 15       |
|    time_elapsed    | 4568     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | -0.595   |
|    critic_loss     | 0.0202   |
|    learning_rate   | 0.000931 |
|    n_updates       | 52026    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-7.31 +/- 32.26
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -7.31    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -0.872   |
|    critic_loss     | 0.0185   |
|    learning_rate   | 0.000923 |
|    n_updates       | 60030    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 65.3     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 14       |
|    time_elapsed    | 5370     |
|    total timesteps | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 66.4     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 14       |
|    time_elapsed    | 5992     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | -1.07    |
|    critic_loss     | 0.0184   |
|    learning_rate   | 0.000915 |
|    n_updates       | 68034    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 65.6     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 14       |
|    time_elapsed    | 6645     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | -1.22    |
|    critic_loss     | 0.0181   |
|    learning_rate   | 0.000907 |
|    n_updates       | 76038    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-23.62 +/- 21.16
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.6    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.0171   |
|    learning_rate   | 0.000903 |
|    n_updates       | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 69.6     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 13       |
|    time_elapsed    | 7543     |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.0216   |
|    learning_rate   | 0.000899 |
|    n_updates       | 84042    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 71.6     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 13       |
|    time_elapsed    | 8347     |
|    total timesteps | 112056   |
| train/             |          |
|    actor_loss      | -1.44    |
|    critic_loss     | 0.0199   |
|    learning_rate   | 0.000891 |
|    n_updates       | 92046    |
---------------------------------
Eval num_timesteps=120000, episode_reward=10.68 +/- 34.13
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 10.7     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -1.52    |
|    critic_loss     | 0.0207   |
|    learning_rate   | 0.000883 |
|    n_updates       | 100050   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 69.7     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 12       |
|    time_elapsed    | 9393     |
|    total timesteps | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 69.9     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 12       |
|    time_elapsed    | 10253    |
|    total timesteps | 128064   |
| train/             |          |
|    actor_loss      | -1.57    |
|    critic_loss     | 0.0198   |
|    learning_rate   | 0.000875 |
|    n_updates       | 108054   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 69.6     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 12       |
|    time_elapsed    | 11121    |
|    total timesteps | 136068   |
| train/             |          |
|    actor_loss      | -1.59    |
|    critic_loss     | 0.0205   |
|    learning_rate   | 0.000867 |
|    n_updates       | 116058   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-4.68 +/- 27.00
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -4.68    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -1.6     |
|    critic_loss     | 0.0211   |
|    learning_rate   | 0.000863 |
|    n_updates       | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 68.7     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 11       |
|    time_elapsed    | 12211    |
|    total timesteps | 144072   |
| train/             |          |
|    actor_loss      | -1.61    |
|    critic_loss     | 0.0203   |
|    learning_rate   | 0.000859 |
|    n_updates       | 124062   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 68.6     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 11       |
|    time_elapsed    | 13117    |
|    total timesteps | 152076   |
| train/             |          |
|    actor_loss      | -1.62    |
|    critic_loss     | 0.0196   |
|    learning_rate   | 0.000851 |
|    n_updates       | 132066   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-1.57 +/- 23.50
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.57    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -1.61    |
|    critic_loss     | 0.0191   |
|    learning_rate   | 0.000844 |
|    n_updates       | 140070   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 68.1     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 11       |
|    time_elapsed    | 14228    |
|    total timesteps | 160080   |
---------------------------------
Terminated
2021-12-14 01:56:21.924416: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-14 01:56:21.924453: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_12
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 147      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 97       |
|    time_elapsed    | 82       |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 112      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 97       |
|    time_elapsed    | 163      |
|    total timesteps | 16008    |
---------------------------------
Eval num_timesteps=20000, episode_reward=0.82 +/- 53.78
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 0.822    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
Traceback (most recent call last):
  File "train.py", line 75, in <module>
    model.learn(args.timesteps)
  File "/home/shandilya/Desktop/Projects/NavigationController/learning/explore.py", line 181, in learn
    callback = self.rl_callback
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 211, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 369, in learn
    self.train(batch_size=self.batch_size, gradient_steps=gradient_steps)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 145, in train
    replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/buffers.py", line 583, in sample
    return super(ReplayBuffer, self).sample(batch_size=batch_size, env=env)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/buffers.py", line 110, in sample
    return self._get_samples(batch_inds, env=env)
  File "/home/shandilya/Desktop/Projects/NavigationController/utils/td3_utils.py", line 950, in _get_samples
    actions = self.actions[batch_inds, 0, :]
IndexError: too many indices for array: array is 2-dimensional, but 3 were indexed
2021-12-14 02:32:16.401816: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-14 02:32:16.401853: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_16
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 72.9     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 18       |
|    time_elapsed    | 442      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | -0.29    |
|    critic_loss     | 0.387    |
|    learning_rate   | 0.000994 |
|    n_updates       | 6003     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 67       |
| time/              |          |
|    episodes        | 8        |
|    fps             | 15       |
|    time_elapsed    | 1010     |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | -6.09    |
|    critic_loss     | 0.911    |
|    learning_rate   | 0.000986 |
|    n_updates       | 14007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=18.75 +/- 32.77
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 18.8     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -7.69    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.000982 |
|    n_updates       | 18009    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 66.2     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 13       |
|    time_elapsed    | 1793     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | -8.5     |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.000978 |
|    n_updates       | 22011    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 61.1     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 13       |
|    time_elapsed    | 2368     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | -9.73    |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.00097  |
|    n_updates       | 30015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=36.90 +/- 46.52
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 36.9     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 1.81     |
|    learning_rate   | 0.000962 |
|    n_updates       | 38019    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 63.5     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 12       |
|    time_elapsed    | 3157     |
|    total timesteps | 40020    |
---------------------------------
Terminated
2021-12-14 03:28:45.521642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-14 03:28:45.521679: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Using cpu device
Logging to assets/out/models/exp14/TD3_17
Terminated
running build_ext
building 'mujoco_py.cymj' extension
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o -fopenmp -w
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py
gcc -pthread -shared /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mjpro150/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mjpro150/bin -lmujoco150 -lglewegl -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp
2021-12-14 12:52:21.103726: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-14 12:52:21.103815: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp14/TD3_18
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 34.9     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 78       |
|    time_elapsed    | 102      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 32.1     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 80       |
|    time_elapsed    | 198      |
|    total timesteps | 16008    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=20000, episode_reward=-295.55 +/- 97.22
Episode length: 1777.80 +/- 446.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.78e+03 |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 24.9     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 38       |
|    time_elapsed    | 621      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 4.71     |
|    critic_loss     | 22.3     |
|    learning_rate   | 0.000978 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 22.8     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 27       |
|    time_elapsed    | 1143     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 6        |
|    critic_loss     | 114      |
|    learning_rate   | 0.00097  |
|    n_updates       | 12006    |
---------------------------------
Terminated
2021-12-14 13:21:09.565993: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-14 13:21:09.566066: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Env Type: maze
Task: CustomGoalReward4Rooms
Using cuda device
Logging to assets/out/models/exp14/TD3_19
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -102     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 78       |
|    time_elapsed    | 101      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -64.6    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 79       |
|    time_elapsed    | 201      |
|    total timesteps | 16008    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=20000, episode_reward=3.61 +/- 16.87
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 3.61     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -46.4    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 37       |
|    time_elapsed    | 645      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 11.8     |
|    critic_loss     | 28.7     |
|    learning_rate   | 0.000978 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -34.7    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 27       |
|    time_elapsed    | 1171     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 8.48     |
|    critic_loss     | 104      |
|    learning_rate   | 0.00097  |
|    n_updates       | 12006    |
---------------------------------
Terminated
2021-12-14 13:49:18.139320: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-14 13:49:18.139391: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Env Type: maze
Task: CustomGoalReward4Rooms
Using cuda device
Logging to assets/out/models/exp14/TD3_20
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 79       |
|    time_elapsed    | 101      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -29      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 80       |
|    time_elapsed    | 199      |
|    total timesteps | 16008    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=20000, episode_reward=15.64 +/- 49.80
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.68    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 37       |
|    time_elapsed    | 648      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 5.09     |
|    critic_loss     | 26.6     |
|    learning_rate   | 0.000978 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -13.1    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 27       |
|    time_elapsed    | 1179     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 5.31     |
|    critic_loss     | 18.6     |
|    learning_rate   | 0.00097  |
|    n_updates       | 12006    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-30.65 +/- 31.06
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -30.6    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 3.74     |
|    critic_loss     | 17.8     |
|    learning_rate   | 0.000962 |
|    n_updates       | 20010    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.8     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 21       |
|    time_elapsed    | 1844     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -3.93    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 20       |
|    time_elapsed    | 2327     |
|    total timesteps | 47407    |
| train/             |          |
|    actor_loss      | 3.13     |
|    critic_loss     | 18       |
|    learning_rate   | 0.000955 |
|    n_updates       | 27397    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -2.66    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 19       |
|    time_elapsed    | 2844     |
|    total timesteps | 55411    |
| train/             |          |
|    actor_loss      | 2.63     |
|    critic_loss     | 16.8     |
|    learning_rate   | 0.000947 |
|    n_updates       | 35401    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-5.87 +/- 21.48
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -5.87    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 2.05     |
|    critic_loss     | 16.5     |
|    learning_rate   | 0.000941 |
|    n_updates       | 41404    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 4.83     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 18       |
|    time_elapsed    | 3494     |
|    total timesteps | 63415    |
| train/             |          |
|    actor_loss      | 1.87     |
|    critic_loss     | 16.5     |
|    learning_rate   | 0.000939 |
|    n_updates       | 43405    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 3.84     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 17       |
|    time_elapsed    | 4011     |
|    total timesteps | 71419    |
| train/             |          |
|    actor_loss      | 1.22     |
|    critic_loss     | 16.3     |
|    learning_rate   | 0.000931 |
|    n_updates       | 51409    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 17       |
|    time_elapsed    | 4494     |
|    total timesteps | 78852    |
| train/             |          |
|    actor_loss      | 1.19     |
|    critic_loss     | 16.9     |
|    learning_rate   | 0.000924 |
|    n_updates       | 58842    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-24.99 +/- 40.04
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -25      |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 1.21     |
|    critic_loss     | 16.8     |
|    learning_rate   | 0.000922 |
|    n_updates       | 60843    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 9.2      |
| time/              |          |
|    episodes        | 44       |
|    fps             | 16       |
|    time_elapsed    | 5150     |
|    total timesteps | 86856    |
| train/             |          |
|    actor_loss      | 1.8      |
|    critic_loss     | 16.7     |
|    learning_rate   | 0.000916 |
|    n_updates       | 66846    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 8.66     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 16       |
|    time_elapsed    | 5673     |
|    total timesteps | 94860    |
| train/             |          |
|    actor_loss      | 2.27     |
|    critic_loss     | 15.7     |
|    learning_rate   | 0.000908 |
|    n_updates       | 74850    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-23.98 +/- 26.15
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -24      |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 1.61     |
|    critic_loss     | 16.1     |
|    learning_rate   | 0.000902 |
|    n_updates       | 80853    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 8.72     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 16       |
|    time_elapsed    | 6288     |
|    total timesteps | 102314   |
| train/             |          |
|    actor_loss      | 1.53     |
|    critic_loss     | 16.3     |
|    learning_rate   | 0.000901 |
|    n_updates       | 82304    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 16       |
|    time_elapsed    | 6811     |
|    total timesteps | 110318   |
| train/             |          |
|    actor_loss      | 1.45     |
|    critic_loss     | 17       |
|    learning_rate   | 0.000893 |
|    n_updates       | 90308    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 16       |
|    time_elapsed    | 7339     |
|    total timesteps | 118322   |
| train/             |          |
|    actor_loss      | 1.16     |
|    critic_loss     | 16.6     |
|    learning_rate   | 0.000885 |
|    n_updates       | 98312    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-24.26 +/- 25.48
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -24.3    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 1.05     |
|    critic_loss     | 16.3     |
|    learning_rate   | 0.000883 |
|    n_updates       | 100313   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 11.4     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 15       |
|    time_elapsed    | 8004     |
|    total timesteps | 126326   |
| train/             |          |
|    actor_loss      | 0.77     |
|    critic_loss     | 15.9     |
|    learning_rate   | 0.000877 |
|    n_updates       | 106316   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 12.2     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 15       |
|    time_elapsed    | 8533     |
|    total timesteps | 134330   |
| train/             |          |
|    actor_loss      | 0.669    |
|    critic_loss     | 16.5     |
|    learning_rate   | 0.000869 |
|    n_updates       | 114320   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-45.60 +/- 29.48
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -45.6    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 0.578    |
|    critic_loss     | 16.7     |
|    learning_rate   | 0.000862 |
|    n_updates       | 121859   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 15       |
|    time_elapsed    | 9164     |
|    total timesteps | 141869   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 15       |
|    time_elapsed    | 9690     |
|    total timesteps | 149657   |
| train/             |          |
|    actor_loss      | 0.39     |
|    critic_loss     | 16.1     |
|    learning_rate   | 0.000854 |
|    n_updates       | 129863   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 15       |
|    time_elapsed    | 10211    |
|    total timesteps | 157661   |
| train/             |          |
|    actor_loss      | 0.252    |
|    critic_loss     | 16.1     |
|    learning_rate   | 0.000846 |
|    n_updates       | 137651   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-22.82 +/- 18.55
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -22.8    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 0.23     |
|    critic_loss     | 16.4     |
|    learning_rate   | 0.000842 |
|    n_updates       | 141653   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 15       |
|    time_elapsed    | 10882    |
|    total timesteps | 165665   |
| train/             |          |
|    actor_loss      | 0.146    |
|    critic_loss     | 16       |
|    learning_rate   | 0.000838 |
|    n_updates       | 145655   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 15       |
|    time_elapsed    | 11408    |
|    total timesteps | 173669   |
| train/             |          |
|    actor_loss      | -0.148   |
|    critic_loss     | 15.4     |
|    learning_rate   | 0.00083  |
|    n_updates       | 153659   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-22.96 +/- 49.40
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23      |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -0.471   |
|    critic_loss     | 15.2     |
|    learning_rate   | 0.000822 |
|    n_updates       | 161663   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 15       |
|    time_elapsed    | 12074    |
|    total timesteps | 181673   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 15       |
|    time_elapsed    | 12605    |
|    total timesteps | 189677   |
| train/             |          |
|    actor_loss      | -0.577   |
|    critic_loss     | 15.3     |
|    learning_rate   | 0.000814 |
|    n_updates       | 169667   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 15       |
|    time_elapsed    | 13132    |
|    total timesteps | 197681   |
| train/             |          |
|    actor_loss      | -0.541   |
|    critic_loss     | 15.7     |
|    learning_rate   | 0.000806 |
|    n_updates       | 177671   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-20.18 +/- 27.73
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -20.2    |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -0.507   |
|    critic_loss     | 15.7     |
|    learning_rate   | 0.000802 |
|    n_updates       | 181488   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    episodes        | 104      |
|    fps             | 14       |
|    time_elapsed    | 13792    |
|    total timesteps | 205500   |
| train/             |          |
|    actor_loss      | -0.462   |
|    critic_loss     | 15.8     |
|    learning_rate   | 0.000799 |
|    n_updates       | 185490   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 14       |
|    time_elapsed    | 14325    |
|    total timesteps | 213504   |
| train/             |          |
|    actor_loss      | -0.369   |
|    critic_loss     | 15.5     |
|    learning_rate   | 0.000791 |
|    n_updates       | 193494   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-27.70 +/- 21.70
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27.7    |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -0.2     |
|    critic_loss     | 15.8     |
|    learning_rate   | 0.000783 |
|    n_updates       | 200930   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 12.9     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 14       |
|    time_elapsed    | 14966    |
|    total timesteps | 220940   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 13       |
| time/              |          |
|    episodes        | 116      |
|    fps             | 14       |
|    time_elapsed    | 15503    |
|    total timesteps | 228944   |
| train/             |          |
|    actor_loss      | -0.0127  |
|    critic_loss     | 16.2     |
|    learning_rate   | 0.000775 |
|    n_updates       | 208934   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 11.4     |
| time/              |          |
|    episodes        | 120      |
|    fps             | 14       |
|    time_elapsed    | 16043    |
|    total timesteps | 236948   |
| train/             |          |
|    actor_loss      | 0.22     |
|    critic_loss     | 15.9     |
|    learning_rate   | 0.000767 |
|    n_updates       | 216938   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-13.80 +/- 26.35
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -13.8    |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.386    |
|    critic_loss     | 16       |
|    learning_rate   | 0.000763 |
|    n_updates       | 220940   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 12.3     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 14       |
|    time_elapsed    | 16720    |
|    total timesteps | 244952   |
| train/             |          |
|    actor_loss      | 0.53     |
|    critic_loss     | 16       |
|    learning_rate   | 0.000759 |
|    n_updates       | 224942   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 12       |
| time/              |          |
|    episodes        | 128      |
|    fps             | 14       |
|    time_elapsed    | 17254    |
|    total timesteps | 252956   |
| train/             |          |
|    actor_loss      | 0.683    |
|    critic_loss     | 16       |
|    learning_rate   | 0.000752 |
|    n_updates       | 232946   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-36.97 +/- 18.13
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -37      |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 0.779    |
|    critic_loss     | 16.1     |
|    learning_rate   | 0.000744 |
|    n_updates       | 240950   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 14       |
|    time_elapsed    | 17926    |
|    total timesteps | 260960   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 10.6     |
| time/              |          |
|    episodes        | 136      |
|    fps             | 14       |
|    time_elapsed    | 18457    |
|    total timesteps | 268964   |
| train/             |          |
|    actor_loss      | 0.832    |
|    critic_loss     | 15.6     |
|    learning_rate   | 0.000736 |
|    n_updates       | 248954   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 8.53     |
| time/              |          |
|    episodes        | 140      |
|    fps             | 14       |
|    time_elapsed    | 18993    |
|    total timesteps | 276968   |
| train/             |          |
|    actor_loss      | 0.866    |
|    critic_loss     | 15.3     |
|    learning_rate   | 0.000728 |
|    n_updates       | 256958   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-27.17 +/- 25.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27.2    |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 0.87     |
|    critic_loss     | 15.4     |
|    learning_rate   | 0.000724 |
|    n_updates       | 260960   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 8.62     |
| time/              |          |
|    episodes        | 144      |
|    fps             | 14       |
|    time_elapsed    | 19667    |
|    total timesteps | 284972   |
| train/             |          |
|    actor_loss      | 0.876    |
|    critic_loss     | 15.7     |
|    learning_rate   | 0.00072  |
|    n_updates       | 264962   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 8.76     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 14       |
|    time_elapsed    | 20201    |
|    total timesteps | 292976   |
| train/             |          |
|    actor_loss      | 0.855    |
|    critic_loss     | 15.4     |
|    learning_rate   | 0.000712 |
|    n_updates       | 272966   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-58.33 +/- 26.43
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -58.3    |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 0.892    |
|    critic_loss     | 15.7     |
|    learning_rate   | 0.000704 |
|    n_updates       | 280970   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 9.7      |
| time/              |          |
|    episodes        | 152      |
|    fps             | 14       |
|    time_elapsed    | 20873    |
|    total timesteps | 300980   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 8.88     |
| time/              |          |
|    episodes        | 156      |
|    fps             | 14       |
|    time_elapsed    | 21406    |
|    total timesteps | 308984   |
| train/             |          |
|    actor_loss      | 0.811    |
|    critic_loss     | 15.6     |
|    learning_rate   | 0.000696 |
|    n_updates       | 288974   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 7.91     |
| time/              |          |
|    episodes        | 160      |
|    fps             | 14       |
|    time_elapsed    | 21937    |
|    total timesteps | 316988   |
| train/             |          |
|    actor_loss      | 0.784    |
|    critic_loss     | 15.6     |
|    learning_rate   | 0.000688 |
|    n_updates       | 296978   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-37.06 +/- 25.73
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -37.1    |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 0.773    |
|    critic_loss     | 15.5     |
|    learning_rate   | 0.000684 |
|    n_updates       | 300980   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 8.59     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 14       |
|    time_elapsed    | 22607    |
|    total timesteps | 324992   |
| train/             |          |
|    actor_loss      | 0.76     |
|    critic_loss     | 15.4     |
|    learning_rate   | 0.00068  |
|    n_updates       | 304982   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 8.14     |
| time/              |          |
|    episodes        | 168      |
|    fps             | 14       |
|    time_elapsed    | 23137    |
|    total timesteps | 332996   |
| train/             |          |
|    actor_loss      | 0.774    |
|    critic_loss     | 15.3     |
|    learning_rate   | 0.000672 |
|    n_updates       | 312986   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-16.32 +/- 15.61
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -16.3    |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | 0.812    |
|    critic_loss     | 15.5     |
|    learning_rate   | 0.000665 |
|    n_updates       | 320768   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 7.76     |
| time/              |          |
|    episodes        | 172      |
|    fps             | 14       |
|    time_elapsed    | 23791    |
|    total timesteps | 340778   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.66     |
| time/              |          |
|    episodes        | 176      |
|    fps             | 14       |
|    time_elapsed    | 24324    |
|    total timesteps | 348782   |
| train/             |          |
|    actor_loss      | 0.818    |
|    critic_loss     | 15.5     |
|    learning_rate   | 0.000657 |
|    n_updates       | 328772   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 7.17     |
| time/              |          |
|    episodes        | 180      |
|    fps             | 14       |
|    time_elapsed    | 24853    |
|    total timesteps | 356786   |
| train/             |          |
|    actor_loss      | 0.749    |
|    critic_loss     | 15.2     |
|    learning_rate   | 0.000649 |
|    n_updates       | 336776   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-15.44 +/- 10.72
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -15.4    |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | 0.725    |
|    critic_loss     | 15.3     |
|    learning_rate   | 0.000645 |
|    n_updates       | 340778   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.66     |
| time/              |          |
|    episodes        | 184      |
|    fps             | 14       |
|    time_elapsed    | 25525    |
|    total timesteps | 364790   |
| train/             |          |
|    actor_loss      | 0.73     |
|    critic_loss     | 15.1     |
|    learning_rate   | 0.000641 |
|    n_updates       | 344780   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.08     |
| time/              |          |
|    episodes        | 188      |
|    fps             | 14       |
|    time_elapsed    | 26056    |
|    total timesteps | 372794   |
| train/             |          |
|    actor_loss      | 0.676    |
|    critic_loss     | 15.2     |
|    learning_rate   | 0.000633 |
|    n_updates       | 352784   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-10.19 +/- 22.39
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.2    |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | 0.722    |
|    critic_loss     | 15.2     |
|    learning_rate   | 0.000625 |
|    n_updates       | 360788   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 0.875    |
| time/              |          |
|    episodes        | 192      |
|    fps             | 14       |
|    time_elapsed    | 26724    |
|    total timesteps | 380798   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    episodes        | 196      |
|    fps             | 14       |
|    time_elapsed    | 27255    |
|    total timesteps | 388802   |
| train/             |          |
|    actor_loss      | 0.705    |
|    critic_loss     | 15.1     |
|    learning_rate   | 0.000617 |
|    n_updates       | 368792   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.09     |
| time/              |          |
|    episodes        | 200      |
|    fps             | 14       |
|    time_elapsed    | 27786    |
|    total timesteps | 396806   |
| train/             |          |
|    actor_loss      | 0.624    |
|    critic_loss     | 14.9     |
|    learning_rate   | 0.000609 |
|    n_updates       | 376796   |
---------------------------------
Eval num_timesteps=400000, episode_reward=-48.65 +/- 17.66
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -48.6    |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | 0.573    |
|    critic_loss     | 14.9     |
|    learning_rate   | 0.000605 |
|    n_updates       | 380798   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.39     |
| time/              |          |
|    episodes        | 204      |
|    fps             | 14       |
|    time_elapsed    | 28455    |
|    total timesteps | 404810   |
| train/             |          |
|    actor_loss      | 0.558    |
|    critic_loss     | 14.9     |
|    learning_rate   | 0.000601 |
|    n_updates       | 384800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 7.52     |
| time/              |          |
|    episodes        | 208      |
|    fps             | 14       |
|    time_elapsed    | 28984    |
|    total timesteps | 412814   |
| train/             |          |
|    actor_loss      | 0.563    |
|    critic_loss     | 14.7     |
|    learning_rate   | 0.000593 |
|    n_updates       | 392804   |
---------------------------------
Eval num_timesteps=420000, episode_reward=-20.53 +/- 26.03
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -20.5    |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | 0.695    |
|    critic_loss     | 15.3     |
|    learning_rate   | 0.000585 |
|    n_updates       | 400808   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 7.83     |
| time/              |          |
|    episodes        | 212      |
|    fps             | 14       |
|    time_elapsed    | 29652    |
|    total timesteps | 420818   |
---------------------------------
Terminated
Terminated
2021-12-14 22:12:15.931000: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-14 22:12:15.931080: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Env Type: maze
Task: CustomGoalReward4Rooms
Using cuda device
Logging to assets/out/models/exp14/TD3_26
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 21.7     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 79       |
|    time_elapsed    | 101      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.92e+03 |
|    ep_rew_mean     | 23.7     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 79       |
|    time_elapsed    | 193      |
|    total timesteps | 15379    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=20000, episode_reward=-21.64 +/- 47.72
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -21.6    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | 22.7     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 43       |
|    time_elapsed    | 541      |
|    total timesteps | 23383    |
| train/             |          |
|    actor_loss      | 9.28     |
|    critic_loss     | 40.7     |
|    learning_rate   | 0.000979 |
|    n_updates       | 2001     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | 15       |
| time/              |          |
|    episodes        | 16       |
|    fps             | 29       |
|    time_elapsed    | 1076     |
|    total timesteps | 31387    |
| train/             |          |
|    actor_loss      | 4.19     |
|    critic_loss     | 17.7     |
|    learning_rate   | 0.000971 |
|    n_updates       | 10005    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 8.65     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 24       |
|    time_elapsed    | 1610     |
|    total timesteps | 39391    |
| train/             |          |
|    actor_loss      | 2.25     |
|    critic_loss     | 17.2     |
|    learning_rate   | 0.000963 |
|    n_updates       | 18009    |
---------------------------------
Eval num_timesteps=40000, episode_reward=7.42 +/- 48.14
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 7.42     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 2.5      |
|    critic_loss     | 16.5     |
|    learning_rate   | 0.000961 |
|    n_updates       | 20010    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | 9.5      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 20       |
|    time_elapsed    | 2288     |
|    total timesteps | 47395    |
| train/             |          |
|    actor_loss      | 1.65     |
|    critic_loss     | 16       |
|    learning_rate   | 0.000955 |
|    n_updates       | 26013    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 6.21     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 19       |
|    time_elapsed    | 2822     |
|    total timesteps | 55399    |
| train/             |          |
|    actor_loss      | 0.492    |
|    critic_loss     | 15.7     |
|    learning_rate   | 0.000947 |
|    n_updates       | 34017    |
---------------------------------
Eval num_timesteps=60000, episode_reward=11.52 +/- 11.08
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.0586   |
|    critic_loss     | 16       |
|    learning_rate   | 0.000941 |
|    n_updates       | 40020    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 18       |
|    time_elapsed    | 3497     |
|    total timesteps | 63403    |
| train/             |          |
|    actor_loss      | -0.0203  |
|    critic_loss     | 15.6     |
|    learning_rate   | 0.000939 |
|    n_updates       | 42021    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | 5.22     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 17       |
|    time_elapsed    | 4033     |
|    total timesteps | 71407    |
| train/             |          |
|    actor_loss      | -0.247   |
|    critic_loss     | 15.5     |
|    learning_rate   | 0.000931 |
|    n_updates       | 50025    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.02     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 17       |
|    time_elapsed    | 4564     |
|    total timesteps | 79411    |
| train/             |          |
|    actor_loss      | -0.458   |
|    critic_loss     | 15.4     |
|    learning_rate   | 0.000923 |
|    n_updates       | 58029    |
---------------------------------
Eval num_timesteps=80000, episode_reward=33.40 +/- 41.07
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 33.4     |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -0.451   |
|    critic_loss     | 15.1     |
|    learning_rate   | 0.000921 |
|    n_updates       | 60030    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.77     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 16       |
|    time_elapsed    | 5240     |
|    total timesteps | 87415    |
| train/             |          |
|    actor_loss      | -0.649   |
|    critic_loss     | 14.7     |
|    learning_rate   | 0.000915 |
|    n_updates       | 66033    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.89     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 16       |
|    time_elapsed    | 5775     |
|    total timesteps | 95419    |
| train/             |          |
|    actor_loss      | -0.608   |
|    critic_loss     | 15.4     |
|    learning_rate   | 0.000908 |
|    n_updates       | 74037    |
---------------------------------
Eval num_timesteps=100000, episode_reward=18.59 +/- 36.56
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 18.6     |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -0.493   |
|    critic_loss     | 15.7     |
|    learning_rate   | 0.000902 |
|    n_updates       | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.32     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 16       |
|    time_elapsed    | 6454     |
|    total timesteps | 103423   |
| train/             |          |
|    actor_loss      | -0.456   |
|    critic_loss     | 15.8     |
|    learning_rate   | 0.0009   |
|    n_updates       | 82041    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 0.185    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 15       |
|    time_elapsed    | 6986     |
|    total timesteps | 111427   |
| train/             |          |
|    actor_loss      | -0.348   |
|    critic_loss     | 16       |
|    learning_rate   | 0.000892 |
|    n_updates       | 90045    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.87     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 15       |
|    time_elapsed    | 7520     |
|    total timesteps | 119431   |
| train/             |          |
|    actor_loss      | -0.18    |
|    critic_loss     | 15.7     |
|    learning_rate   | 0.000884 |
|    n_updates       | 98049    |
---------------------------------
Eval num_timesteps=120000, episode_reward=27.42 +/- 15.69
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 27.4     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -0.17    |
|    critic_loss     | 15.5     |
|    learning_rate   | 0.000882 |
|    n_updates       | 100050   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 15       |
|    time_elapsed    | 8198     |
|    total timesteps | 127435   |
| train/             |          |
|    actor_loss      | -0.285   |
|    critic_loss     | 15.7     |
|    learning_rate   | 0.000876 |
|    n_updates       | 106053   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.18     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 15       |
|    time_elapsed    | 8731     |
|    total timesteps | 135439   |
| train/             |          |
|    actor_loss      | -0.339   |
|    critic_loss     | 15.6     |
|    learning_rate   | 0.000868 |
|    n_updates       | 114057   |
---------------------------------
Eval num_timesteps=140000, episode_reward=31.08 +/- 24.08
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 31.1     |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -0.412   |
|    critic_loss     | 15.2     |
|    learning_rate   | 0.000862 |
|    n_updates       | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4        |
| time/              |          |
|    episodes        | 72       |
|    fps             | 15       |
|    time_elapsed    | 9414     |
|    total timesteps | 143443   |
| train/             |          |
|    actor_loss      | -0.425   |
|    critic_loss     | 15.2     |
|    learning_rate   | 0.00086  |
|    n_updates       | 122061   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.1      |
| time/              |          |
|    episodes        | 76       |
|    fps             | 15       |
|    time_elapsed    | 9948     |
|    total timesteps | 151447   |
| train/             |          |
|    actor_loss      | -0.617   |
|    critic_loss     | 14.7     |
|    learning_rate   | 0.000852 |
|    n_updates       | 130065   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 8.22     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 15       |
|    time_elapsed    | 10480    |
|    total timesteps | 159451   |
| train/             |          |
|    actor_loss      | -0.829   |
|    critic_loss     | 14.7     |
|    learning_rate   | 0.000844 |
|    n_updates       | 138069   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-6.89 +/- 32.41
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -6.89    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -0.882   |
|    critic_loss     | 14.9     |
|    learning_rate   | 0.000842 |
|    n_updates       | 140070   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 9.5      |
| time/              |          |
|    episodes        | 84       |
|    fps             | 15       |
|    time_elapsed    | 11116    |
|    total timesteps | 166829   |
| train/             |          |
|    actor_loss      | -1.08    |
|    critic_loss     | 14.8     |
|    learning_rate   | 0.000837 |
|    n_updates       | 145447   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 15       |
|    time_elapsed    | 11644    |
|    total timesteps | 174833   |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 14.7     |
|    learning_rate   | 0.000829 |
|    n_updates       | 153451   |
---------------------------------
Eval num_timesteps=180000, episode_reward=37.97 +/- 31.13
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 38       |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -1.43    |
|    critic_loss     | 14.7     |
|    learning_rate   | 0.000823 |
|    n_updates       | 159454   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 9.08     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 14       |
|    time_elapsed    | 12318    |
|    total timesteps | 182837   |
| train/             |          |
|    actor_loss      | -1.47    |
|    critic_loss     | 14.7     |
|    learning_rate   | 0.000821 |
|    n_updates       | 161455   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 8.75     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 14       |
|    time_elapsed    | 12846    |
|    total timesteps | 190841   |
| train/             |          |
|    actor_loss      | -1.56    |
|    critic_loss     | 14.6     |
|    learning_rate   | 0.000813 |
|    n_updates       | 169459   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 8.86     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 14       |
|    time_elapsed    | 13377    |
|    total timesteps | 198845   |
| train/             |          |
|    actor_loss      | -1.59    |
|    critic_loss     | 14.6     |
|    learning_rate   | 0.000805 |
|    n_updates       | 177463   |
---------------------------------
Eval num_timesteps=200000, episode_reward=26.13 +/- 7.10
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 26.1     |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -1.58    |
|    critic_loss     | 14.3     |
|    learning_rate   | 0.000803 |
|    n_updates       | 179464   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.76     |
| time/              |          |
|    episodes        | 104      |
|    fps             | 14       |
|    time_elapsed    | 14047    |
|    total timesteps | 206849   |
| train/             |          |
|    actor_loss      | -1.57    |
|    critic_loss     | 14.5     |
|    learning_rate   | 0.000797 |
|    n_updates       | 185467   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.96     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 14       |
|    time_elapsed    | 14577    |
|    total timesteps | 214853   |
| train/             |          |
|    actor_loss      | -1.46    |
|    critic_loss     | 14.9     |
|    learning_rate   | 0.000789 |
|    n_updates       | 193471   |
---------------------------------
Eval num_timesteps=220000, episode_reward=6.00 +/- 45.24
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 6        |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -1.43    |
|    critic_loss     | 14.9     |
|    learning_rate   | 0.000783 |
|    n_updates       | 199474   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.67     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 14       |
|    time_elapsed    | 15256    |
|    total timesteps | 222857   |
| train/             |          |
|    actor_loss      | -1.42    |
|    critic_loss     | 14.7     |
|    learning_rate   | 0.000781 |
|    n_updates       | 201475   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.8      |
| time/              |          |
|    episodes        | 116      |
|    fps             | 14       |
|    time_elapsed    | 15792    |
|    total timesteps | 230861   |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 14.9     |
|    learning_rate   | 0.000773 |
|    n_updates       | 209479   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 8.4      |
| time/              |          |
|    episodes        | 120      |
|    fps             | 14       |
|    time_elapsed    | 16323    |
|    total timesteps | 238865   |
| train/             |          |
|    actor_loss      | -1.46    |
|    critic_loss     | 14.9     |
|    learning_rate   | 0.000766 |
|    n_updates       | 217483   |
---------------------------------
Eval num_timesteps=240000, episode_reward=34.46 +/- 12.84
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 34.5     |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -1.47    |
|    critic_loss     | 14.9     |
|    learning_rate   | 0.000764 |
|    n_updates       | 219484   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 8.42     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 14       |
|    time_elapsed    | 16982    |
|    total timesteps | 246672   |
| train/             |          |
|    actor_loss      | -1.52    |
|    critic_loss     | 14.6     |
|    learning_rate   | 0.000758 |
|    n_updates       | 225290   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 7.34     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 14       |
|    time_elapsed    | 17514    |
|    total timesteps | 254676   |
| train/             |          |
|    actor_loss      | -1.54    |
|    critic_loss     | 14.8     |
|    learning_rate   | 0.00075  |
|    n_updates       | 233294   |
---------------------------------
Eval num_timesteps=260000, episode_reward=14.67 +/- 14.59
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -1.45    |
|    critic_loss     | 14.9     |
|    learning_rate   | 0.000744 |
|    n_updates       | 239297   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 14       |
|    time_elapsed    | 18189    |
|    total timesteps | 262680   |
| train/             |          |
|    actor_loss      | -1.43    |
|    critic_loss     | 14.9     |
|    learning_rate   | 0.000742 |
|    n_updates       | 241298   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.78     |
| time/              |          |
|    episodes        | 136      |
|    fps             | 14       |
|    time_elapsed    | 18716    |
|    total timesteps | 270684   |
| train/             |          |
|    actor_loss      | -1.4     |
|    critic_loss     | 15.2     |
|    learning_rate   | 0.000734 |
|    n_updates       | 249302   |
---------------------------------
/root/trainer/simulations/point.py:67: RuntimeWarning: invalid value encountered in true_divide
  depth = 255 * ((depth - depth.min()) / (depth.max() - depth.min()))
Traceback (most recent call last):
  File "train.py", line 75, in <module>
    policy_version = args.policy_version,
  File "/root/trainer/learning/explore.py", line 186, in learn
    save_path = self.logdir,
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 211, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 359, in learn
    log_interval=log_interval,
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 566, in collect_rollouts
    new_obs, reward, done, infos = env.step(action)
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 162, in step
    return self.step_wait()
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/vec_transpose.py", line 83, in step_wait
    observations, rewards, dones, infos = self.venv.step_wait()
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 44, in step_wait
    self.actions[env_idx]
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/monitor.py", line 90, in step
    observation, reward, done, info = self.env.step(action)
  File "/root/trainer/simulations/maze_env.py", line 571, in step
    inner_next_obs, inner_reward, _, info = self.wrapped_env.step(action)
  File "/root/trainer/simulations/point.py", line 54, in step
    self.sim.step()
  File "mujoco_py/mjsim.pyx", line 119, in mujoco_py.cymj.MjSim.step
  File "mujoco_py/cymj.pyx", line 115, in mujoco_py.cymj.wrap_mujoco_warning.__exit__
  File "mujoco_py/cymj.pyx", line 75, in mujoco_py.cymj.c_warning_callback
  File "/usr/local/lib/python3.6/site-packages/mujoco_py/builder.py", line 354, in user_warning_raise_exception
    raise MujocoException(warn + 'Check for NaN in simulation.')
mujoco_py.builder.MujocoException: Unknown warning type Time = 35.0000.Check for NaN in simulation.
2021-12-15 13:25:23.835215: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-15 13:25:23.835259: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Env Type: maze
Task: CustomGoalReward4Rooms
Using cpu device
Logging to assets/out/models/exp14/TD3_27
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 33.7     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 14       |
|    time_elapsed    | 556      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 134      |
|    critic_loss     | 127      |
|    learning_rate   | 0.000994 |
|    n_updates       | 6003     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 36.5     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 12       |
|    time_elapsed    | 1249     |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 126      |
|    critic_loss     | 57.4     |
|    learning_rate   | 0.000986 |
|    n_updates       | 14007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-14.50 +/- 43.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.5    |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 114      |
|    critic_loss     | 53.4     |
|    learning_rate   | 0.000982 |
|    n_updates       | 18009    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 28.7     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 10       |
|    time_elapsed    | 2218     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 102      |
|    critic_loss     | 48.7     |
|    learning_rate   | 0.000978 |
|    n_updates       | 22011    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 19.8     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 10       |
|    time_elapsed    | 2974     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 80.8     |
|    critic_loss     | 24.6     |
|    learning_rate   | 0.00097  |
|    n_updates       | 30015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-6.41 +/- 33.87
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -6.41    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 63.9     |
|    critic_loss     | 14.5     |
|    learning_rate   | 0.000962 |
|    n_updates       | 38019    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 20.3     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 10       |
|    time_elapsed    | 3947     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 10       |
|    time_elapsed    | 4674     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 50.9     |
|    critic_loss     | 13       |
|    learning_rate   | 0.000954 |
|    n_updates       | 46023    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 10       |
|    time_elapsed    | 5446     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 41.2     |
|    critic_loss     | 8.76     |
|    learning_rate   | 0.000947 |
|    n_updates       | 54027    |
---------------------------------
Eval num_timesteps=60000, episode_reward=16.68 +/- 26.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 16.7     |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 36.5     |
|    critic_loss     | 7.32     |
|    learning_rate   | 0.000943 |
|    n_updates       | 58029    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 14.7     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 9        |
|    time_elapsed    | 6432     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 32.4     |
|    critic_loss     | 7.1      |
|    learning_rate   | 0.000939 |
|    n_updates       | 62031    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 20.6     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 9        |
|    time_elapsed    | 7252     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 25.1     |
|    critic_loss     | 5.44     |
|    learning_rate   | 0.000931 |
|    n_updates       | 70035    |
---------------------------------
Eval num_timesteps=80000, episode_reward=13.53 +/- 27.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 13.5     |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 19.4     |
|    critic_loss     | 4.53     |
|    learning_rate   | 0.000923 |
|    n_updates       | 78039    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 9        |
|    time_elapsed    | 8302     |
|    total timesteps | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 12.4     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 9        |
|    time_elapsed    | 9169     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 15       |
|    critic_loss     | 4.04     |
|    learning_rate   | 0.000915 |
|    n_updates       | 86043    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 7.16     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 9        |
|    time_elapsed    | 10090    |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 11.6     |
|    critic_loss     | 3.9      |
|    learning_rate   | 0.000907 |
|    n_updates       | 94047    |
---------------------------------
Eval num_timesteps=100000, episode_reward=32.35 +/- 28.20
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 32.3     |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 10.2     |
|    critic_loss     | 4.35     |
|    learning_rate   | 0.000903 |
|    n_updates       | 98049    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.04     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 9        |
|    time_elapsed    | 11258    |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | 9.08     |
|    critic_loss     | 3.77     |
|    learning_rate   | 0.000899 |
|    n_updates       | 102051   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.97     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 9        |
|    time_elapsed    | 12215    |
|    total timesteps | 112056   |
| train/             |          |
|    actor_loss      | 7        |
|    critic_loss     | 3.7      |
|    learning_rate   | 0.000891 |
|    n_updates       | 110055   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-12.16 +/- 30.35
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -12.2    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 5.44     |
|    critic_loss     | 4.15     |
|    learning_rate   | 0.000883 |
|    n_updates       | 118059   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.07     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 8        |
|    time_elapsed    | 13394    |
|    total timesteps | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.4      |
| time/              |          |
|    episodes        | 64       |
|    fps             | 8        |
|    time_elapsed    | 14349    |
|    total timesteps | 128064   |
| train/             |          |
|    actor_loss      | 4.17     |
|    critic_loss     | 3.86     |
|    learning_rate   | 0.000875 |
|    n_updates       | 126063   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.2     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 8        |
|    time_elapsed    | 15320    |
|    total timesteps | 136068   |
| train/             |          |
|    actor_loss      | 3.24     |
|    critic_loss     | 4.09     |
|    learning_rate   | 0.000867 |
|    n_updates       | 134067   |
---------------------------------
Eval num_timesteps=140000, episode_reward=31.82 +/- 49.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 31.8     |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 2.92     |
|    critic_loss     | 4.06     |
|    learning_rate   | 0.000863 |
|    n_updates       | 138069   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.95    |
| time/              |          |
|    episodes        | 72       |
|    fps             | 8        |
|    time_elapsed    | 16489    |
|    total timesteps | 144072   |
| train/             |          |
|    actor_loss      | 2.5      |
|    critic_loss     | 3.72     |
|    learning_rate   | 0.000859 |
|    n_updates       | 142071   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.01    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 8        |
|    time_elapsed    | 17455    |
|    total timesteps | 152076   |
| train/             |          |
|    actor_loss      | 2.27     |
|    critic_loss     | 4.1      |
|    learning_rate   | 0.000851 |
|    n_updates       | 150075   |
---------------------------------
Eval num_timesteps=160000, episode_reward=28.06 +/- 12.60
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 28.1     |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 1.92     |
|    critic_loss     | 3.88     |
|    learning_rate   | 0.000844 |
|    n_updates       | 158079   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.985    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 8        |
|    time_elapsed    | 18630    |
|    total timesteps | 160080   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.21     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 8        |
|    time_elapsed    | 19611    |
|    total timesteps | 168084   |
| train/             |          |
|    actor_loss      | 2.07     |
|    critic_loss     | 3.71     |
|    learning_rate   | 0.000836 |
|    n_updates       | 166083   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.66     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 8        |
|    time_elapsed    | 20586    |
|    total timesteps | 176088   |
| train/             |          |
|    actor_loss      | 1.89     |
|    critic_loss     | 3.85     |
|    learning_rate   | 0.000828 |
|    n_updates       | 174087   |
---------------------------------
Eval num_timesteps=180000, episode_reward=20.24 +/- 57.17
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 20.2     |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 1.74     |
|    critic_loss     | 4        |
|    learning_rate   | 0.000824 |
|    n_updates       | 178089   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.43     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 8        |
|    time_elapsed    | 21774    |
|    total timesteps | 184092   |
| train/             |          |
|    actor_loss      | 1.63     |
|    critic_loss     | 3.46     |
|    learning_rate   | 0.00082  |
|    n_updates       | 182091   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 8        |
|    time_elapsed    | 22734    |
|    total timesteps | 192096   |
| train/             |          |
|    actor_loss      | 1.64     |
|    critic_loss     | 3.8      |
|    learning_rate   | 0.000812 |
|    n_updates       | 190095   |
---------------------------------
Eval num_timesteps=200000, episode_reward=34.24 +/- 16.87
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 34.2     |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 1.55     |
|    critic_loss     | 3.61     |
|    learning_rate   | 0.000804 |
|    n_updates       | 198099   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3        |
| time/              |          |
|    episodes        | 100      |
|    fps             | 8        |
|    time_elapsed    | 23903    |
|    total timesteps | 200100   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.32     |
| time/              |          |
|    episodes        | 104      |
|    fps             | 8        |
|    time_elapsed    | 24876    |
|    total timesteps | 208104   |
| train/             |          |
|    actor_loss      | 1.31     |
|    critic_loss     | 4.05     |
|    learning_rate   | 0.000796 |
|    n_updates       | 206103   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 8        |
|    time_elapsed    | 25851    |
|    total timesteps | 216108   |
| train/             |          |
|    actor_loss      | 1.1      |
|    critic_loss     | 3.63     |
|    learning_rate   | 0.000788 |
|    n_updates       | 214107   |
---------------------------------
Eval num_timesteps=220000, episode_reward=43.13 +/- 38.33
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 43.1     |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 0.936    |
|    critic_loss     | 3.49     |
|    learning_rate   | 0.000784 |
|    n_updates       | 218109   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 8        |
|    time_elapsed    | 27023    |
|    total timesteps | 224112   |
| train/             |          |
|    actor_loss      | 0.837    |
|    critic_loss     | 3.66     |
|    learning_rate   | 0.00078  |
|    n_updates       | 222111   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 8        |
|    time_elapsed    | 27974    |
|    total timesteps | 232116   |
| train/             |          |
|    actor_loss      | 0.736    |
|    critic_loss     | 3.84     |
|    learning_rate   | 0.000772 |
|    n_updates       | 230115   |
---------------------------------
Eval num_timesteps=240000, episode_reward=6.61 +/- 33.39
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 6.61     |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.618    |
|    critic_loss     | 3.96     |
|    learning_rate   | 0.000764 |
|    n_updates       | 238119   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.14     |
| time/              |          |
|    episodes        | 120      |
|    fps             | 8        |
|    time_elapsed    | 29146    |
|    total timesteps | 240120   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.36     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 8        |
|    time_elapsed    | 30118    |
|    total timesteps | 248124   |
| train/             |          |
|    actor_loss      | 0.422    |
|    critic_loss     | 3.12     |
|    learning_rate   | 0.000756 |
|    n_updates       | 246123   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.06     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 8        |
|    time_elapsed    | 31082    |
|    total timesteps | 256128   |
| train/             |          |
|    actor_loss      | 0.175    |
|    critic_loss     | 3.59     |
|    learning_rate   | 0.000748 |
|    n_updates       | 254127   |
---------------------------------
Eval num_timesteps=260000, episode_reward=3.13 +/- 16.37
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 3.13     |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 0.0453   |
|    critic_loss     | 3.42     |
|    learning_rate   | 0.000744 |
|    n_updates       | 258129   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 8        |
|    time_elapsed    | 32264    |
|    total timesteps | 264132   |
| train/             |          |
|    actor_loss      | -0.0628  |
|    critic_loss     | 3.5      |
|    learning_rate   | 0.00074  |
|    n_updates       | 262131   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.73     |
| time/              |          |
|    episodes        | 136      |
|    fps             | 8        |
|    time_elapsed    | 33233    |
|    total timesteps | 272136   |
| train/             |          |
|    actor_loss      | -0.305   |
|    critic_loss     | 3.66     |
|    learning_rate   | 0.000733 |
|    n_updates       | 270135   |
---------------------------------
Eval num_timesteps=280000, episode_reward=36.28 +/- 43.93
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 36.3     |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -0.549   |
|    critic_loss     | 3.19     |
|    learning_rate   | 0.000725 |
|    n_updates       | 278139   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 7.76     |
| time/              |          |
|    episodes        | 140      |
|    fps             | 8        |
|    time_elapsed    | 34400    |
|    total timesteps | 280140   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.54     |
| time/              |          |
|    episodes        | 144      |
|    fps             | 8        |
|    time_elapsed    | 35357    |
|    total timesteps | 288144   |
| train/             |          |
|    actor_loss      | -0.86    |
|    critic_loss     | 2.93     |
|    learning_rate   | 0.000717 |
|    n_updates       | 286143   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 8.63     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 8        |
|    time_elapsed    | 36315    |
|    total timesteps | 296148   |
| train/             |          |
|    actor_loss      | -1.07    |
|    critic_loss     | 3.28     |
|    learning_rate   | 0.000709 |
|    n_updates       | 294147   |
---------------------------------
Eval num_timesteps=300000, episode_reward=3.71 +/- 23.73
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 3.71     |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -1.17    |
|    critic_loss     | 3.23     |
|    learning_rate   | 0.000705 |
|    n_updates       | 298149   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 10.1     |
| time/              |          |
|    episodes        | 152      |
|    fps             | 8        |
|    time_elapsed    | 37493    |
|    total timesteps | 304152   |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 2.72     |
|    learning_rate   | 0.000701 |
|    n_updates       | 302151   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.28     |
| time/              |          |
|    episodes        | 156      |
|    fps             | 8        |
|    time_elapsed    | 38468    |
|    total timesteps | 312156   |
| train/             |          |
|    actor_loss      | -1.36    |
|    critic_loss     | 3.17     |
|    learning_rate   | 0.000693 |
|    n_updates       | 310155   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.69     |
| time/              |          |
|    episodes        | 160      |
|    fps             | 8        |
|    time_elapsed    | 39432    |
|    total timesteps | 319512   |
| train/             |          |
|    actor_loss      | -1.32    |
|    critic_loss     | 3.56     |
|    learning_rate   | 0.000685 |
|    n_updates       | 318159   |
---------------------------------
Eval num_timesteps=320000, episode_reward=1.15 +/- 29.74
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 1.15     |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 3.3      |
|    learning_rate   | 0.000684 |
|    n_updates       | 319512   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.89     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 8        |
|    time_elapsed    | 40549    |
|    total timesteps | 327516   |
| train/             |          |
|    actor_loss      | -1.31    |
|    critic_loss     | 3        |
|    learning_rate   | 0.000678 |
|    n_updates       | 325515   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 8.39     |
| time/              |          |
|    episodes        | 168      |
|    fps             | 8        |
|    time_elapsed    | 41513    |
|    total timesteps | 335520   |
| train/             |          |
|    actor_loss      | -1.21    |
|    critic_loss     | 3.17     |
|    learning_rate   | 0.00067  |
|    n_updates       | 333519   |
---------------------------------
Terminated
