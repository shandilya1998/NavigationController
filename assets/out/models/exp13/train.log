running build_ext
building 'mujoco_py.cymj' extension
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o -fopenmp -w
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py
gcc -pthread -shared /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mjpro150/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mjpro150/bin -lmujoco150 -lglewegl -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp
2021-12-11 13:42:27.541429: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-11 13:42:27.541497: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp13/TD3_5
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -44.8    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 41       |
|    time_elapsed    | 193      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 0.0589   |
|    critic_loss     | 0.607    |
|    learning_rate   | 0.0005   |
|    n_updates       | 2001     |
---------------------------------
Eval num_timesteps=10000, episode_reward=2.01 +/- 26.46
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.01     |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 0.138    |
|    critic_loss     | 0.844    |
|    learning_rate   | 0.0005   |
|    n_updates       | 4002     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -34.9    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 20       |
|    time_elapsed    | 790      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 0.855    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 10005    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-29.76 +/- 21.51
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29.8    |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 0.893    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.0005   |
|    n_updates       | 14007    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -29.4    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 17       |
|    time_elapsed    | 1389     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 0.854    |
|    critic_loss     | 1.12     |
|    learning_rate   | 0.0005   |
|    n_updates       | 18009    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-23.83 +/- 34.12
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.8    |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 0.838    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 24012    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -39.4    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 16       |
|    time_elapsed    | 1984     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 0.844    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 26013    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-62.84 +/- 12.30
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -62.8    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.896    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 34017    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -36.9    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 15       |
|    time_elapsed    | 2577     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -30.7    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 15       |
|    time_elapsed    | 3057     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.892    |
|    critic_loss     | 1.18     |
|    learning_rate   | 0.0005   |
|    n_updates       | 42021    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-26.84 +/- 48.65
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -26.8    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.883    |
|    critic_loss     | 1.15     |
|    learning_rate   | 0.0005   |
|    n_updates       | 44022    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -29.9    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 15       |
|    time_elapsed    | 3654     |
|    total timesteps | 55886    |
| train/             |          |
|    actor_loss      | 0.886    |
|    critic_loss     | 1.15     |
|    learning_rate   | 0.0005   |
|    n_updates       | 50025    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-58.94 +/- 29.76
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -58.9    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.829    |
|    critic_loss     | 1.14     |
|    learning_rate   | 0.0005   |
|    n_updates       | 55886    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -22.3    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 15       |
|    time_elapsed    | 4246     |
|    total timesteps | 63890    |
| train/             |          |
|    actor_loss      | 0.803    |
|    critic_loss     | 1.1      |
|    learning_rate   | 0.0005   |
|    n_updates       | 57887    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-12.83 +/- 26.56
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -12.8    |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 0.723    |
|    critic_loss     | 1.09     |
|    learning_rate   | 0.0005   |
|    n_updates       | 65891    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -27.4    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 14       |
|    time_elapsed    | 4842     |
|    total timesteps | 71894    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -24.1    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 15       |
|    time_elapsed    | 5321     |
|    total timesteps | 79898    |
| train/             |          |
|    actor_loss      | 0.697    |
|    critic_loss     | 1.09     |
|    learning_rate   | 0.0005   |
|    n_updates       | 73895    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-7.56 +/- 31.98
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -7.56    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.687    |
|    critic_loss     | 1.1      |
|    learning_rate   | 0.0005   |
|    n_updates       | 75896    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -20.4    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 14       |
|    time_elapsed    | 5921     |
|    total timesteps | 87902    |
| train/             |          |
|    actor_loss      | 0.647    |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.0005   |
|    n_updates       | 81899    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-53.63 +/- 17.95
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -53.6    |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 0.616    |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.0005   |
|    n_updates       | 85901    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -21.2    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 14       |
|    time_elapsed    | 6521     |
|    total timesteps | 95906    |
| train/             |          |
|    actor_loss      | 0.596    |
|    critic_loss     | 1.07     |
|    learning_rate   | 0.0005   |
|    n_updates       | 89903    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-46.60 +/- 32.11
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -46.6    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.58     |
|    critic_loss     | 1.05     |
|    learning_rate   | 0.0005   |
|    n_updates       | 95906    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -18.9    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 14       |
|    time_elapsed    | 7124     |
|    total timesteps | 103910   |
| train/             |          |
|    actor_loss      | 0.567    |
|    critic_loss     | 1        |
|    learning_rate   | 0.0005   |
|    n_updates       | 97907    |
---------------------------------
Terminated
2021-12-11 15:52:20.148254: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-11 15:52:20.148326: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp13/TD3_6
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -13.7    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 81       |
|    time_elapsed    | 98       |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 9.14     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 82       |
|    time_elapsed    | 193      |
|    total timesteps | 16008    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-38.85 +/- 69.39
Episode length: 1568.80 +/- 556.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.57e+03 |
|    mean_reward     | -38.8    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 40       |
|    time_elapsed    | 589      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 0.112    |
|    critic_loss     | 0.0136   |
|    learning_rate   | 0.0005   |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 29       |
|    time_elapsed    | 1077     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 0.407    |
|    critic_loss     | 0.0137   |
|    learning_rate   | 0.0005   |
|    n_updates       | 12006    |
---------------------------------
Eval num_timesteps=40000, episode_reward=13.26 +/- 27.35
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 13.3     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.275    |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.0005   |
|    n_updates       | 20010    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 12.1     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 23       |
|    time_elapsed    | 1697     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 9.16     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 21       |
|    time_elapsed    | 2184     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.206    |
|    critic_loss     | 0.016    |
|    learning_rate   | 0.0005   |
|    n_updates       | 28014    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.73     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 20       |
|    time_elapsed    | 2674     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 0.174    |
|    critic_loss     | 0.0155   |
|    learning_rate   | 0.0005   |
|    n_updates       | 36018    |
---------------------------------
Eval num_timesteps=60000, episode_reward=3.52 +/- 31.95
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 3.52     |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.152    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.0005   |
|    n_updates       | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.29     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 19       |
|    time_elapsed    | 3292     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 0.133    |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.0005   |
|    n_updates       | 44022    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.49     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 19       |
|    time_elapsed    | 3783     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 0.105    |
|    critic_loss     | 0.0138   |
|    learning_rate   | 0.0005   |
|    n_updates       | 52026    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-19.63 +/- 35.08
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19.6    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.0766   |
|    critic_loss     | 0.0134   |
|    learning_rate   | 0.0005   |
|    n_updates       | 60030    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 7        |
| time/              |          |
|    episodes        | 40       |
|    fps             | 18       |
|    time_elapsed    | 4406     |
|    total timesteps | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.02    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 17       |
|    time_elapsed    | 4895     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 0.0662   |
|    critic_loss     | 0.0135   |
|    learning_rate   | 0.0005   |
|    n_updates       | 68034    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.17     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 17       |
|    time_elapsed    | 5386     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 0.0609   |
|    critic_loss     | 0.0134   |
|    learning_rate   | 0.0005   |
|    n_updates       | 76038    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-12.08 +/- 21.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -12.1    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.0524   |
|    critic_loss     | 0.0132   |
|    learning_rate   | 0.0005   |
|    n_updates       | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.23    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 17       |
|    time_elapsed    | 6010     |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | 0.0479   |
|    critic_loss     | 0.0136   |
|    learning_rate   | 0.0005   |
|    n_updates       | 84042    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.87    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 17       |
|    time_elapsed    | 6502     |
|    total timesteps | 112056   |
| train/             |          |
|    actor_loss      | 0.0485   |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.0005   |
|    n_updates       | 92046    |
---------------------------------
Eval num_timesteps=120000, episode_reward=2.84 +/- 36.97
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 0.0534   |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.0005   |
|    n_updates       | 100050   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.75    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 16       |
|    time_elapsed    | 7124     |
|    total timesteps | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.51    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 16       |
|    time_elapsed    | 7614     |
|    total timesteps | 128064   |
| train/             |          |
|    actor_loss      | 0.0544   |
|    critic_loss     | 0.0142   |
|    learning_rate   | 0.0005   |
|    n_updates       | 108054   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8       |
| time/              |          |
|    episodes        | 68       |
|    fps             | 16       |
|    time_elapsed    | 8105     |
|    total timesteps | 136068   |
| train/             |          |
|    actor_loss      | 0.0577   |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.0005   |
|    n_updates       | 116058   |
---------------------------------
Eval num_timesteps=140000, episode_reward=10.06 +/- 16.68
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 0.0577   |
|    critic_loss     | 0.0137   |
|    learning_rate   | 0.0005   |
|    n_updates       | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.54    |
| time/              |          |
|    episodes        | 72       |
|    fps             | 16       |
|    time_elapsed    | 8724     |
|    total timesteps | 144072   |
| train/             |          |
|    actor_loss      | 0.0584   |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.0005   |
|    n_updates       | 124062   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.35    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 16       |
|    time_elapsed    | 9215     |
|    total timesteps | 152076   |
| train/             |          |
|    actor_loss      | 0.0565   |
|    critic_loss     | 0.0143   |
|    learning_rate   | 0.0005   |
|    n_updates       | 132066   |
---------------------------------
Eval num_timesteps=160000, episode_reward=20.44 +/- 18.91
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 20.4     |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 0.0553   |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.0005   |
|    n_updates       | 140070   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.49    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 16       |
|    time_elapsed    | 9841     |
|    total timesteps | 160080   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.87    |
| time/              |          |
|    episodes        | 84       |
|    fps             | 16       |
|    time_elapsed    | 10330    |
|    total timesteps | 168084   |
| train/             |          |
|    actor_loss      | 0.0553   |
|    critic_loss     | 0.0148   |
|    learning_rate   | 0.0005   |
|    n_updates       | 148074   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.87    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 16       |
|    time_elapsed    | 10821    |
|    total timesteps | 176088   |
| train/             |          |
|    actor_loss      | 0.0557   |
|    critic_loss     | 0.0143   |
|    learning_rate   | 0.0005   |
|    n_updates       | 156078   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-14.98 +/- 54.79
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -15      |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 0.0536   |
|    critic_loss     | 0.0139   |
|    learning_rate   | 0.0005   |
|    n_updates       | 160080   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.47    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 16       |
|    time_elapsed    | 11443    |
|    total timesteps | 184092   |
| train/             |          |
|    actor_loss      | 0.0569   |
|    critic_loss     | 0.0145   |
|    learning_rate   | 0.0005   |
|    n_updates       | 164082   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.46    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 16       |
|    time_elapsed    | 11932    |
|    total timesteps | 192096   |
| train/             |          |
|    actor_loss      | 0.0587   |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.0005   |
|    n_updates       | 172086   |
---------------------------------
Eval num_timesteps=200000, episode_reward=14.94 +/- 22.82
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 0.0591   |
|    critic_loss     | 0.0141   |
|    learning_rate   | 0.0005   |
|    n_updates       | 180090   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.13    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 15       |
|    time_elapsed    | 12555    |
|    total timesteps | 200100   |
---------------------------------
Terminated
2021-12-11 19:37:52.600549: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-11 19:37:52.600628: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp13/TD3_8
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.16    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 19       |
|    time_elapsed    | 405      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 7.44     |
|    critic_loss     | 1.28     |
|    learning_rate   | 0.0005   |
|    n_updates       | 6003     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.997   |
| time/              |          |
|    episodes        | 8        |
|    fps             | 17       |
|    time_elapsed    | 908      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 4.16     |
|    critic_loss     | 0.903    |
|    learning_rate   | 0.0005   |
|    n_updates       | 14007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.39 +/- 40.79
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -0.395   |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 2.74     |
|    critic_loss     | 1.02     |
|    learning_rate   | 0.0005   |
|    n_updates       | 19248    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | 1.04     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 15       |
|    time_elapsed    | 1471     |
|    total timesteps | 23250    |
| train/             |          |
|    actor_loss      | 2.4      |
|    critic_loss     | 0.921    |
|    learning_rate   | 0.0005   |
|    n_updates       | 21249    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -8.73    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 15       |
|    time_elapsed    | 1960     |
|    total timesteps | 31254    |
| train/             |          |
|    actor_loss      | 1.28     |
|    critic_loss     | 0.995    |
|    learning_rate   | 0.0005   |
|    n_updates       | 29253    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -22.3    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 16       |
|    time_elapsed    | 2444     |
|    total timesteps | 39258    |
| train/             |          |
|    actor_loss      | 0.844    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 37257    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-23.75 +/- 26.25
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.7    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.795    |
|    critic_loss     | 1.17     |
|    learning_rate   | 0.0005   |
|    n_updates       | 39258    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -26.5    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 15       |
|    time_elapsed    | 3044     |
|    total timesteps | 47262    |
| train/             |          |
|    actor_loss      | 0.703    |
|    critic_loss     | 1.14     |
|    learning_rate   | 0.0005   |
|    n_updates       | 45261    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -23.7    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 15       |
|    time_elapsed    | 3531     |
|    total timesteps | 55266    |
| train/             |          |
|    actor_loss      | 0.595    |
|    critic_loss     | 1.16     |
|    learning_rate   | 0.0005   |
|    n_updates       | 53265    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-39.77 +/- 21.08
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -39.8    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.543    |
|    critic_loss     | 1.16     |
|    learning_rate   | 0.0005   |
|    n_updates       | 59268    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -21.9    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 15       |
|    time_elapsed    | 4136     |
|    total timesteps | 63270    |
| train/             |          |
|    actor_loss      | 0.521    |
|    critic_loss     | 1.15     |
|    learning_rate   | 0.0005   |
|    n_updates       | 61269    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -18.3    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 15       |
|    time_elapsed    | 4620     |
|    total timesteps | 71274    |
| train/             |          |
|    actor_loss      | 0.448    |
|    critic_loss     | 1.09     |
|    learning_rate   | 0.0005   |
|    n_updates       | 69273    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -17.8    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 15       |
|    time_elapsed    | 5109     |
|    total timesteps | 79278    |
| train/             |          |
|    actor_loss      | 0.403    |
|    critic_loss     | 1.14     |
|    learning_rate   | 0.0005   |
|    n_updates       | 77277    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-35.48 +/- 23.62
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -35.5    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.392    |
|    critic_loss     | 1.12     |
|    learning_rate   | 0.0005   |
|    n_updates       | 79278    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -19.5    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 15       |
|    time_elapsed    | 5716     |
|    total timesteps | 87282    |
| train/             |          |
|    actor_loss      | 0.355    |
|    critic_loss     | 1.12     |
|    learning_rate   | 0.0005   |
|    n_updates       | 85281    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -21.4    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 15       |
|    time_elapsed    | 6176     |
|    total timesteps | 94785    |
| train/             |          |
|    actor_loss      | 0.339    |
|    critic_loss     | 1.17     |
|    learning_rate   | 0.0005   |
|    n_updates       | 92784    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-36.91 +/- 44.17
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -36.9    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.337    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 98787    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -23.5    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 15       |
|    time_elapsed    | 6783     |
|    total timesteps | 102789   |
| train/             |          |
|    actor_loss      | 0.339    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 100788   |
---------------------------------
Terminated
2021-12-11 21:35:54.171356: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-11 21:35:54.171451: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp13/TD3_9
Terminated
2021-12-11 21:36:27.893238: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-11 21:36:27.893300: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp13/TD3_10
Found 1 GPUs for rendering. Using device 0.
Terminated
2021-12-11 21:39:09.457142: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-11 21:39:09.457210: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp13/TD3_11
Terminated
2021-12-11 21:40:08.744553: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-11 21:40:08.744632: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp13/TD3_12
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.74e+03 |
|    ep_rew_mean     | 20.3     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 80       |
|    time_elapsed    | 85       |
|    total timesteps | 6947     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.72e+03 |
|    ep_rew_mean     | -16.2    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 81       |
|    time_elapsed    | 168      |
|    total timesteps | 13775    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-56.80 +/- 96.74
Episode length: 1719.80 +/- 388.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.72e+03 |
|    mean_reward     | -56.8    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.81e+03 |
|    ep_rew_mean     | -31.7    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 56       |
|    time_elapsed    | 382      |
|    total timesteps | 21779    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.86e+03 |
|    ep_rew_mean     | -24.9    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 33       |
|    time_elapsed    | 878      |
|    total timesteps | 29783    |
| train/             |          |
|    actor_loss      | 7.9      |
|    critic_loss     | 2.3      |
|    learning_rate   | 0.0005   |
|    n_updates       | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.89e+03 |
|    ep_rew_mean     | -17.1    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 27       |
|    time_elapsed    | 1377     |
|    total timesteps | 37787    |
| train/             |          |
|    actor_loss      | 5.85     |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.0005   |
|    n_updates       | 16008    |
---------------------------------
Eval num_timesteps=40000, episode_reward=12.94 +/- 16.77
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 5.1      |
|    critic_loss     | 1.38     |
|    learning_rate   | 0.0005   |
|    n_updates       | 20010    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.91e+03 |
|    ep_rew_mean     | -18.7    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 22       |
|    time_elapsed    | 2010     |
|    total timesteps | 45791    |
| train/             |          |
|    actor_loss      | 4.39     |
|    critic_loss     | 1.33     |
|    learning_rate   | 0.0005   |
|    n_updates       | 24012    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.92e+03 |
|    ep_rew_mean     | -17.5    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 21       |
|    time_elapsed    | 2511     |
|    total timesteps | 53795    |
| train/             |          |
|    actor_loss      | 3.23     |
|    critic_loss     | 1.27     |
|    learning_rate   | 0.0005   |
|    n_updates       | 32016    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-12.22 +/- 33.01
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -12.2    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 2.85     |
|    critic_loss     | 1.19     |
|    learning_rate   | 0.0005   |
|    n_updates       | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.93e+03 |
|    ep_rew_mean     | -18.1    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 19       |
|    time_elapsed    | 3142     |
|    total timesteps | 61799    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -13.3    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 19       |
|    time_elapsed    | 3642     |
|    total timesteps | 69803    |
| train/             |          |
|    actor_loss      | 2.05     |
|    critic_loss     | 1.15     |
|    learning_rate   | 0.0005   |
|    n_updates       | 48024    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -13.2    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 18       |
|    time_elapsed    | 4141     |
|    total timesteps | 77807    |
| train/             |          |
|    actor_loss      | 1.48     |
|    critic_loss     | 1.12     |
|    learning_rate   | 0.0005   |
|    n_updates       | 56028    |
---------------------------------
Eval num_timesteps=80000, episode_reward=36.83 +/- 27.69
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 36.8     |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 1.27     |
|    critic_loss     | 1.07     |
|    learning_rate   | 0.0005   |
|    n_updates       | 60030    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -8.02    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 17       |
|    time_elapsed    | 4771     |
|    total timesteps | 85811    |
| train/             |          |
|    actor_loss      | 1.07     |
|    critic_loss     | 1.08     |
|    learning_rate   | 0.0005   |
|    n_updates       | 64032    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.93e+03 |
|    ep_rew_mean     | -5.75    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 17       |
|    time_elapsed    | 5200     |
|    total timesteps | 92699    |
| train/             |          |
|    actor_loss      | 0.803    |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.0005   |
|    n_updates       | 70920    |
---------------------------------
Eval num_timesteps=100000, episode_reward=30.96 +/- 12.59
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 31       |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.58     |
|    critic_loss     | 1.02     |
|    learning_rate   | 0.0005   |
|    n_updates       | 78924    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -2.8     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 17       |
|    time_elapsed    | 5830     |
|    total timesteps | 100703   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -1.46    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 17       |
|    time_elapsed    | 6332     |
|    total timesteps | 108707   |
| train/             |          |
|    actor_loss      | 0.41     |
|    critic_loss     | 1        |
|    learning_rate   | 0.0005   |
|    n_updates       | 86928    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -2.7     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 17       |
|    time_elapsed    | 6832     |
|    total timesteps | 116711   |
| train/             |          |
|    actor_loss      | 0.281    |
|    critic_loss     | 0.98     |
|    learning_rate   | 0.0005   |
|    n_updates       | 94932    |
---------------------------------
Eval num_timesteps=120000, episode_reward=0.78 +/- 44.79
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 0.785    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 0.242    |
|    critic_loss     | 0.967    |
|    learning_rate   | 0.0005   |
|    n_updates       | 98934    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -0.266   |
| time/              |          |
|    episodes        | 64       |
|    fps             | 16       |
|    time_elapsed    | 7462     |
|    total timesteps | 124715   |
| train/             |          |
|    actor_loss      | 0.202    |
|    critic_loss     | 0.939    |
|    learning_rate   | 0.0005   |
|    n_updates       | 102936   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -1.58    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 16       |
|    time_elapsed    | 7963     |
|    total timesteps | 132719   |
| train/             |          |
|    actor_loss      | 0.148    |
|    critic_loss     | 0.962    |
|    learning_rate   | 0.0005   |
|    n_updates       | 110940   |
---------------------------------
Eval num_timesteps=140000, episode_reward=50.02 +/- 14.60
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 50       |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 0.101    |
|    critic_loss     | 0.951    |
|    learning_rate   | 0.0005   |
|    n_updates       | 118944   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -1.48    |
| time/              |          |
|    episodes        | 72       |
|    fps             | 16       |
|    time_elapsed    | 8598     |
|    total timesteps | 140723   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -0.832   |
| time/              |          |
|    episodes        | 76       |
|    fps             | 16       |
|    time_elapsed    | 9094     |
|    total timesteps | 148727   |
| train/             |          |
|    actor_loss      | 0.0707   |
|    critic_loss     | 0.958    |
|    learning_rate   | 0.0005   |
|    n_updates       | 126948   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -0.703   |
| time/              |          |
|    episodes        | 80       |
|    fps             | 16       |
|    time_elapsed    | 9594     |
|    total timesteps | 156731   |
| train/             |          |
|    actor_loss      | 0.0569   |
|    critic_loss     | 0.928    |
|    learning_rate   | 0.0005   |
|    n_updates       | 134952   |
---------------------------------
Eval num_timesteps=160000, episode_reward=82.55 +/- 41.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 82.6     |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 0.051    |
|    critic_loss     | 0.929    |
|    learning_rate   | 0.0005   |
|    n_updates       | 138954   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | 1.53     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 16       |
|    time_elapsed    | 10229    |
|    total timesteps | 164735   |
| train/             |          |
|    actor_loss      | 0.0404   |
|    critic_loss     | 0.95     |
|    learning_rate   | 0.0005   |
|    n_updates       | 142956   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | 0.975    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 16       |
|    time_elapsed    | 10728    |
|    total timesteps | 172739   |
| train/             |          |
|    actor_loss      | 0.0108   |
|    critic_loss     | 0.911    |
|    learning_rate   | 0.0005   |
|    n_updates       | 150960   |
---------------------------------
Eval num_timesteps=180000, episode_reward=79.30 +/- 54.12
Episode length: 1872.80 +/- 256.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.87e+03 |
|    mean_reward     | 79.3     |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 0.000337 |
|    critic_loss     | 0.917    |
|    learning_rate   | 0.0005   |
|    n_updates       | 158964   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 15       |
|    time_elapsed    | 11349    |
|    total timesteps | 180743   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -0.944   |
| time/              |          |
|    episodes        | 96       |
|    fps             | 15       |
|    time_elapsed    | 11846    |
|    total timesteps | 188747   |
| train/             |          |
|    actor_loss      | 0.00172  |
|    critic_loss     | 0.908    |
|    learning_rate   | 0.0005   |
|    n_updates       | 166968   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -3.74    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 15       |
|    time_elapsed    | 12342    |
|    total timesteps | 196751   |
| train/             |          |
|    actor_loss      | 0.0126   |
|    critic_loss     | 0.934    |
|    learning_rate   | 0.0005   |
|    n_updates       | 174972   |
---------------------------------
Eval num_timesteps=200000, episode_reward=54.70 +/- 45.11
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 54.7     |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 0.0293   |
|    critic_loss     | 0.947    |
|    learning_rate   | 0.0005   |
|    n_updates       | 178974   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -4.94    |
| time/              |          |
|    episodes        | 104      |
|    fps             | 15       |
|    time_elapsed    | 12972    |
|    total timesteps | 204755   |
| train/             |          |
|    actor_loss      | 0.0458   |
|    critic_loss     | 0.944    |
|    learning_rate   | 0.0005   |
|    n_updates       | 182976   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 0.286    |
| time/              |          |
|    episodes        | 108      |
|    fps             | 15       |
|    time_elapsed    | 13471    |
|    total timesteps | 212759   |
| train/             |          |
|    actor_loss      | 0.0537   |
|    critic_loss     | 0.929    |
|    learning_rate   | 0.0005   |
|    n_updates       | 190980   |
---------------------------------
Eval num_timesteps=220000, episode_reward=74.86 +/- 54.98
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 74.9     |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 0.0483   |
|    critic_loss     | 0.922    |
|    learning_rate   | 0.0005   |
|    n_updates       | 198984   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.27     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 15       |
|    time_elapsed    | 14107    |
|    total timesteps | 220763   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.47     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 15       |
|    time_elapsed    | 14619    |
|    total timesteps | 228460   |
| train/             |          |
|    actor_loss      | 0.0336   |
|    critic_loss     | 0.91     |
|    learning_rate   | 0.0005   |
|    n_updates       | 206988   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.09     |
| time/              |          |
|    episodes        | 120      |
|    fps             | 15       |
|    time_elapsed    | 15108    |
|    total timesteps | 236464   |
| train/             |          |
|    actor_loss      | 0.0282   |
|    critic_loss     | 0.933    |
|    learning_rate   | 0.0005   |
|    n_updates       | 214685   |
---------------------------------
Eval num_timesteps=240000, episode_reward=40.35 +/- 56.55
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 40.4     |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.0257   |
|    critic_loss     | 0.943    |
|    learning_rate   | 0.0005   |
|    n_updates       | 218687   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.96     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 15       |
|    time_elapsed    | 15738    |
|    total timesteps | 244468   |
| train/             |          |
|    actor_loss      | 0.0214   |
|    critic_loss     | 0.908    |
|    learning_rate   | 0.0005   |
|    n_updates       | 222689   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.39     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 15       |
|    time_elapsed    | 16237    |
|    total timesteps | 252472   |
| train/             |          |
|    actor_loss      | 0.0101   |
|    critic_loss     | 0.885    |
|    learning_rate   | 0.0005   |
|    n_updates       | 230693   |
---------------------------------
Eval num_timesteps=260000, episode_reward=43.47 +/- 47.16
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 43.5     |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -0.0113  |
|    critic_loss     | 0.889    |
|    learning_rate   | 0.0005   |
|    n_updates       | 238697   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 8.51     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 15       |
|    time_elapsed    | 16864    |
|    total timesteps | 260476   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.58     |
| time/              |          |
|    episodes        | 136      |
|    fps             | 15       |
|    time_elapsed    | 17359    |
|    total timesteps | 268480   |
| train/             |          |
|    actor_loss      | -0.0225  |
|    critic_loss     | 0.873    |
|    learning_rate   | 0.0005   |
|    n_updates       | 246701   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 7.37     |
| time/              |          |
|    episodes        | 140      |
|    fps             | 15       |
|    time_elapsed    | 17854    |
|    total timesteps | 276484   |
| train/             |          |
|    actor_loss      | -0.0332  |
|    critic_loss     | 0.873    |
|    learning_rate   | 0.0005   |
|    n_updates       | 254705   |
---------------------------------
Eval num_timesteps=280000, episode_reward=56.94 +/- 23.80
Episode length: 1992.00 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.99e+03 |
|    mean_reward     | 56.9     |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -0.0408  |
|    critic_loss     | 0.866    |
|    learning_rate   | 0.0005   |
|    n_updates       | 258707   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.36     |
| time/              |          |
|    episodes        | 144      |
|    fps             | 15       |
|    time_elapsed    | 18482    |
|    total timesteps | 284488   |
| train/             |          |
|    actor_loss      | -0.0379  |
|    critic_loss     | 0.9      |
|    learning_rate   | 0.0005   |
|    n_updates       | 262709   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.81     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 15       |
|    time_elapsed    | 18984    |
|    total timesteps | 292492   |
| train/             |          |
|    actor_loss      | -0.0339  |
|    critic_loss     | 0.905    |
|    learning_rate   | 0.0005   |
|    n_updates       | 270713   |
---------------------------------
Eval num_timesteps=300000, episode_reward=51.75 +/- 18.80
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 51.8     |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -0.0296  |
|    critic_loss     | 0.899    |
|    learning_rate   | 0.0005   |
|    n_updates       | 278717   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.61     |
| time/              |          |
|    episodes        | 152      |
|    fps             | 15       |
|    time_elapsed    | 19618    |
|    total timesteps | 300496   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.5      |
| time/              |          |
|    episodes        | 156      |
|    fps             | 15       |
|    time_elapsed    | 20114    |
|    total timesteps | 308500   |
| train/             |          |
|    actor_loss      | -0.023   |
|    critic_loss     | 0.894    |
|    learning_rate   | 0.0005   |
|    n_updates       | 286721   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.2      |
| time/              |          |
|    episodes        | 160      |
|    fps             | 15       |
|    time_elapsed    | 20611    |
|    total timesteps | 316504   |
| train/             |          |
|    actor_loss      | -0.00731 |
|    critic_loss     | 0.888    |
|    learning_rate   | 0.0005   |
|    n_updates       | 294725   |
---------------------------------
Eval num_timesteps=320000, episode_reward=111.90 +/- 58.37
Episode length: 1931.60 +/- 96.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.93e+03 |
|    mean_reward     | 112      |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -0.00118 |
|    critic_loss     | 0.897    |
|    learning_rate   | 0.0005   |
|    n_updates       | 298727   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.87     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 15       |
|    time_elapsed    | 21240    |
|    total timesteps | 324508   |
| train/             |          |
|    actor_loss      | 0.00308  |
|    critic_loss     | 0.91     |
|    learning_rate   | 0.0005   |
|    n_updates       | 302729   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    episodes        | 168      |
|    fps             | 15       |
|    time_elapsed    | 21741    |
|    total timesteps | 332512   |
| train/             |          |
|    actor_loss      | -0.00961 |
|    critic_loss     | 0.872    |
|    learning_rate   | 0.0005   |
|    n_updates       | 310733   |
---------------------------------
Eval num_timesteps=340000, episode_reward=42.88 +/- 29.11
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 42.9     |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -0.0167  |
|    critic_loss     | 0.889    |
|    learning_rate   | 0.0005   |
|    n_updates       | 318737   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.6      |
| time/              |          |
|    episodes        | 172      |
|    fps             | 15       |
|    time_elapsed    | 22375    |
|    total timesteps | 340516   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    episodes        | 176      |
|    fps             | 15       |
|    time_elapsed    | 22874    |
|    total timesteps | 348520   |
| train/             |          |
|    actor_loss      | -0.0296  |
|    critic_loss     | 0.873    |
|    learning_rate   | 0.0005   |
|    n_updates       | 326741   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.2      |
| time/              |          |
|    episodes        | 180      |
|    fps             | 15       |
|    time_elapsed    | 23374    |
|    total timesteps | 356524   |
| train/             |          |
|    actor_loss      | -0.0372  |
|    critic_loss     | 0.875    |
|    learning_rate   | 0.0005   |
|    n_updates       | 334745   |
---------------------------------
Eval num_timesteps=360000, episode_reward=42.50 +/- 27.88
Episode length: 1977.40 +/- 47.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.98e+03 |
|    mean_reward     | 42.5     |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -0.0386  |
|    critic_loss     | 0.887    |
|    learning_rate   | 0.0005   |
|    n_updates       | 338747   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    episodes        | 184      |
|    fps             | 15       |
|    time_elapsed    | 24008    |
|    total timesteps | 364528   |
| train/             |          |
|    actor_loss      | -0.0431  |
|    critic_loss     | 0.853    |
|    learning_rate   | 0.0005   |
|    n_updates       | 342749   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.11     |
| time/              |          |
|    episodes        | 188      |
|    fps             | 15       |
|    time_elapsed    | 24512    |
|    total timesteps | 372532   |
| train/             |          |
|    actor_loss      | -0.0564  |
|    critic_loss     | 0.884    |
|    learning_rate   | 0.0005   |
|    n_updates       | 350753   |
---------------------------------
Eval num_timesteps=380000, episode_reward=80.43 +/- 29.70
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 80.4     |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -0.0582  |
|    critic_loss     | 0.859    |
|    learning_rate   | 0.0005   |
|    n_updates       | 358757   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.722    |
| time/              |          |
|    episodes        | 192      |
|    fps             | 15       |
|    time_elapsed    | 25143    |
|    total timesteps | 380536   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.415    |
| time/              |          |
|    episodes        | 196      |
|    fps             | 15       |
|    time_elapsed    | 25644    |
|    total timesteps | 388540   |
| train/             |          |
|    actor_loss      | -0.0417  |
|    critic_loss     | 0.898    |
|    learning_rate   | 0.0005   |
|    n_updates       | 366761   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.46     |
| time/              |          |
|    episodes        | 200      |
|    fps             | 15       |
|    time_elapsed    | 26146    |
|    total timesteps | 396544   |
| train/             |          |
|    actor_loss      | -0.0259  |
|    critic_loss     | 0.878    |
|    learning_rate   | 0.0005   |
|    n_updates       | 374765   |
---------------------------------
Eval num_timesteps=400000, episode_reward=68.65 +/- 50.85
Episode length: 1995.60 +/- 10.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 68.6     |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -0.0189  |
|    critic_loss     | 0.904    |
|    learning_rate   | 0.0005   |
|    n_updates       | 378767   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.94     |
| time/              |          |
|    episodes        | 204      |
|    fps             | 15       |
|    time_elapsed    | 26775    |
|    total timesteps | 404548   |
| train/             |          |
|    actor_loss      | -0.0152  |
|    critic_loss     | 0.891    |
|    learning_rate   | 0.0005   |
|    n_updates       | 382769   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2e+03     |
|    ep_rew_mean     | 1.59      |
| time/              |           |
|    episodes        | 208       |
|    fps             | 15        |
|    time_elapsed    | 27271     |
|    total timesteps | 412552    |
| train/             |           |
|    actor_loss      | -0.000934 |
|    critic_loss     | 0.912     |
|    learning_rate   | 0.0005    |
|    n_updates       | 390773    |
----------------------------------
Eval num_timesteps=420000, episode_reward=70.01 +/- 45.37
Episode length: 1753.00 +/- 307.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.75e+03 |
|    mean_reward     | 70       |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -0.00193 |
|    critic_loss     | 0.897    |
|    learning_rate   | 0.0005   |
|    n_updates       | 398777   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.474    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 15       |
|    time_elapsed    | 27884    |
|    total timesteps | 420556   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.19    |
| time/              |          |
|    episodes        | 216      |
|    fps             | 15       |
|    time_elapsed    | 28383    |
|    total timesteps | 428560   |
| train/             |          |
|    actor_loss      | 0.00315  |
|    critic_loss     | 0.903    |
|    learning_rate   | 0.0005   |
|    n_updates       | 406781   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.67    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 15       |
|    time_elapsed    | 28883    |
|    total timesteps | 436564   |
| train/             |          |
|    actor_loss      | 0.0185   |
|    critic_loss     | 0.91     |
|    learning_rate   | 0.0005   |
|    n_updates       | 414785   |
---------------------------------
Eval num_timesteps=440000, episode_reward=101.83 +/- 39.15
Episode length: 1928.00 +/- 146.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.93e+03 |
|    mean_reward     | 102      |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | 0.0288   |
|    critic_loss     | 0.92     |
|    learning_rate   | 0.0005   |
|    n_updates       | 418787   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.19    |
| time/              |          |
|    episodes        | 224      |
|    fps             | 15       |
|    time_elapsed    | 29516    |
|    total timesteps | 444568   |
| train/             |          |
|    actor_loss      | 0.0342   |
|    critic_loss     | 0.906    |
|    learning_rate   | 0.0005   |
|    n_updates       | 422789   |
---------------------------------
Killed
2021-12-12 12:30:23.877022: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-12 12:30:23.877102: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp13/TD3_13
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.7e+03  |
|    ep_rew_mean     | -45.5    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 79       |
|    time_elapsed    | 85       |
|    total timesteps | 6785     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.85e+03 |
|    ep_rew_mean     | -37.2    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 81       |
|    time_elapsed    | 181      |
|    total timesteps | 14789    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-107.40 +/- 63.65
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -107     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.9e+03  |
|    ep_rew_mean     | -24.3    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 41       |
|    time_elapsed    | 544      |
|    total timesteps | 22793    |
| train/             |          |
|    actor_loss      | 10.2     |
|    critic_loss     | 3.56     |
|    learning_rate   | 0.000978 |
|    n_updates       | 2600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.92e+03 |
|    ep_rew_mean     | -44.1    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 29       |
|    time_elapsed    | 1044     |
|    total timesteps | 30797    |
| train/             |          |
|    actor_loss      | 8.2      |
|    critic_loss     | 2.16     |
|    learning_rate   | 0.00097  |
|    n_updates       | 10600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.94e+03 |
|    ep_rew_mean     | -36.1    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 24       |
|    time_elapsed    | 1553     |
|    total timesteps | 38801    |
| train/             |          |
|    actor_loss      | 5.77     |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.000962 |
|    n_updates       | 18800    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-10.45 +/- 18.71
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.5    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 5.53     |
|    critic_loss     | 1.52     |
|    learning_rate   | 0.000961 |
|    n_updates       | 19800    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -22.1    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 21       |
|    time_elapsed    | 2189     |
|    total timesteps | 46805    |
| train/             |          |
|    actor_loss      | 4.09     |
|    critic_loss     | 1.47     |
|    learning_rate   | 0.000954 |
|    n_updates       | 26800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -27.1    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 20       |
|    time_elapsed    | 2692     |
|    total timesteps | 54809    |
| train/             |          |
|    actor_loss      | 3.03     |
|    critic_loss     | 1.44     |
|    learning_rate   | 0.000946 |
|    n_updates       | 34800    |
---------------------------------
Eval num_timesteps=60000, episode_reward=43.03 +/- 51.63
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 43       |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 2.66     |
|    critic_loss     | 1.27     |
|    learning_rate   | 0.000941 |
|    n_updates       | 39800    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -24.2    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 18       |
|    time_elapsed    | 3327     |
|    total timesteps | 62813    |
| train/             |          |
|    actor_loss      | 2.44     |
|    critic_loss     | 1.52     |
|    learning_rate   | 0.000938 |
|    n_updates       | 42800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -23.3    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 18       |
|    time_elapsed    | 3829     |
|    total timesteps | 70817    |
| train/             |          |
|    actor_loss      | 1.88     |
|    critic_loss     | 1.38     |
|    learning_rate   | 0.00093  |
|    n_updates       | 50800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | -26.7    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 18       |
|    time_elapsed    | 4286     |
|    total timesteps | 78136    |
| train/             |          |
|    actor_loss      | 1.49     |
|    critic_loss     | 1.34     |
|    learning_rate   | 0.000923 |
|    n_updates       | 58000    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-82.23 +/- 47.41
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -82.2    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 1.41     |
|    critic_loss     | 1.47     |
|    learning_rate   | 0.000921 |
|    n_updates       | 59800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -32      |
| time/              |          |
|    episodes        | 44       |
|    fps             | 17       |
|    time_elapsed    | 4946     |
|    total timesteps | 86140    |
| train/             |          |
|    actor_loss      | 1.16     |
|    critic_loss     | 1.44     |
|    learning_rate   | 0.000915 |
|    n_updates       | 66000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -34.4    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 17       |
|    time_elapsed    | 5448     |
|    total timesteps | 94144    |
| train/             |          |
|    actor_loss      | 0.958    |
|    critic_loss     | 1.45     |
|    learning_rate   | 0.000907 |
|    n_updates       | 74000    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-22.80 +/- 71.41
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -22.8    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.84     |
|    critic_loss     | 1.32     |
|    learning_rate   | 0.000901 |
|    n_updates       | 79800    |
---------------------------------
Terminated
2021-12-12 14:12:13.426739: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-12 14:12:13.426808: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp13/TD3_14
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -59.6    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 81       |
|    time_elapsed    | 97       |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -48      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 82       |
|    time_elapsed    | 193      |
|    total timesteps | 16008    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-7.71 +/- 106.96
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -7.71    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -26.2    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 38       |
|    time_elapsed    | 619      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 11.3     |
|    critic_loss     | 3.9      |
|    learning_rate   | 0.000979 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -17      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 28       |
|    time_elapsed    | 1119     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 7.61     |
|    critic_loss     | 3.46     |
|    learning_rate   | 0.000971 |
|    n_updates       | 12006    |
---------------------------------
Eval num_timesteps=40000, episode_reward=35.74 +/- 45.95
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 35.7     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 4.65     |
|    critic_loss     | 4.7      |
|    learning_rate   | 0.000964 |
|    n_updates       | 20010    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.33    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 22       |
|    time_elapsed    | 1752     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.86    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 21       |
|    time_elapsed    | 2254     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 3.93     |
|    critic_loss     | 3.84     |
|    learning_rate   | 0.000956 |
|    n_updates       | 28014    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.809    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 20       |
|    time_elapsed    | 2754     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 3.37     |
|    critic_loss     | 3.08     |
|    learning_rate   | 0.000949 |
|    n_updates       | 36018    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-60.06 +/- 64.65
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -60.1    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 3.16     |
|    critic_loss     | 2.86     |
|    learning_rate   | 0.000945 |
|    n_updates       | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.58    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 18       |
|    time_elapsed    | 3388     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 3        |
|    critic_loss     | 2.54     |
|    learning_rate   | 0.000941 |
|    n_updates       | 44022    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.02    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 18       |
|    time_elapsed    | 3886     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 2.63     |
|    critic_loss     | 2.37     |
|    learning_rate   | 0.000933 |
|    n_updates       | 52026    |
---------------------------------
Eval num_timesteps=80000, episode_reward=16.47 +/- 66.82
Episode length: 1973.60 +/- 54.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.97e+03 |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 2.4      |
|    critic_loss     | 2.19     |
|    learning_rate   | 0.000926 |
|    n_updates       | 60030    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.96    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 17       |
|    time_elapsed    | 4518     |
|    total timesteps | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.685   |
| time/              |          |
|    episodes        | 44       |
|    fps             | 17       |
|    time_elapsed    | 5018     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 2.18     |
|    critic_loss     | 2.11     |
|    learning_rate   | 0.000918 |
|    n_updates       | 68034    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.74    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 17       |
|    time_elapsed    | 5519     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 1.96     |
|    critic_loss     | 2.1      |
|    learning_rate   | 0.000911 |
|    n_updates       | 76038    |
---------------------------------
Eval num_timesteps=100000, episode_reward=1.73 +/- 32.53
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 1.73     |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 1.9      |
|    critic_loss     | 2.08     |
|    learning_rate   | 0.000907 |
|    n_updates       | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4       |
| time/              |          |
|    episodes        | 52       |
|    fps             | 16       |
|    time_elapsed    | 6153     |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | 1.86     |
|    critic_loss     | 1.96     |
|    learning_rate   | 0.000903 |
|    n_updates       | 84042    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.02    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 16       |
|    time_elapsed    | 6656     |
|    total timesteps | 112056   |
| train/             |          |
|    actor_loss      | 1.73     |
|    critic_loss     | 1.96     |
|    learning_rate   | 0.000895 |
|    n_updates       | 92046    |
---------------------------------
Eval num_timesteps=120000, episode_reward=11.68 +/- 67.79
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 1.68     |
|    critic_loss     | 2.05     |
|    learning_rate   | 0.000888 |
|    n_updates       | 100050   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.47    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 16       |
|    time_elapsed    | 7299     |
|    total timesteps | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.27    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 16       |
|    time_elapsed    | 7797     |
|    total timesteps | 128064   |
| train/             |          |
|    actor_loss      | 1.62     |
|    critic_loss     | 1.79     |
|    learning_rate   | 0.00088  |
|    n_updates       | 108054   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.21    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 16       |
|    time_elapsed    | 8298     |
|    total timesteps | 136068   |
| train/             |          |
|    actor_loss      | 1.52     |
|    critic_loss     | 1.83     |
|    learning_rate   | 0.000873 |
|    n_updates       | 116058   |
---------------------------------
Eval num_timesteps=140000, episode_reward=6.03 +/- 15.26
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 6.03     |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 1.48     |
|    critic_loss     | 1.79     |
|    learning_rate   | 0.000869 |
|    n_updates       | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -11      |
| time/              |          |
|    episodes        | 72       |
|    fps             | 16       |
|    time_elapsed    | 8934     |
|    total timesteps | 144072   |
| train/             |          |
|    actor_loss      | 1.46     |
|    critic_loss     | 1.78     |
|    learning_rate   | 0.000865 |
|    n_updates       | 124062   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -11.5    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 16       |
|    time_elapsed    | 9438     |
|    total timesteps | 152076   |
| train/             |          |
|    actor_loss      | 1.43     |
|    critic_loss     | 1.8      |
|    learning_rate   | 0.000857 |
|    n_updates       | 132066   |
---------------------------------
Eval num_timesteps=160000, episode_reward=17.25 +/- 83.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 17.3     |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 1.41     |
|    critic_loss     | 1.72     |
|    learning_rate   | 0.00085  |
|    n_updates       | 140070   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.75    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 15       |
|    time_elapsed    | 10074    |
|    total timesteps | 160080   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -10.8    |
| time/              |          |
|    episodes        | 84       |
|    fps             | 15       |
|    time_elapsed    | 10575    |
|    total timesteps | 168084   |
| train/             |          |
|    actor_loss      | 1.37     |
|    critic_loss     | 1.73     |
|    learning_rate   | 0.000842 |
|    n_updates       | 148074   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 15       |
|    time_elapsed    | 11073    |
|    total timesteps | 176088   |
| train/             |          |
|    actor_loss      | 1.37     |
|    critic_loss     | 1.75     |
|    learning_rate   | 0.000835 |
|    n_updates       | 156078   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-7.48 +/- 29.38
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -7.48    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 1.37     |
|    critic_loss     | 1.69     |
|    learning_rate   | 0.000831 |
|    n_updates       | 160080   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 15       |
|    time_elapsed    | 11706    |
|    total timesteps | 184092   |
| train/             |          |
|    actor_loss      | 1.33     |
|    critic_loss     | 1.67     |
|    learning_rate   | 0.000827 |
|    n_updates       | 164082   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -14.2    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 15       |
|    time_elapsed    | 12207    |
|    total timesteps | 192096   |
| train/             |          |
|    actor_loss      | 1.3      |
|    critic_loss     | 1.68     |
|    learning_rate   | 0.000819 |
|    n_updates       | 172086   |
---------------------------------
Terminated
2021-12-12 17:44:25.940945: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-12 17:44:25.941014: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp13/TD3_15
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 79       |
|    time_elapsed    | 100      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -46.7    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 80       |
|    time_elapsed    | 199      |
|    total timesteps | 16008    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-166.69 +/- 61.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -42.8    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 37       |
|    time_elapsed    | 632      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | -2.38    |
|    critic_loss     | 1.71     |
|    learning_rate   | 0.000735 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -31.6    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 28       |
|    time_elapsed    | 1134     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | -2.2     |
|    critic_loss     | 2.85     |
|    learning_rate   | 0.000729 |
|    n_updates       | 12006    |
---------------------------------
Eval num_timesteps=40000, episode_reward=2.58 +/- 53.11
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -1.42    |
|    critic_loss     | 4.36     |
|    learning_rate   | 0.000723 |
|    n_updates       | 20010    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -33      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 22       |
|    time_elapsed    | 1772     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -32.6    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 21       |
|    time_elapsed    | 2277     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.393    |
|    critic_loss     | 3.37     |
|    learning_rate   | 0.000718 |
|    n_updates       | 28014    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -26.8    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 20       |
|    time_elapsed    | 2779     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 2.13     |
|    critic_loss     | 2.74     |
|    learning_rate   | 0.000712 |
|    n_updates       | 36018    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-33.67 +/- 102.33
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -33.7    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 2.84     |
|    critic_loss     | 2.81     |
|    learning_rate   | 0.000709 |
|    n_updates       | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -29.8    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 18       |
|    time_elapsed    | 3416     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 3.49     |
|    critic_loss     | 2.74     |
|    learning_rate   | 0.000707 |
|    n_updates       | 44022    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -22.9    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 18       |
|    time_elapsed    | 3916     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 4.5      |
|    critic_loss     | 2.6      |
|    learning_rate   | 0.000701 |
|    n_updates       | 52026    |
---------------------------------
Eval num_timesteps=80000, episode_reward=12.06 +/- 40.98
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 4.92     |
|    critic_loss     | 2.44     |
|    learning_rate   | 0.000695 |
|    n_updates       | 60030    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -21.2    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 17       |
|    time_elapsed    | 4552     |
|    total timesteps | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 17       |
|    time_elapsed    | 5051     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 5.04     |
|    critic_loss     | 2.38     |
|    learning_rate   | 0.00069  |
|    n_updates       | 68034    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -11.9    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 17       |
|    time_elapsed    | 5551     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 4.94     |
|    critic_loss     | 2.17     |
|    learning_rate   | 0.000684 |
|    n_updates       | 76038    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-43.61 +/- 48.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -43.6    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 4.93     |
|    critic_loss     | 2.13     |
|    learning_rate   | 0.000681 |
|    n_updates       | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.14    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 16       |
|    time_elapsed    | 6184     |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | 4.8      |
|    critic_loss     | 2.06     |
|    learning_rate   | 0.000679 |
|    n_updates       | 84042    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 16       |
|    time_elapsed    | 6685     |
|    total timesteps | 112056   |
| train/             |          |
|    actor_loss      | 4.5      |
|    critic_loss     | 1.97     |
|    learning_rate   | 0.000673 |
|    n_updates       | 92046    |
---------------------------------
Eval num_timesteps=120000, episode_reward=14.54 +/- 56.18
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 14.5     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 4.26     |
|    critic_loss     | 1.94     |
|    learning_rate   | 0.000667 |
|    n_updates       | 100050   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.7     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 16       |
|    time_elapsed    | 7345     |
|    total timesteps | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -11      |
| time/              |          |
|    episodes        | 64       |
|    fps             | 16       |
|    time_elapsed    | 7852     |
|    total timesteps | 128064   |
| train/             |          |
|    actor_loss      | 4.03     |
|    critic_loss     | 1.9      |
|    learning_rate   | 0.000662 |
|    n_updates       | 108054   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.64    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 16       |
|    time_elapsed    | 8354     |
|    total timesteps | 136068   |
| train/             |          |
|    actor_loss      | 3.87     |
|    critic_loss     | 1.83     |
|    learning_rate   | 0.000656 |
|    n_updates       | 116058   |
---------------------------------
Eval num_timesteps=140000, episode_reward=13.56 +/- 67.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 3.76     |
|    critic_loss     | 1.76     |
|    learning_rate   | 0.000653 |
|    n_updates       | 120060   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.3     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 16       |
|    time_elapsed    | 8988     |
|    total timesteps | 144072   |
| train/             |          |
|    actor_loss      | 3.67     |
|    critic_loss     | 1.8      |
|    learning_rate   | 0.000651 |
|    n_updates       | 124062   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.46    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 16       |
|    time_elapsed    | 9488     |
|    total timesteps | 152076   |
| train/             |          |
|    actor_loss      | 3.52     |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.000645 |
|    n_updates       | 132066   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-13.75 +/- 46.46
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -13.7    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 3.5      |
|    critic_loss     | 1.68     |
|    learning_rate   | 0.000639 |
|    n_updates       | 140070   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.55    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 15       |
|    time_elapsed    | 10121    |
|    total timesteps | 160080   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.82    |
| time/              |          |
|    episodes        | 84       |
|    fps             | 15       |
|    time_elapsed    | 10618    |
|    total timesteps | 168084   |
| train/             |          |
|    actor_loss      | 3.43     |
|    critic_loss     | 1.65     |
|    learning_rate   | 0.000634 |
|    n_updates       | 148074   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.93    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 15       |
|    time_elapsed    | 11120    |
|    total timesteps | 176088   |
| train/             |          |
|    actor_loss      | 3.38     |
|    critic_loss     | 1.62     |
|    learning_rate   | 0.000628 |
|    n_updates       | 156078   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-8.72 +/- 58.26
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -8.72    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 3.35     |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.000625 |
|    n_updates       | 160080   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.13    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 15       |
|    time_elapsed    | 11757    |
|    total timesteps | 184092   |
| train/             |          |
|    actor_loss      | 3.33     |
|    critic_loss     | 1.62     |
|    learning_rate   | 0.000623 |
|    n_updates       | 164082   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.68    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 15       |
|    time_elapsed    | 12259    |
|    total timesteps | 192096   |
| train/             |          |
|    actor_loss      | 3.31     |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.000617 |
|    n_updates       | 172086   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-22.31 +/- 66.56
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -22.3    |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 3.29     |
|    critic_loss     | 1.62     |
|    learning_rate   | 0.000611 |
|    n_updates       | 180090   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.73    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 15       |
|    time_elapsed    | 12891    |
|    total timesteps | 200100   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.25    |
| time/              |          |
|    episodes        | 104      |
|    fps             | 15       |
|    time_elapsed    | 13388    |
|    total timesteps | 208104   |
| train/             |          |
|    actor_loss      | 3.28     |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.000606 |
|    n_updates       | 188094   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.493   |
| time/              |          |
|    episodes        | 108      |
|    fps             | 15       |
|    time_elapsed    | 13888    |
|    total timesteps | 216108   |
| train/             |          |
|    actor_loss      | 3.28     |
|    critic_loss     | 1.65     |
|    learning_rate   | 0.0006   |
|    n_updates       | 196098   |
---------------------------------
Eval num_timesteps=220000, episode_reward=18.23 +/- 34.28
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 18.2     |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 3.22     |
|    critic_loss     | 1.61     |
|    learning_rate   | 0.000597 |
|    n_updates       | 200100   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.81     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 15       |
|    time_elapsed    | 14520    |
|    total timesteps | 224112   |
| train/             |          |
|    actor_loss      | 3.2      |
|    critic_loss     | 1.62     |
|    learning_rate   | 0.000595 |
|    n_updates       | 204102   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.944    |
| time/              |          |
|    episodes        | 116      |
|    fps             | 15       |
|    time_elapsed    | 15021    |
|    total timesteps | 232116   |
| train/             |          |
|    actor_loss      | 3.15     |
|    critic_loss     | 1.62     |
|    learning_rate   | 0.000589 |
|    n_updates       | 212106   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-5.58 +/- 53.03
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -5.58    |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 3.11     |
|    critic_loss     | 1.6      |
|    learning_rate   | 0.000583 |
|    n_updates       | 220110   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.998    |
| time/              |          |
|    episodes        | 120      |
|    fps             | 15       |
|    time_elapsed    | 15653    |
|    total timesteps | 240120   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.85     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 15       |
|    time_elapsed    | 16150    |
|    total timesteps | 248124   |
| train/             |          |
|    actor_loss      | 3.05     |
|    critic_loss     | 1.66     |
|    learning_rate   | 0.000578 |
|    n_updates       | 228114   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 6.21     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 15       |
|    time_elapsed    | 16651    |
|    total timesteps | 256128   |
| train/             |          |
|    actor_loss      | 2.98     |
|    critic_loss     | 1.61     |
|    learning_rate   | 0.000572 |
|    n_updates       | 236118   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-27.27 +/- 68.68
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27.3    |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 2.94     |
|    critic_loss     | 1.66     |
|    learning_rate   | 0.000569 |
|    n_updates       | 240120   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 7.97     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 15       |
|    time_elapsed    | 17285    |
|    total timesteps | 264132   |
| train/             |          |
|    actor_loss      | 2.93     |
|    critic_loss     | 1.6      |
|    learning_rate   | 0.000567 |
|    n_updates       | 244122   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.66     |
| time/              |          |
|    episodes        | 136      |
|    fps             | 15       |
|    time_elapsed    | 17786    |
|    total timesteps | 272136   |
| train/             |          |
|    actor_loss      | 2.8      |
|    critic_loss     | 1.52     |
|    learning_rate   | 0.000561 |
|    n_updates       | 252126   |
---------------------------------
Eval num_timesteps=280000, episode_reward=8.96 +/- 54.83
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 8.96     |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 2.74     |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.000555 |
|    n_updates       | 260130   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 7.17     |
| time/              |          |
|    episodes        | 140      |
|    fps             | 15       |
|    time_elapsed    | 18426    |
|    total timesteps | 280140   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 6.93     |
| time/              |          |
|    episodes        | 144      |
|    fps             | 15       |
|    time_elapsed    | 18930    |
|    total timesteps | 288144   |
| train/             |          |
|    actor_loss      | 2.68     |
|    critic_loss     | 1.51     |
|    learning_rate   | 0.00055  |
|    n_updates       | 268134   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.75     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 15       |
|    time_elapsed    | 19433    |
|    total timesteps | 296148   |
| train/             |          |
|    actor_loss      | 2.59     |
|    critic_loss     | 1.5      |
|    learning_rate   | 0.000544 |
|    n_updates       | 276138   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-37.28 +/- 68.30
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -37.3    |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 2.58     |
|    critic_loss     | 1.56     |
|    learning_rate   | 0.000541 |
|    n_updates       | 280140   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.94     |
| time/              |          |
|    episodes        | 152      |
|    fps             | 15       |
|    time_elapsed    | 20068    |
|    total timesteps | 304152   |
| train/             |          |
|    actor_loss      | 2.55     |
|    critic_loss     | 1.52     |
|    learning_rate   | 0.000538 |
|    n_updates       | 284142   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.96     |
| time/              |          |
|    episodes        | 156      |
|    fps             | 15       |
|    time_elapsed    | 20569    |
|    total timesteps | 312156   |
| train/             |          |
|    actor_loss      | 2.52     |
|    critic_loss     | 1.5      |
|    learning_rate   | 0.000533 |
|    n_updates       | 292146   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-3.13 +/- 73.17
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -3.13    |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 2.49     |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.000527 |
|    n_updates       | 300150   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    episodes        | 160      |
|    fps             | 15       |
|    time_elapsed    | 21211    |
|    total timesteps | 320160   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.11     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 15       |
|    time_elapsed    | 21716    |
|    total timesteps | 328164   |
| train/             |          |
|    actor_loss      | 2.47     |
|    critic_loss     | 1.54     |
|    learning_rate   | 0.000522 |
|    n_updates       | 308154   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.1      |
| time/              |          |
|    episodes        | 168      |
|    fps             | 15       |
|    time_elapsed    | 22220    |
|    total timesteps | 336168   |
| train/             |          |
|    actor_loss      | 2.49     |
|    critic_loss     | 1.56     |
|    learning_rate   | 0.000516 |
|    n_updates       | 316158   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-62.74 +/- 98.78
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -62.7    |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | 2.49     |
|    critic_loss     | 1.51     |
|    learning_rate   | 0.000513 |
|    n_updates       | 320160   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.09     |
| time/              |          |
|    episodes        | 172      |
|    fps             | 15       |
|    time_elapsed    | 22855    |
|    total timesteps | 344172   |
| train/             |          |
|    actor_loss      | 2.46     |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.00051  |
|    n_updates       | 324162   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    episodes        | 176      |
|    fps             | 15       |
|    time_elapsed    | 23318    |
|    total timesteps | 351547   |
| train/             |          |
|    actor_loss      | 2.42     |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.000505 |
|    n_updates       | 331537   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.01     |
| time/              |          |
|    episodes        | 180      |
|    fps             | 15       |
|    time_elapsed    | 23820    |
|    total timesteps | 359551   |
| train/             |          |
|    actor_loss      | 2.37     |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.0005   |
|    n_updates       | 339541   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-7.55 +/- 33.16
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -7.55    |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | 2.38     |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.000498 |
|    n_updates       | 341542   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    episodes        | 184      |
|    fps             | 15       |
|    time_elapsed    | 24459    |
|    total timesteps | 367555   |
| train/             |          |
|    actor_loss      | 2.38     |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.000494 |
|    n_updates       | 347545   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    episodes        | 188      |
|    fps             | 15       |
|    time_elapsed    | 24960    |
|    total timesteps | 375559   |
| train/             |          |
|    actor_loss      | 2.36     |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.000489 |
|    n_updates       | 355549   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-28.39 +/- 20.48
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -28.4    |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | 2.31     |
|    critic_loss     | 1.52     |
|    learning_rate   | 0.000484 |
|    n_updates       | 361552   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.35     |
| time/              |          |
|    episodes        | 192      |
|    fps             | 14       |
|    time_elapsed    | 25595    |
|    total timesteps | 383563   |
| train/             |          |
|    actor_loss      | 2.3      |
|    critic_loss     | 1.54     |
|    learning_rate   | 0.000483 |
|    n_updates       | 363553   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.62     |
| time/              |          |
|    episodes        | 196      |
|    fps             | 15       |
|    time_elapsed    | 26099    |
|    total timesteps | 391567   |
| train/             |          |
|    actor_loss      | 2.29     |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.000477 |
|    n_updates       | 371557   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.32     |
| time/              |          |
|    episodes        | 200      |
|    fps             | 15       |
|    time_elapsed    | 26603    |
|    total timesteps | 399571   |
| train/             |          |
|    actor_loss      | 2.25     |
|    critic_loss     | 1.52     |
|    learning_rate   | 0.000472 |
|    n_updates       | 379561   |
---------------------------------
Eval num_timesteps=400000, episode_reward=-39.87 +/- 101.30
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -39.9    |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | 2.24     |
|    critic_loss     | 1.49     |
|    learning_rate   | 0.00047  |
|    n_updates       | 381562   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.29     |
| time/              |          |
|    episodes        | 204      |
|    fps             | 14       |
|    time_elapsed    | 27243    |
|    total timesteps | 407575   |
| train/             |          |
|    actor_loss      | 1.89     |
|    critic_loss     | 1.5      |
|    learning_rate   | 0.000466 |
|    n_updates       | 387565   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.35     |
| time/              |          |
|    episodes        | 208      |
|    fps             | 14       |
|    time_elapsed    | 27749    |
|    total timesteps | 415579   |
| train/             |          |
|    actor_loss      | 1.39     |
|    critic_loss     | 1.44     |
|    learning_rate   | 0.00046  |
|    n_updates       | 395569   |
---------------------------------
Eval num_timesteps=420000, episode_reward=45.90 +/- 65.97
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 45.9     |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | 1.04     |
|    critic_loss     | 1.25     |
|    learning_rate   | 0.000456 |
|    n_updates       | 401572   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.59    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 14       |
|    time_elapsed    | 28390    |
|    total timesteps | 423583   |
| train/             |          |
|    actor_loss      | 1.01     |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.000455 |
|    n_updates       | 403573   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -2.61    |
| time/              |          |
|    episodes        | 216      |
|    fps             | 14       |
|    time_elapsed    | 28910    |
|    total timesteps | 431587   |
| train/             |          |
|    actor_loss      | 1.07     |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.000449 |
|    n_updates       | 411577   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.31    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 14       |
|    time_elapsed    | 29418    |
|    total timesteps | 439591   |
| train/             |          |
|    actor_loss      | 0.955    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.000444 |
|    n_updates       | 419581   |
---------------------------------
Eval num_timesteps=440000, episode_reward=4.16 +/- 68.39
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 4.16     |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | 0.924    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.000442 |
|    n_updates       | 421582   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -4.6     |
| time/              |          |
|    episodes        | 224      |
|    fps             | 14       |
|    time_elapsed    | 30051    |
|    total timesteps | 447595   |
| train/             |          |
|    actor_loss      | 0.792    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.000438 |
|    n_updates       | 427585   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -7.2     |
| time/              |          |
|    episodes        | 228      |
|    fps             | 14       |
|    time_elapsed    | 30550    |
|    total timesteps | 455599   |
| train/             |          |
|    actor_loss      | 0.698    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.000432 |
|    n_updates       | 435589   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-40.15 +/- 41.82
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -40.1    |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | 0.64     |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.000428 |
|    n_updates       | 441592   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -7.57    |
| time/              |          |
|    episodes        | 232      |
|    fps             | 14       |
|    time_elapsed    | 31181    |
|    total timesteps | 463603   |
| train/             |          |
|    actor_loss      | 0.62     |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.000427 |
|    n_updates       | 443593   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -6.88    |
| time/              |          |
|    episodes        | 236      |
|    fps             | 14       |
|    time_elapsed    | 31680    |
|    total timesteps | 471607   |
| train/             |          |
|    actor_loss      | 0.587    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.000421 |
|    n_updates       | 451597   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -6.95    |
| time/              |          |
|    episodes        | 240      |
|    fps             | 14       |
|    time_elapsed    | 32179    |
|    total timesteps | 479611   |
| train/             |          |
|    actor_loss      | 0.559    |
|    critic_loss     | 1.24     |
|    learning_rate   | 0.000416 |
|    n_updates       | 459601   |
---------------------------------
Eval num_timesteps=480000, episode_reward=17.02 +/- 48.48
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | 0.552    |
|    critic_loss     | 1.27     |
|    learning_rate   | 0.000414 |
|    n_updates       | 461602   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -8.51    |
| time/              |          |
|    episodes        | 244      |
|    fps             | 14       |
|    time_elapsed    | 32815    |
|    total timesteps | 487615   |
| train/             |          |
|    actor_loss      | 0.509    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.00041  |
|    n_updates       | 467605   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -9.87    |
| time/              |          |
|    episodes        | 248      |
|    fps             | 14       |
|    time_elapsed    | 33317    |
|    total timesteps | 495619   |
| train/             |          |
|    actor_loss      | 0.487    |
|    critic_loss     | 1.24     |
|    learning_rate   | 0.000404 |
|    n_updates       | 475609   |
---------------------------------
Eval num_timesteps=500000, episode_reward=3.58 +/- 46.39
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 3.58     |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | 0.458    |
|    critic_loss     | 1.24     |
|    learning_rate   | 0.0004   |
|    n_updates       | 481612   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -8.78    |
| time/              |          |
|    episodes        | 252      |
|    fps             | 14       |
|    time_elapsed    | 33952    |
|    total timesteps | 503623   |
| train/             |          |
|    actor_loss      | 0.449    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.000399 |
|    n_updates       | 483613   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -8.6     |
| time/              |          |
|    episodes        | 256      |
|    fps             | 14       |
|    time_elapsed    | 34452    |
|    total timesteps | 511627   |
| train/             |          |
|    actor_loss      | 0.43     |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.000393 |
|    n_updates       | 491617   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -7.71    |
| time/              |          |
|    episodes        | 260      |
|    fps             | 14       |
|    time_elapsed    | 34953    |
|    total timesteps | 519631   |
| train/             |          |
|    actor_loss      | 0.411    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.000388 |
|    n_updates       | 499621   |
---------------------------------
Eval num_timesteps=520000, episode_reward=-7.64 +/- 32.02
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -7.64    |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | 0.412    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.000386 |
|    n_updates       | 501622   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -7.24    |
| time/              |          |
|    episodes        | 264      |
|    fps             | 14       |
|    time_elapsed    | 35591    |
|    total timesteps | 527635   |
| train/             |          |
|    actor_loss      | 0.396    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.000382 |
|    n_updates       | 507625   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -4.49    |
| time/              |          |
|    episodes        | 268      |
|    fps             | 14       |
|    time_elapsed    | 36096    |
|    total timesteps | 535639   |
| train/             |          |
|    actor_loss      | 0.376    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.000376 |
|    n_updates       | 515629   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-31.32 +/- 18.46
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -31.3    |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | 0.361    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.000372 |
|    n_updates       | 521632   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -3.56    |
| time/              |          |
|    episodes        | 272      |
|    fps             | 14       |
|    time_elapsed    | 36737    |
|    total timesteps | 543643   |
| train/             |          |
|    actor_loss      | 0.356    |
|    critic_loss     | 1.19     |
|    learning_rate   | 0.000371 |
|    n_updates       | 523633   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.55    |
| time/              |          |
|    episodes        | 276      |
|    fps             | 14       |
|    time_elapsed    | 37240    |
|    total timesteps | 551647   |
| train/             |          |
|    actor_loss      | 0.387    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.000365 |
|    n_updates       | 531637   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.52    |
| time/              |          |
|    episodes        | 280      |
|    fps             | 14       |
|    time_elapsed    | 37739    |
|    total timesteps | 559651   |
| train/             |          |
|    actor_loss      | 0.384    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.00036  |
|    n_updates       | 539641   |
---------------------------------
Eval num_timesteps=560000, episode_reward=-17.66 +/- 52.81
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -17.7    |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | 0.381    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.000358 |
|    n_updates       | 541642   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.42    |
| time/              |          |
|    episodes        | 284      |
|    fps             | 14       |
|    time_elapsed    | 38376    |
|    total timesteps | 567655   |
| train/             |          |
|    actor_loss      | 0.416    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.000354 |
|    n_updates       | 547645   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.66    |
| time/              |          |
|    episodes        | 288      |
|    fps             | 14       |
|    time_elapsed    | 38877    |
|    total timesteps | 575659   |
| train/             |          |
|    actor_loss      | 0.388    |
|    critic_loss     | 1.24     |
|    learning_rate   | 0.000348 |
|    n_updates       | 555649   |
---------------------------------
Eval num_timesteps=580000, episode_reward=11.95 +/- 43.54
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12       |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | 0.359    |
|    critic_loss     | 1.25     |
|    learning_rate   | 0.000344 |
|    n_updates       | 561652   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.06    |
| time/              |          |
|    episodes        | 292      |
|    fps             | 14       |
|    time_elapsed    | 39515    |
|    total timesteps | 583663   |
| train/             |          |
|    actor_loss      | 0.343    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.000343 |
|    n_updates       | 563653   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.93    |
| time/              |          |
|    episodes        | 296      |
|    fps             | 14       |
|    time_elapsed    | 40015    |
|    total timesteps | 591667   |
| train/             |          |
|    actor_loss      | 0.273    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.000337 |
|    n_updates       | 571657   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    episodes        | 300      |
|    fps             | 14       |
|    time_elapsed    | 40518    |
|    total timesteps | 599671   |
| train/             |          |
|    actor_loss      | 0.201    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.000332 |
|    n_updates       | 579661   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-31.09 +/- 60.76
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -31.1    |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | 0.187    |
|    critic_loss     | 1.29     |
|    learning_rate   | 0.00033  |
|    n_updates       | 581662   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -12.4    |
| time/              |          |
|    episodes        | 304      |
|    fps             | 14       |
|    time_elapsed    | 41158    |
|    total timesteps | 607675   |
| train/             |          |
|    actor_loss      | 0.175    |
|    critic_loss     | 1.27     |
|    learning_rate   | 0.000326 |
|    n_updates       | 587665   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -13.2    |
| time/              |          |
|    episodes        | 308      |
|    fps             | 14       |
|    time_elapsed    | 41664    |
|    total timesteps | 615679   |
| train/             |          |
|    actor_loss      | 0.197    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.00032  |
|    n_updates       | 595669   |
---------------------------------
Eval num_timesteps=620000, episode_reward=0.84 +/- 92.81
Episode length: 1944.20 +/- 113.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.94e+03 |
|    mean_reward     | 0.838    |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | 0.206    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.000316 |
|    n_updates       | 601672   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -10.2    |
| time/              |          |
|    episodes        | 312      |
|    fps             | 14       |
|    time_elapsed    | 42299    |
|    total timesteps | 623683   |
| train/             |          |
|    actor_loss      | 0.23     |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.000315 |
|    n_updates       | 603673   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.62    |
| time/              |          |
|    episodes        | 316      |
|    fps             | 14       |
|    time_elapsed    | 42802    |
|    total timesteps | 631687   |
| train/             |          |
|    actor_loss      | 0.308    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.000309 |
|    n_updates       | 611677   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.88    |
| time/              |          |
|    episodes        | 320      |
|    fps             | 14       |
|    time_elapsed    | 43307    |
|    total timesteps | 639691   |
| train/             |          |
|    actor_loss      | 0.259    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.000304 |
|    n_updates       | 619681   |
---------------------------------
Eval num_timesteps=640000, episode_reward=-0.69 +/- 47.99
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -0.685   |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | 0.237    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.000302 |
|    n_updates       | 621682   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.27    |
| time/              |          |
|    episodes        | 324      |
|    fps             | 14       |
|    time_elapsed    | 43943    |
|    total timesteps | 647695   |
| train/             |          |
|    actor_loss      | 0.231    |
|    critic_loss     | 1.28     |
|    learning_rate   | 0.000298 |
|    n_updates       | 627685   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.69    |
| time/              |          |
|    episodes        | 328      |
|    fps             | 14       |
|    time_elapsed    | 44447    |
|    total timesteps | 655699   |
| train/             |          |
|    actor_loss      | 0.213    |
|    critic_loss     | 1.28     |
|    learning_rate   | 0.000292 |
|    n_updates       | 635689   |
---------------------------------
Eval num_timesteps=660000, episode_reward=-39.60 +/- 79.68
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -39.6    |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | 0.2      |
|    critic_loss     | 1.27     |
|    learning_rate   | 0.000288 |
|    n_updates       | 641692   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    episodes        | 332      |
|    fps             | 14       |
|    time_elapsed    | 45084    |
|    total timesteps | 663703   |
| train/             |          |
|    actor_loss      | 0.193    |
|    critic_loss     | 1.24     |
|    learning_rate   | 0.000287 |
|    n_updates       | 643693   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.32    |
| time/              |          |
|    episodes        | 336      |
|    fps             | 14       |
|    time_elapsed    | 45584    |
|    total timesteps | 671707   |
| train/             |          |
|    actor_loss      | 0.194    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.000281 |
|    n_updates       | 651697   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -11.7    |
| time/              |          |
|    episodes        | 340      |
|    fps             | 14       |
|    time_elapsed    | 46084    |
|    total timesteps | 679711   |
| train/             |          |
|    actor_loss      | 0.467    |
|    critic_loss     | 1.29     |
|    learning_rate   | 0.000276 |
|    n_updates       | 659701   |
---------------------------------
Eval num_timesteps=680000, episode_reward=20.67 +/- 53.01
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 20.7     |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | 0.459    |
|    critic_loss     | 1.25     |
|    learning_rate   | 0.000274 |
|    n_updates       | 661702   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -11      |
| time/              |          |
|    episodes        | 344      |
|    fps             | 14       |
|    time_elapsed    | 46719    |
|    total timesteps | 687715   |
| train/             |          |
|    actor_loss      | 0.452    |
|    critic_loss     | 1.3      |
|    learning_rate   | 0.00027  |
|    n_updates       | 667705   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -10      |
| time/              |          |
|    episodes        | 348      |
|    fps             | 14       |
|    time_elapsed    | 47218    |
|    total timesteps | 695719   |
| train/             |          |
|    actor_loss      | 0.444    |
|    critic_loss     | 1.31     |
|    learning_rate   | 0.000264 |
|    n_updates       | 675709   |
---------------------------------
Eval num_timesteps=700000, episode_reward=32.99 +/- 60.49
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 33       |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | 0.445    |
|    critic_loss     | 1.3      |
|    learning_rate   | 0.00026  |
|    n_updates       | 681712   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -10.7    |
| time/              |          |
|    episodes        | 352      |
|    fps             | 14       |
|    time_elapsed    | 47853    |
|    total timesteps | 703723   |
| train/             |          |
|    actor_loss      | 0.437    |
|    critic_loss     | 1.29     |
|    learning_rate   | 0.000259 |
|    n_updates       | 683713   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    episodes        | 356      |
|    fps             | 14       |
|    time_elapsed    | 48356    |
|    total timesteps | 711727   |
| train/             |          |
|    actor_loss      | 0.409    |
|    critic_loss     | 1.33     |
|    learning_rate   | 0.000253 |
|    n_updates       | 691717   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -13.6    |
| time/              |          |
|    episodes        | 360      |
|    fps             | 14       |
|    time_elapsed    | 48859    |
|    total timesteps | 719731   |
| train/             |          |
|    actor_loss      | 0.404    |
|    critic_loss     | 1.32     |
|    learning_rate   | 0.000248 |
|    n_updates       | 699721   |
---------------------------------
Eval num_timesteps=720000, episode_reward=-59.55 +/- 85.50
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -59.6    |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | 0.399    |
|    critic_loss     | 1.25     |
|    learning_rate   | 0.000246 |
|    n_updates       | 701722   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -15.8    |
| time/              |          |
|    episodes        | 364      |
|    fps             | 14       |
|    time_elapsed    | 49496    |
|    total timesteps | 727735   |
| train/             |          |
|    actor_loss      | 0.393    |
|    critic_loss     | 1.32     |
|    learning_rate   | 0.000242 |
|    n_updates       | 707725   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -16.5    |
| time/              |          |
|    episodes        | 368      |
|    fps             | 14       |
|    time_elapsed    | 50007    |
|    total timesteps | 735739   |
| train/             |          |
|    actor_loss      | 0.391    |
|    critic_loss     | 1.29     |
|    learning_rate   | 0.000236 |
|    n_updates       | 715729   |
---------------------------------
Eval num_timesteps=740000, episode_reward=-29.02 +/- 28.05
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29      |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | 0.39     |
|    critic_loss     | 1.29     |
|    learning_rate   | 0.000232 |
|    n_updates       | 721732   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -15.6    |
| time/              |          |
|    episodes        | 372      |
|    fps             | 14       |
|    time_elapsed    | 50661    |
|    total timesteps | 743743   |
| train/             |          |
|    actor_loss      | 0.392    |
|    critic_loss     | 1.32     |
|    learning_rate   | 0.000231 |
|    n_updates       | 723733   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -16.1    |
| time/              |          |
|    episodes        | 376      |
|    fps             | 14       |
|    time_elapsed    | 51165    |
|    total timesteps | 751747   |
| train/             |          |
|    actor_loss      | 0.378    |
|    critic_loss     | 1.3      |
|    learning_rate   | 0.000225 |
|    n_updates       | 731737   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -15.8    |
| time/              |          |
|    episodes        | 380      |
|    fps             | 14       |
|    time_elapsed    | 51670    |
|    total timesteps | 759751   |
| train/             |          |
|    actor_loss      | 0.403    |
|    critic_loss     | 1.36     |
|    learning_rate   | 0.00022  |
|    n_updates       | 739741   |
---------------------------------
Eval num_timesteps=760000, episode_reward=-49.99 +/- 40.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -50      |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | 0.407    |
|    critic_loss     | 1.34     |
|    learning_rate   | 0.000218 |
|    n_updates       | 741742   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -16.9    |
| time/              |          |
|    episodes        | 384      |
|    fps             | 14       |
|    time_elapsed    | 52306    |
|    total timesteps | 767755   |
| train/             |          |
|    actor_loss      | 0.431    |
|    critic_loss     | 1.38     |
|    learning_rate   | 0.000214 |
|    n_updates       | 747745   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -17.7    |
| time/              |          |
|    episodes        | 388      |
|    fps             | 14       |
|    time_elapsed    | 52808    |
|    total timesteps | 775759   |
| train/             |          |
|    actor_loss      | 0.488    |
|    critic_loss     | 1.43     |
|    learning_rate   | 0.000208 |
|    n_updates       | 755749   |
---------------------------------
Eval num_timesteps=780000, episode_reward=-26.55 +/- 30.84
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -26.5    |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | 0.516    |
|    critic_loss     | 1.42     |
|    learning_rate   | 0.000204 |
|    n_updates       | 761752   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -19.7    |
| time/              |          |
|    episodes        | 392      |
|    fps             | 14       |
|    time_elapsed    | 53445    |
|    total timesteps | 783763   |
| train/             |          |
|    actor_loss      | 0.534    |
|    critic_loss     | 1.44     |
|    learning_rate   | 0.000203 |
|    n_updates       | 763753   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -19.1    |
| time/              |          |
|    episodes        | 396      |
|    fps             | 14       |
|    time_elapsed    | 53919    |
|    total timesteps | 791280   |
| train/             |          |
|    actor_loss      | 0.59     |
|    critic_loss     | 1.48     |
|    learning_rate   | 0.000198 |
|    n_updates       | 771270   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -19.7    |
| time/              |          |
|    episodes        | 400      |
|    fps             | 14       |
|    time_elapsed    | 54416    |
|    total timesteps | 799284   |
| train/             |          |
|    actor_loss      | 0.639    |
|    critic_loss     | 1.44     |
|    learning_rate   | 0.000192 |
|    n_updates       | 779274   |
---------------------------------
Eval num_timesteps=800000, episode_reward=2.65 +/- 24.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.65     |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | 0.682    |
|    critic_loss     | 1.51     |
|    learning_rate   | 0.000191 |
|    n_updates       | 781275   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -19.8    |
| time/              |          |
|    episodes        | 404      |
|    fps             | 14       |
|    time_elapsed    | 55057    |
|    total timesteps | 807288   |
| train/             |          |
|    actor_loss      | 0.807    |
|    critic_loss     | 1.5      |
|    learning_rate   | 0.000186 |
|    n_updates       | 787278   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -17.5    |
| time/              |          |
|    episodes        | 408      |
|    fps             | 14       |
|    time_elapsed    | 55561    |
|    total timesteps | 815292   |
| train/             |          |
|    actor_loss      | 0.824    |
|    critic_loss     | 1.51     |
|    learning_rate   | 0.000181 |
|    n_updates       | 795282   |
---------------------------------
Eval num_timesteps=820000, episode_reward=-26.03 +/- 37.26
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -26      |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | 0.824    |
|    critic_loss     | 1.47     |
|    learning_rate   | 0.000176 |
|    n_updates       | 801285   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -19.9    |
| time/              |          |
|    episodes        | 412      |
|    fps             | 14       |
|    time_elapsed    | 56194    |
|    total timesteps | 823296   |
| train/             |          |
|    actor_loss      | 0.816    |
|    critic_loss     | 1.46     |
|    learning_rate   | 0.000175 |
|    n_updates       | 803286   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -18.8    |
| time/              |          |
|    episodes        | 416      |
|    fps             | 14       |
|    time_elapsed    | 56694    |
|    total timesteps | 831300   |
| train/             |          |
|    actor_loss      | 0.772    |
|    critic_loss     | 1.46     |
|    learning_rate   | 0.000169 |
|    n_updates       | 811290   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -18.3    |
| time/              |          |
|    episodes        | 420      |
|    fps             | 14       |
|    time_elapsed    | 57195    |
|    total timesteps | 839304   |
| train/             |          |
|    actor_loss      | 0.745    |
|    critic_loss     | 1.47     |
|    learning_rate   | 0.000164 |
|    n_updates       | 819294   |
---------------------------------
Eval num_timesteps=840000, episode_reward=1.08 +/- 28.76
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 1.08     |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | 0.729    |
|    critic_loss     | 1.43     |
|    learning_rate   | 0.000162 |
|    n_updates       | 821295   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -16.5    |
| time/              |          |
|    episodes        | 424      |
|    fps             | 14       |
|    time_elapsed    | 57831    |
|    total timesteps | 847308   |
| train/             |          |
|    actor_loss      | 0.703    |
|    critic_loss     | 1.45     |
|    learning_rate   | 0.000158 |
|    n_updates       | 827298   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -15.7    |
| time/              |          |
|    episodes        | 428      |
|    fps             | 14       |
|    time_elapsed    | 58334    |
|    total timesteps | 855312   |
| train/             |          |
|    actor_loss      | 0.669    |
|    critic_loss     | 1.49     |
|    learning_rate   | 0.000153 |
|    n_updates       | 835302   |
---------------------------------
Eval num_timesteps=860000, episode_reward=-35.04 +/- 49.26
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -35      |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | 0.66     |
|    critic_loss     | 1.44     |
|    learning_rate   | 0.000148 |
|    n_updates       | 841305   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -13.4    |
| time/              |          |
|    episodes        | 432      |
|    fps             | 14       |
|    time_elapsed    | 58967    |
|    total timesteps | 863316   |
| train/             |          |
|    actor_loss      | 0.653    |
|    critic_loss     | 1.42     |
|    learning_rate   | 0.000147 |
|    n_updates       | 843306   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -15.8    |
| time/              |          |
|    episodes        | 436      |
|    fps             | 14       |
|    time_elapsed    | 59467    |
|    total timesteps | 871320   |
| train/             |          |
|    actor_loss      | 0.66     |
|    critic_loss     | 1.44     |
|    learning_rate   | 0.000141 |
|    n_updates       | 851310   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -15.9    |
| time/              |          |
|    episodes        | 440      |
|    fps             | 14       |
|    time_elapsed    | 59969    |
|    total timesteps | 879324   |
| train/             |          |
|    actor_loss      | 0.704    |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.000136 |
|    n_updates       | 859314   |
---------------------------------
Eval num_timesteps=880000, episode_reward=15.93 +/- 40.03
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | 0.724    |
|    critic_loss     | 1.49     |
|    learning_rate   | 0.000134 |
|    n_updates       | 861315   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -16.7    |
| time/              |          |
|    episodes        | 444      |
|    fps             | 14       |
|    time_elapsed    | 60600    |
|    total timesteps | 887328   |
| train/             |          |
|    actor_loss      | 0.738    |
|    critic_loss     | 1.48     |
|    learning_rate   | 0.00013  |
|    n_updates       | 867318   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -18.2    |
| time/              |          |
|    episodes        | 448      |
|    fps             | 14       |
|    time_elapsed    | 61109    |
|    total timesteps | 895332   |
| train/             |          |
|    actor_loss      | 0.785    |
|    critic_loss     | 1.49     |
|    learning_rate   | 0.000125 |
|    n_updates       | 875322   |
---------------------------------
Traceback (most recent call last):
  File "train.py", line 61, in <module>
    model.learn(args.timesteps)
  File "/root/trainer/learning/explore.py", line 165, in learn
    callback = self.rl_callback
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 211, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 359, in learn
    log_interval=log_interval,
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 575, in collect_rollouts
    if callback.on_step() is False:
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/callbacks.py", line 88, in on_step
    return self._on_step()
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/callbacks.py", line 192, in _on_step
    continue_training = callback.on_step() and continue_training
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/callbacks.py", line 88, in on_step
    return self._on_step()
  File "/root/trainer/utils/callbacks.py", line 317, in _on_step
    callback=self._log_success_callback,
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/evaluation.py", line 86, in evaluate_policy
    observations, rewards, dones, infos = env.step(actions)
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 162, in step
    return self.step_wait()
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/vec_normalize.py", line 113, in step_wait
    obs, rewards, dones, infos = self.venv.step_wait()
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/vec_transpose.py", line 83, in step_wait
    observations, rewards, dones, infos = self.venv.step_wait()
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 44, in step_wait
    self.actions[env_idx]
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/monitor.py", line 90, in step
    observation, reward, done, info = self.env.step(action)
  File "/root/trainer/simulations/maze_env.py", line 571, in step
    inner_next_obs, inner_reward, _, info = self.wrapped_env.step(action)
  File "/root/trainer/simulations/point.py", line 54, in step
    self.sim.step()
  File "mujoco_py/mjsim.pyx", line 119, in mujoco_py.cymj.MjSim.step
  File "mujoco_py/cymj.pyx", line 115, in mujoco_py.cymj.wrap_mujoco_warning.__exit__
  File "mujoco_py/cymj.pyx", line 75, in mujoco_py.cymj.c_warning_callback
  File "/usr/local/lib/python3.6/site-packages/mujoco_py/builder.py", line 354, in user_warning_raise_exception
    raise MujocoException(warn + 'Check for NaN in simulation.')
mujoco_py.builder.MujocoException: Unknown warning type Time = 19.0000.Check for NaN in simulation.
running build_ext
building 'mujoco_py.cymj' extension
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o -fopenmp -w
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py
gcc -pthread -shared /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mjpro150/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mjpro150/bin -lmujoco150 -lglewegl -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp
2021-12-13 13:54:51.184815: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-13 13:54:51.184902: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp13/TD3_17
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 79       |
|    time_elapsed    | 100      |
|    total timesteps | 8004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -25.3    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 81       |
|    time_elapsed    | 197      |
|    total timesteps | 16008    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=20000, episode_reward=-203.59 +/- 394.98
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -27.6    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 45       |
|    time_elapsed    | 522      |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 8.6      |
|    critic_loss     | 3.19     |
|    learning_rate   | 0.000978 |
|    n_updates       | 4002     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -29.3    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 38       |
|    time_elapsed    | 823      |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 5.66     |
|    critic_loss     | 1.52     |
|    learning_rate   | 0.00097  |
|    n_updates       | 12006    |
---------------------------------
Terminated
2021-12-13 14:15:46.513784: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-13 14:15:46.513849: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp13/TD3_18
