running build_ext
building 'mujoco_py.cymj' extension
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o -fopenmp -w
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py
gcc -pthread -shared /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mjpro150/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mjpro150/bin -lmujoco150 -lglewegl -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp
2021-12-15 13:11:45.552371: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-15 13:11:45.552458: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Env Type: maze
Task: CustomGoalReward4Rooms
Using cuda device
Logging to assets/out/models/exp15/TD3_26
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -32.5    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 11       |
|    time_elapsed    | 718      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 29.7     |
|    critic_loss     | 62.4     |
|    learning_rate   | 0.000994 |
|    n_updates       | 6003     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -43.5    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 9        |
|    time_elapsed    | 1634     |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 4.34     |
|    critic_loss     | 37.2     |
|    learning_rate   | 0.000986 |
|    n_updates       | 14007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-10.48 +/- 17.24
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.5    |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 3.71     |
|    critic_loss     | 34.7     |
|    learning_rate   | 0.000982 |
|    n_updates       | 18009    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -39.2    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 8        |
|    time_elapsed    | 2681     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 4.7      |
|    critic_loss     | 40.3     |
|    learning_rate   | 0.000978 |
|    n_updates       | 22011    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -47.1    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 8        |
|    time_elapsed    | 3587     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 4.47     |
|    critic_loss     | 38.3     |
|    learning_rate   | 0.00097  |
|    n_updates       | 30015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-46.78 +/- 28.59
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -46.8    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 4.19     |
|    critic_loss     | 38       |
|    learning_rate   | 0.000962 |
|    n_updates       | 38019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -42.4    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 8        |
|    time_elapsed    | 4643     |
|    total timesteps | 40020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -44.2    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 8        |
|    time_elapsed    | 5551     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 4.9      |
|    critic_loss     | 38.7     |
|    learning_rate   | 0.000954 |
|    n_updates       | 46023    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -32.7    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 8        |
|    time_elapsed    | 6443     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 3.99     |
|    critic_loss     | 35.2     |
|    learning_rate   | 0.000947 |
|    n_updates       | 54027    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-15.06 +/- 53.15
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -15.1    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 3.29     |
|    critic_loss     | 32.4     |
|    learning_rate   | 0.000943 |
|    n_updates       | 58029    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -27      |
| time/              |          |
|    episodes        | 32       |
|    fps             | 8        |
|    time_elapsed    | 7465     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 3.1      |
|    critic_loss     | 34.4     |
|    learning_rate   | 0.000939 |
|    n_updates       | 62031    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -24.4    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 8        |
|    time_elapsed    | 8359     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 3.01     |
|    critic_loss     | 33.6     |
|    learning_rate   | 0.000931 |
|    n_updates       | 70035    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-37.19 +/- 29.72
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -37.2    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 2.43     |
|    critic_loss     | 30.7     |
|    learning_rate   | 0.000923 |
|    n_updates       | 78039    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -21.6    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 8        |
|    time_elapsed    | 9388     |
|    total timesteps | 80040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 8        |
|    time_elapsed    | 10277    |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 2.25     |
|    critic_loss     | 28.7     |
|    learning_rate   | 0.000915 |
|    n_updates       | 86043    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 8        |
|    time_elapsed    | 11153    |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 1.97     |
|    critic_loss     | 27.6     |
|    learning_rate   | 0.000907 |
|    n_updates       | 94047    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-52.60 +/- 37.76
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -52.6    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 1.78     |
|    critic_loss     | 26.4     |
|    learning_rate   | 0.000903 |
|    n_updates       | 98049    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.48    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 8        |
|    time_elapsed    | 12140    |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | 1.63     |
|    critic_loss     | 26       |
|    learning_rate   | 0.000899 |
|    n_updates       | 102051   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -3.4     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 8        |
|    time_elapsed    | 12932    |
|    total timesteps | 111627   |
| train/             |          |
|    actor_loss      | 0.681    |
|    critic_loss     | 26.7     |
|    learning_rate   | 0.000891 |
|    n_updates       | 109626   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -4.33    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 8        |
|    time_elapsed    | 13767    |
|    total timesteps | 119631   |
| train/             |          |
|    actor_loss      | 0.374    |
|    critic_loss     | 26.8     |
|    learning_rate   | 0.000884 |
|    n_updates       | 117630   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-12.35 +/- 33.59
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -12.3    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 0.402    |
|    critic_loss     | 25.8     |
|    learning_rate   | 0.000882 |
|    n_updates       | 119631   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -3.28    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 8        |
|    time_elapsed    | 14741    |
|    total timesteps | 127635   |
| train/             |          |
|    actor_loss      | 0.453    |
|    critic_loss     | 25.9     |
|    learning_rate   | 0.000876 |
|    n_updates       | 125634   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -2.77    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 8        |
|    time_elapsed    | 15594    |
|    total timesteps | 135639   |
| train/             |          |
|    actor_loss      | 0.362    |
|    critic_loss     | 25.1     |
|    learning_rate   | 0.000868 |
|    n_updates       | 133638   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-18.67 +/- 39.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -18.7    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 0.275    |
|    critic_loss     | 24.2     |
|    learning_rate   | 0.000862 |
|    n_updates       | 139641   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.4     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 8        |
|    time_elapsed    | 16621    |
|    total timesteps | 143643   |
| train/             |          |
|    actor_loss      | 0.366    |
|    critic_loss     | 25.5     |
|    learning_rate   | 0.00086  |
|    n_updates       | 141642   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.937   |
| time/              |          |
|    episodes        | 76       |
|    fps             | 8        |
|    time_elapsed    | 17515    |
|    total timesteps | 151647   |
| train/             |          |
|    actor_loss      | 0.179    |
|    critic_loss     | 25.4     |
|    learning_rate   | 0.000852 |
|    n_updates       | 149646   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.21     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 8        |
|    time_elapsed    | 18408    |
|    total timesteps | 159651   |
| train/             |          |
|    actor_loss      | -0.0714  |
|    critic_loss     | 24.1     |
|    learning_rate   | 0.000844 |
|    n_updates       | 157650   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-45.49 +/- 52.36
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -45.5    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -0.101   |
|    critic_loss     | 25.8     |
|    learning_rate   | 0.000842 |
|    n_updates       | 159651   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.328   |
| time/              |          |
|    episodes        | 84       |
|    fps             | 8        |
|    time_elapsed    | 19427    |
|    total timesteps | 167655   |
| train/             |          |
|    actor_loss      | -0.156   |
|    critic_loss     | 24.5     |
|    learning_rate   | 0.000836 |
|    n_updates       | 165654   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 8        |
|    time_elapsed    | 20325    |
|    total timesteps | 175659   |
| train/             |          |
|    actor_loss      | -0.11    |
|    critic_loss     | 23.9     |
|    learning_rate   | 0.000828 |
|    n_updates       | 173658   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-5.03 +/- 20.39
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -5.03    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -0.0264  |
|    critic_loss     | 24.8     |
|    learning_rate   | 0.000822 |
|    n_updates       | 179661   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.13    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 8        |
|    time_elapsed    | 21362    |
|    total timesteps | 183663   |
| train/             |          |
|    actor_loss      | 0.00981  |
|    critic_loss     | 25.9     |
|    learning_rate   | 0.00082  |
|    n_updates       | 181662   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.36    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 8        |
|    time_elapsed    | 22262    |
|    total timesteps | 191667   |
| train/             |          |
|    actor_loss      | 0.257    |
|    critic_loss     | 26.3     |
|    learning_rate   | 0.000812 |
|    n_updates       | 189666   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.05    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 8        |
|    time_elapsed    | 23164    |
|    total timesteps | 199671   |
| train/             |          |
|    actor_loss      | 0.294    |
|    critic_loss     | 25.2     |
|    learning_rate   | 0.000804 |
|    n_updates       | 197670   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-29.22 +/- 10.80
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29.2    |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 0.251    |
|    critic_loss     | 25.8     |
|    learning_rate   | 0.000802 |
|    n_updates       | 199671   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.56    |
| time/              |          |
|    episodes        | 104      |
|    fps             | 8        |
|    time_elapsed    | 24166    |
|    total timesteps | 207675   |
| train/             |          |
|    actor_loss      | 0.403    |
|    critic_loss     | 26.2     |
|    learning_rate   | 0.000796 |
|    n_updates       | 205674   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -2.55    |
| time/              |          |
|    episodes        | 108      |
|    fps             | 8        |
|    time_elapsed    | 25003    |
|    total timesteps | 215504   |
| train/             |          |
|    actor_loss      | 0.547    |
|    critic_loss     | 27       |
|    learning_rate   | 0.000789 |
|    n_updates       | 213503   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-27.22 +/- 22.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27.2    |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 0.458    |
|    critic_loss     | 25.8     |
|    learning_rate   | 0.000783 |
|    n_updates       | 219506   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -0.0925  |
| time/              |          |
|    episodes        | 112      |
|    fps             | 8        |
|    time_elapsed    | 25990    |
|    total timesteps | 223508   |
| train/             |          |
|    actor_loss      | 0.496    |
|    critic_loss     | 25.5     |
|    learning_rate   | 0.000781 |
|    n_updates       | 221507   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.57     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 8        |
|    time_elapsed    | 26840    |
|    total timesteps | 231512   |
| train/             |          |
|    actor_loss      | 0.524    |
|    critic_loss     | 25.6     |
|    learning_rate   | 0.000773 |
|    n_updates       | 229511   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.09     |
| time/              |          |
|    episodes        | 120      |
|    fps             | 8        |
|    time_elapsed    | 27684    |
|    total timesteps | 239516   |
| train/             |          |
|    actor_loss      | 0.794    |
|    critic_loss     | 27       |
|    learning_rate   | 0.000765 |
|    n_updates       | 237515   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-45.57 +/- 17.54
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -45.6    |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.613    |
|    critic_loss     | 25.7     |
|    learning_rate   | 0.000763 |
|    n_updates       | 239516   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.19     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 8        |
|    time_elapsed    | 28701    |
|    total timesteps | 247520   |
| train/             |          |
|    actor_loss      | 0.54     |
|    critic_loss     | 27.4     |
|    learning_rate   | 0.000757 |
|    n_updates       | 245519   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.77     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 8        |
|    time_elapsed    | 29593    |
|    total timesteps | 255524   |
| train/             |          |
|    actor_loss      | 0.528    |
|    critic_loss     | 26.7     |
|    learning_rate   | 0.000749 |
|    n_updates       | 253523   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-36.71 +/- 14.76
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -36.7    |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 0.704    |
|    critic_loss     | 27.5     |
|    learning_rate   | 0.000743 |
|    n_updates       | 259526   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.65     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 8        |
|    time_elapsed    | 30627    |
|    total timesteps | 263528   |
| train/             |          |
|    actor_loss      | 0.632    |
|    critic_loss     | 26.8     |
|    learning_rate   | 0.000741 |
|    n_updates       | 261527   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.55     |
| time/              |          |
|    episodes        | 136      |
|    fps             | 8        |
|    time_elapsed    | 31519    |
|    total timesteps | 271532   |
| train/             |          |
|    actor_loss      | 0.461    |
|    critic_loss     | 25.9     |
|    learning_rate   | 0.000733 |
|    n_updates       | 269531   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.08     |
| time/              |          |
|    episodes        | 140      |
|    fps             | 8        |
|    time_elapsed    | 32415    |
|    total timesteps | 279536   |
| train/             |          |
|    actor_loss      | 0.413    |
|    critic_loss     | 27       |
|    learning_rate   | 0.000725 |
|    n_updates       | 277535   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-23.31 +/- 25.09
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.3    |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 0.464    |
|    critic_loss     | 27.4     |
|    learning_rate   | 0.000723 |
|    n_updates       | 279536   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 0.786    |
| time/              |          |
|    episodes        | 144      |
|    fps             | 8        |
|    time_elapsed    | 33446    |
|    total timesteps | 287540   |
| train/             |          |
|    actor_loss      | 0.668    |
|    critic_loss     | 26.5     |
|    learning_rate   | 0.000717 |
|    n_updates       | 285539   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.01     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 8        |
|    time_elapsed    | 34338    |
|    total timesteps | 295544   |
| train/             |          |
|    actor_loss      | 0.654    |
|    critic_loss     | 26.9     |
|    learning_rate   | 0.000709 |
|    n_updates       | 293543   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-29.96 +/- 18.47
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -30      |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 0.664    |
|    critic_loss     | 26.5     |
|    learning_rate   | 0.000703 |
|    n_updates       | 299546   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -3.14    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 8        |
|    time_elapsed    | 35364    |
|    total timesteps | 303548   |
| train/             |          |
|    actor_loss      | 0.636    |
|    critic_loss     | 27.5     |
|    learning_rate   | 0.000701 |
|    n_updates       | 301547   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.49    |
| time/              |          |
|    episodes        | 156      |
|    fps             | 8        |
|    time_elapsed    | 36258    |
|    total timesteps | 311552   |
| train/             |          |
|    actor_loss      | 0.597    |
|    critic_loss     | 26.6     |
|    learning_rate   | 0.000694 |
|    n_updates       | 309551   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.98    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 8        |
|    time_elapsed    | 37152    |
|    total timesteps | 319556   |
| train/             |          |
|    actor_loss      | 0.409    |
|    critic_loss     | 26.3     |
|    learning_rate   | 0.000686 |
|    n_updates       | 317555   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-19.05 +/- 8.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19      |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 0.455    |
|    critic_loss     | 25.8     |
|    learning_rate   | 0.000684 |
|    n_updates       | 319556   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.19    |
| time/              |          |
|    episodes        | 164      |
|    fps             | 8        |
|    time_elapsed    | 38184    |
|    total timesteps | 327560   |
| train/             |          |
|    actor_loss      | 0.592    |
|    critic_loss     | 26.3     |
|    learning_rate   | 0.000678 |
|    n_updates       | 325559   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.45    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 8        |
|    time_elapsed    | 39076    |
|    total timesteps | 335564   |
| train/             |          |
|    actor_loss      | 0.532    |
|    critic_loss     | 25.5     |
|    learning_rate   | 0.00067  |
|    n_updates       | 333563   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-13.52 +/- 22.99
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -13.5    |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | 0.549    |
|    critic_loss     | 26.3     |
|    learning_rate   | 0.000664 |
|    n_updates       | 339566   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.69    |
| time/              |          |
|    episodes        | 172      |
|    fps             | 8        |
|    time_elapsed    | 40104    |
|    total timesteps | 343568   |
| train/             |          |
|    actor_loss      | 0.75     |
|    critic_loss     | 27.2     |
|    learning_rate   | 0.000662 |
|    n_updates       | 341567   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.26    |
| time/              |          |
|    episodes        | 176      |
|    fps             | 8        |
|    time_elapsed    | 40996    |
|    total timesteps | 351572   |
| train/             |          |
|    actor_loss      | 0.629    |
|    critic_loss     | 25.8     |
|    learning_rate   | 0.000654 |
|    n_updates       | 349571   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.95    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 8        |
|    time_elapsed    | 41882    |
|    total timesteps | 359576   |
| train/             |          |
|    actor_loss      | 0.773    |
|    critic_loss     | 26.9     |
|    learning_rate   | 0.000646 |
|    n_updates       | 357575   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-22.80 +/- 55.01
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -22.8    |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | 0.686    |
|    critic_loss     | 26.8     |
|    learning_rate   | 0.000644 |
|    n_updates       | 359576   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.59    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 8        |
|    time_elapsed    | 42907    |
|    total timesteps | 367580   |
| train/             |          |
|    actor_loss      | 0.44     |
|    critic_loss     | 26.6     |
|    learning_rate   | 0.000638 |
|    n_updates       | 365579   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.17    |
| time/              |          |
|    episodes        | 188      |
|    fps             | 8        |
|    time_elapsed    | 43796    |
|    total timesteps | 375584   |
| train/             |          |
|    actor_loss      | 0.442    |
|    critic_loss     | 27.2     |
|    learning_rate   | 0.00063  |
|    n_updates       | 373583   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-15.33 +/- 17.57
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -15.3    |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | 0.403    |
|    critic_loss     | 27.6     |
|    learning_rate   | 0.000624 |
|    n_updates       | 379586   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.51    |
| time/              |          |
|    episodes        | 192      |
|    fps             | 8        |
|    time_elapsed    | 44795    |
|    total timesteps | 383588   |
| train/             |          |
|    actor_loss      | 0.26     |
|    critic_loss     | 26.8     |
|    learning_rate   | 0.000622 |
|    n_updates       | 381587   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.72    |
| time/              |          |
|    episodes        | 196      |
|    fps             | 8        |
|    time_elapsed    | 45681    |
|    total timesteps | 391592   |
| train/             |          |
|    actor_loss      | 0.0295   |
|    critic_loss     | 25.5     |
|    learning_rate   | 0.000614 |
|    n_updates       | 389591   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.91    |
| time/              |          |
|    episodes        | 200      |
|    fps             | 8        |
|    time_elapsed    | 46570    |
|    total timesteps | 399596   |
| train/             |          |
|    actor_loss      | -0.0292  |
|    critic_loss     | 25.8     |
|    learning_rate   | 0.000606 |
|    n_updates       | 397595   |
---------------------------------
Eval num_timesteps=400000, episode_reward=-43.00 +/- 22.78
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -43      |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | 0.0316   |
|    critic_loss     | 25.8     |
|    learning_rate   | 0.000604 |
|    n_updates       | 399596   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.43    |
| time/              |          |
|    episodes        | 204      |
|    fps             | 8        |
|    time_elapsed    | 47594    |
|    total timesteps | 407600   |
| train/             |          |
|    actor_loss      | -0.152   |
|    critic_loss     | 25.7     |
|    learning_rate   | 0.000598 |
|    n_updates       | 405599   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.0574   |
| time/              |          |
|    episodes        | 208      |
|    fps             | 8        |
|    time_elapsed    | 48463    |
|    total timesteps | 415604   |
| train/             |          |
|    actor_loss      | -0.0647  |
|    critic_loss     | 25.9     |
|    learning_rate   | 0.000591 |
|    n_updates       | 413603   |
---------------------------------
Eval num_timesteps=420000, episode_reward=-36.08 +/- 26.50
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -36.1    |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -0.304   |
|    critic_loss     | 25.2     |
|    learning_rate   | 0.000585 |
|    n_updates       | 419606   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.916    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 8        |
|    time_elapsed    | 49449    |
|    total timesteps | 423608   |
| train/             |          |
|    actor_loss      | -0.362   |
|    critic_loss     | 26.6     |
|    learning_rate   | 0.000583 |
|    n_updates       | 421607   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.18     |
| time/              |          |
|    episodes        | 216      |
|    fps             | 8        |
|    time_elapsed    | 50295    |
|    total timesteps | 431612   |
| train/             |          |
|    actor_loss      | -0.53    |
|    critic_loss     | 25.3     |
|    learning_rate   | 0.000575 |
|    n_updates       | 429611   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 6.03     |
| time/              |          |
|    episodes        | 220      |
|    fps             | 8        |
|    time_elapsed    | 51139    |
|    total timesteps | 439616   |
| train/             |          |
|    actor_loss      | -0.433   |
|    critic_loss     | 24.2     |
|    learning_rate   | 0.000567 |
|    n_updates       | 437615   |
---------------------------------
Eval num_timesteps=440000, episode_reward=-30.53 +/- 39.98
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -30.5    |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -0.591   |
|    critic_loss     | 24.9     |
|    learning_rate   | 0.000565 |
|    n_updates       | 439616   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.2      |
| time/              |          |
|    episodes        | 224      |
|    fps             | 8        |
|    time_elapsed    | 52129    |
|    total timesteps | 447620   |
| train/             |          |
|    actor_loss      | -0.524   |
|    critic_loss     | 25.4     |
|    learning_rate   | 0.000559 |
|    n_updates       | 445619   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.28     |
| time/              |          |
|    episodes        | 228      |
|    fps             | 8        |
|    time_elapsed    | 53017    |
|    total timesteps | 455624   |
| train/             |          |
|    actor_loss      | -0.191   |
|    critic_loss     | 25.7     |
|    learning_rate   | 0.000551 |
|    n_updates       | 453623   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-8.59 +/- 23.97
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -8.59    |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -0.141   |
|    critic_loss     | 26.3     |
|    learning_rate   | 0.000545 |
|    n_updates       | 459626   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.55     |
| time/              |          |
|    episodes        | 232      |
|    fps             | 8        |
|    time_elapsed    | 54046    |
|    total timesteps | 463628   |
| train/             |          |
|    actor_loss      | 0.0586   |
|    critic_loss     | 26.5     |
|    learning_rate   | 0.000543 |
|    n_updates       | 461627   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    episodes        | 236      |
|    fps             | 8        |
|    time_elapsed    | 54931    |
|    total timesteps | 471632   |
| train/             |          |
|    actor_loss      | -0.0443  |
|    critic_loss     | 25.4     |
|    learning_rate   | 0.000535 |
|    n_updates       | 469631   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 7.82     |
| time/              |          |
|    episodes        | 240      |
|    fps             | 8        |
|    time_elapsed    | 55753    |
|    total timesteps | 479077   |
| train/             |          |
|    actor_loss      | -0.214   |
|    critic_loss     | 25.1     |
|    learning_rate   | 0.000528 |
|    n_updates       | 477076   |
---------------------------------
Eval num_timesteps=480000, episode_reward=-17.47 +/- 18.72
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -17.5    |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -0.239   |
|    critic_loss     | 24.6     |
|    learning_rate   | 0.000526 |
|    n_updates       | 479077   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 9.92     |
| time/              |          |
|    episodes        | 244      |
|    fps             | 8        |
|    time_elapsed    | 56771    |
|    total timesteps | 487081   |
| train/             |          |
|    actor_loss      | -0.366   |
|    critic_loss     | 25.4     |
|    learning_rate   | 0.00052  |
|    n_updates       | 485080   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 6.69     |
| time/              |          |
|    episodes        | 248      |
|    fps             | 8        |
|    time_elapsed    | 57653    |
|    total timesteps | 495085   |
| train/             |          |
|    actor_loss      | -0.295   |
|    critic_loss     | 24.8     |
|    learning_rate   | 0.000512 |
|    n_updates       | 493084   |
---------------------------------
Eval num_timesteps=500000, episode_reward=-25.97 +/- 30.07
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -26      |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -0.291   |
|    critic_loss     | 25.6     |
|    learning_rate   | 0.000506 |
|    n_updates       | 499087   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 8.95     |
| time/              |          |
|    episodes        | 252      |
|    fps             | 8        |
|    time_elapsed    | 58672    |
|    total timesteps | 503089   |
| train/             |          |
|    actor_loss      | -0.237   |
|    critic_loss     | 25.4     |
|    learning_rate   | 0.000504 |
|    n_updates       | 501088   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 8.38     |
| time/              |          |
|    episodes        | 256      |
|    fps             | 8        |
|    time_elapsed    | 59562    |
|    total timesteps | 511093   |
| train/             |          |
|    actor_loss      | -0.189   |
|    critic_loss     | 23.7     |
|    learning_rate   | 0.000496 |
|    n_updates       | 509092   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 9.84     |
| time/              |          |
|    episodes        | 260      |
|    fps             | 8        |
|    time_elapsed    | 60454    |
|    total timesteps | 519097   |
| train/             |          |
|    actor_loss      | -0.16    |
|    critic_loss     | 25.8     |
|    learning_rate   | 0.000488 |
|    n_updates       | 517096   |
---------------------------------
Eval num_timesteps=520000, episode_reward=-25.43 +/- 20.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -25.4    |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -0.197   |
|    critic_loss     | 26.5     |
|    learning_rate   | 0.000486 |
|    n_updates       | 519097   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    episodes        | 264      |
|    fps             | 8        |
|    time_elapsed    | 61474    |
|    total timesteps | 527101   |
| train/             |          |
|    actor_loss      | -0.26    |
|    critic_loss     | 25.5     |
|    learning_rate   | 0.00048  |
|    n_updates       | 525100   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    episodes        | 268      |
|    fps             | 8        |
|    time_elapsed    | 62365    |
|    total timesteps | 535105   |
| train/             |          |
|    actor_loss      | -0.269   |
|    critic_loss     | 25.7     |
|    learning_rate   | 0.000472 |
|    n_updates       | 533104   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-40.97 +/- 38.07
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -41      |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -0.0216  |
|    critic_loss     | 26.8     |
|    learning_rate   | 0.000466 |
|    n_updates       | 539107   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 8.77     |
| time/              |          |
|    episodes        | 272      |
|    fps             | 8        |
|    time_elapsed    | 63367    |
|    total timesteps | 543109   |
| train/             |          |
|    actor_loss      | -0.0715  |
|    critic_loss     | 25.8     |
|    learning_rate   | 0.000464 |
|    n_updates       | 541108   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 8.7      |
| time/              |          |
|    episodes        | 276      |
|    fps             | 8        |
|    time_elapsed    | 64233    |
|    total timesteps | 551113   |
| train/             |          |
|    actor_loss      | -0.148   |
|    critic_loss     | 26.6     |
|    learning_rate   | 0.000456 |
|    n_updates       | 549112   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    episodes        | 280      |
|    fps             | 8        |
|    time_elapsed    | 65095    |
|    total timesteps | 559117   |
| train/             |          |
|    actor_loss      | -0.144   |
|    critic_loss     | 24.7     |
|    learning_rate   | 0.000448 |
|    n_updates       | 557116   |
---------------------------------
Eval num_timesteps=560000, episode_reward=-19.80 +/- 51.52
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19.8    |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -0.381   |
|    critic_loss     | 25.2     |
|    learning_rate   | 0.000446 |
|    n_updates       | 559117   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    episodes        | 284      |
|    fps             | 8        |
|    time_elapsed    | 66084    |
|    total timesteps | 567121   |
| train/             |          |
|    actor_loss      | -0.407   |
|    critic_loss     | 25.1     |
|    learning_rate   | 0.000441 |
|    n_updates       | 565120   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    episodes        | 288      |
|    fps             | 8        |
|    time_elapsed    | 66936    |
|    total timesteps | 575125   |
| train/             |          |
|    actor_loss      | -0.768   |
|    critic_loss     | 24.2     |
|    learning_rate   | 0.000433 |
|    n_updates       | 573124   |
---------------------------------
Eval num_timesteps=580000, episode_reward=14.62 +/- 4.69
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -0.821   |
|    critic_loss     | 25.5     |
|    learning_rate   | 0.000427 |
|    n_updates       | 579127   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 12.2     |
| time/              |          |
|    episodes        | 292      |
|    fps             | 8        |
|    time_elapsed    | 67935    |
|    total timesteps | 583129   |
| train/             |          |
|    actor_loss      | -0.778   |
|    critic_loss     | 24.6     |
|    learning_rate   | 0.000425 |
|    n_updates       | 581128   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 12.7     |
| time/              |          |
|    episodes        | 296      |
|    fps             | 8        |
|    time_elapsed    | 68829    |
|    total timesteps | 591133   |
| train/             |          |
|    actor_loss      | -0.684   |
|    critic_loss     | 24.3     |
|    learning_rate   | 0.000417 |
|    n_updates       | 589132   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 10.4     |
| time/              |          |
|    episodes        | 300      |
|    fps             | 8        |
|    time_elapsed    | 69719    |
|    total timesteps | 599137   |
| train/             |          |
|    actor_loss      | -0.722   |
|    critic_loss     | 23.5     |
|    learning_rate   | 0.000409 |
|    n_updates       | 597136   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-35.47 +/- 37.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -35.5    |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -0.682   |
|    critic_loss     | 24.6     |
|    learning_rate   | 0.000407 |
|    n_updates       | 599137   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 9.05     |
| time/              |          |
|    episodes        | 304      |
|    fps             | 8        |
|    time_elapsed    | 70749    |
|    total timesteps | 607141   |
| train/             |          |
|    actor_loss      | -0.437   |
|    critic_loss     | 25.2     |
|    learning_rate   | 0.000401 |
|    n_updates       | 605140   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 8.71     |
| time/              |          |
|    episodes        | 308      |
|    fps             | 8        |
|    time_elapsed    | 71644    |
|    total timesteps | 615145   |
| train/             |          |
|    actor_loss      | -0.553   |
|    critic_loss     | 24.8     |
|    learning_rate   | 0.000393 |
|    n_updates       | 613144   |
---------------------------------
Eval num_timesteps=620000, episode_reward=-35.82 +/- 26.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -35.8    |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -0.509   |
|    critic_loss     | 24.2     |
|    learning_rate   | 0.000387 |
|    n_updates       | 619147   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 8.78     |
| time/              |          |
|    episodes        | 312      |
|    fps             | 8        |
|    time_elapsed    | 72674    |
|    total timesteps | 623149   |
| train/             |          |
|    actor_loss      | -0.493   |
|    critic_loss     | 24.2     |
|    learning_rate   | 0.000385 |
|    n_updates       | 621148   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 8.3      |
| time/              |          |
|    episodes        | 316      |
|    fps             | 8        |
|    time_elapsed    | 73569    |
|    total timesteps | 631153   |
| train/             |          |
|    actor_loss      | -0.705   |
|    critic_loss     | 24.5     |
|    learning_rate   | 0.000377 |
|    n_updates       | 629152   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 6.92     |
| time/              |          |
|    episodes        | 320      |
|    fps             | 8        |
|    time_elapsed    | 74463    |
|    total timesteps | 639157   |
| train/             |          |
|    actor_loss      | -0.892   |
|    critic_loss     | 25.4     |
|    learning_rate   | 0.000369 |
|    n_updates       | 637156   |
---------------------------------
Eval num_timesteps=640000, episode_reward=-4.01 +/- 29.11
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -4.01    |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -0.923   |
|    critic_loss     | 24.3     |
|    learning_rate   | 0.000367 |
|    n_updates       | 639157   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 9.64     |
| time/              |          |
|    episodes        | 324      |
|    fps             | 8        |
|    time_elapsed    | 75491    |
|    total timesteps | 647161   |
| train/             |          |
|    actor_loss      | -0.992   |
|    critic_loss     | 23.4     |
|    learning_rate   | 0.000361 |
|    n_updates       | 645160   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 10       |
| time/              |          |
|    episodes        | 328      |
|    fps             | 8        |
|    time_elapsed    | 76387    |
|    total timesteps | 655165   |
| train/             |          |
|    actor_loss      | -1.23    |
|    critic_loss     | 22.9     |
|    learning_rate   | 0.000353 |
|    n_updates       | 653164   |
---------------------------------
Eval num_timesteps=660000, episode_reward=-46.05 +/- 35.00
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -46.1    |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 25.4     |
|    learning_rate   | 0.000347 |
|    n_updates       | 659167   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    episodes        | 332      |
|    fps             | 8        |
|    time_elapsed    | 77423    |
|    total timesteps | 663169   |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 22.4     |
|    learning_rate   | 0.000345 |
|    n_updates       | 661168   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 12       |
| time/              |          |
|    episodes        | 336      |
|    fps             | 8        |
|    time_elapsed    | 78321    |
|    total timesteps | 671173   |
| train/             |          |
|    actor_loss      | -0.993   |
|    critic_loss     | 24.9     |
|    learning_rate   | 0.000338 |
|    n_updates       | 669172   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 9.74     |
| time/              |          |
|    episodes        | 340      |
|    fps             | 8        |
|    time_elapsed    | 79219    |
|    total timesteps | 679177   |
| train/             |          |
|    actor_loss      | -1.15    |
|    critic_loss     | 22.7     |
|    learning_rate   | 0.00033  |
|    n_updates       | 677176   |
---------------------------------
Eval num_timesteps=680000, episode_reward=-25.21 +/- 26.06
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -25.2    |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -1.32    |
|    critic_loss     | 23.2     |
|    learning_rate   | 0.000328 |
|    n_updates       | 679177   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    episodes        | 344      |
|    fps             | 8        |
|    time_elapsed    | 80172    |
|    total timesteps | 686529   |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 23.6     |
|    learning_rate   | 0.000322 |
|    n_updates       | 684528   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 12.7     |
| time/              |          |
|    episodes        | 348      |
|    fps             | 8        |
|    time_elapsed    | 81031    |
|    total timesteps | 694533   |
| train/             |          |
|    actor_loss      | -1.62    |
|    critic_loss     | 23.3     |
|    learning_rate   | 0.000314 |
|    n_updates       | 692532   |
---------------------------------
Eval num_timesteps=700000, episode_reward=-47.45 +/- 37.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -47.5    |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -1.59    |
|    critic_loss     | 22.8     |
|    learning_rate   | 0.000308 |
|    n_updates       | 698535   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 10.6     |
| time/              |          |
|    episodes        | 352      |
|    fps             | 8        |
|    time_elapsed    | 82040    |
|    total timesteps | 702537   |
| train/             |          |
|    actor_loss      | -1.67    |
|    critic_loss     | 22.6     |
|    learning_rate   | 0.000306 |
|    n_updates       | 700536   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 12.9     |
| time/              |          |
|    episodes        | 356      |
|    fps             | 8        |
|    time_elapsed    | 82901    |
|    total timesteps | 710541   |
| train/             |          |
|    actor_loss      | -1.63    |
|    critic_loss     | 22.4     |
|    learning_rate   | 0.000299 |
|    n_updates       | 708540   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    episodes        | 360      |
|    fps             | 8        |
|    time_elapsed    | 83761    |
|    total timesteps | 718545   |
| train/             |          |
|    actor_loss      | -1.58    |
|    critic_loss     | 24.3     |
|    learning_rate   | 0.000291 |
|    n_updates       | 716544   |
---------------------------------
Eval num_timesteps=720000, episode_reward=-16.07 +/- 17.50
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -16.1    |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -1.68    |
|    critic_loss     | 23.2     |
|    learning_rate   | 0.000289 |
|    n_updates       | 718545   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 11.2     |
| time/              |          |
|    episodes        | 364      |
|    fps             | 8        |
|    time_elapsed    | 84789    |
|    total timesteps | 726549   |
| train/             |          |
|    actor_loss      | -1.81    |
|    critic_loss     | 22.8     |
|    learning_rate   | 0.000283 |
|    n_updates       | 724548   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 12.8     |
| time/              |          |
|    episodes        | 368      |
|    fps             | 8        |
|    time_elapsed    | 85687    |
|    total timesteps | 734553   |
| train/             |          |
|    actor_loss      | -1.69    |
|    critic_loss     | 23.4     |
|    learning_rate   | 0.000275 |
|    n_updates       | 732552   |
---------------------------------
Eval num_timesteps=740000, episode_reward=5.49 +/- 14.74
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 5.49     |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -1.58    |
|    critic_loss     | 24.3     |
|    learning_rate   | 0.000269 |
|    n_updates       | 738555   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    episodes        | 372      |
|    fps             | 8        |
|    time_elapsed    | 86717    |
|    total timesteps | 742385   |
| train/             |          |
|    actor_loss      | -1.68    |
|    critic_loss     | 22.7     |
|    learning_rate   | 0.000267 |
|    n_updates       | 740556   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 20.6     |
| time/              |          |
|    episodes        | 376      |
|    fps             | 8        |
|    time_elapsed    | 87595    |
|    total timesteps | 750389   |
| train/             |          |
|    actor_loss      | -1.85    |
|    critic_loss     | 24       |
|    learning_rate   | 0.000259 |
|    n_updates       | 748388   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    episodes        | 380      |
|    fps             | 8        |
|    time_elapsed    | 88495    |
|    total timesteps | 758393   |
| train/             |          |
|    actor_loss      | -2.02    |
|    critic_loss     | 23.3     |
|    learning_rate   | 0.000251 |
|    n_updates       | 756392   |
---------------------------------
Eval num_timesteps=760000, episode_reward=-31.60 +/- 24.57
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -31.6    |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -1.97    |
|    critic_loss     | 23.3     |
|    learning_rate   | 0.000249 |
|    n_updates       | 758393   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    episodes        | 384      |
|    fps             | 8        |
|    time_elapsed    | 89497    |
|    total timesteps | 766397   |
| train/             |          |
|    actor_loss      | -1.83    |
|    critic_loss     | 22.8     |
|    learning_rate   | 0.000243 |
|    n_updates       | 764396   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    episodes        | 388      |
|    fps             | 8        |
|    time_elapsed    | 90272    |
|    total timesteps | 773630   |
| train/             |          |
|    actor_loss      | -1.58    |
|    critic_loss     | 23       |
|    learning_rate   | 0.000236 |
|    n_updates       | 771629   |
---------------------------------
Eval num_timesteps=780000, episode_reward=-30.55 +/- 22.02
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -30.6    |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 22.7     |
|    learning_rate   | 0.000228 |
|    n_updates       | 779633   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    episodes        | 392      |
|    fps             | 8        |
|    time_elapsed    | 91261    |
|    total timesteps | 781634   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    episodes        | 396      |
|    fps             | 8        |
|    time_elapsed    | 92120    |
|    total timesteps | 789638   |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 23.2     |
|    learning_rate   | 0.00022  |
|    n_updates       | 787637   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    episodes        | 400      |
|    fps             | 8        |
|    time_elapsed    | 92990    |
|    total timesteps | 797642   |
| train/             |          |
|    actor_loss      | -1.3     |
|    critic_loss     | 23.4     |
|    learning_rate   | 0.000212 |
|    n_updates       | 795641   |
---------------------------------
Eval num_timesteps=800000, episode_reward=-42.26 +/- 26.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -42.3    |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -1.32    |
|    critic_loss     | 23.5     |
|    learning_rate   | 0.000208 |
|    n_updates       | 799643   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    episodes        | 404      |
|    fps             | 8        |
|    time_elapsed    | 93997    |
|    total timesteps | 805646   |
| train/             |          |
|    actor_loss      | -1.32    |
|    critic_loss     | 23.1     |
|    learning_rate   | 0.000204 |
|    n_updates       | 803645   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    episodes        | 408      |
|    fps             | 8        |
|    time_elapsed    | 94861    |
|    total timesteps | 813650   |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 22.9     |
|    learning_rate   | 0.000196 |
|    n_updates       | 811649   |
---------------------------------
Eval num_timesteps=820000, episode_reward=-0.78 +/- 15.56
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -0.784   |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -1.52    |
|    critic_loss     | 21.8     |
|    learning_rate   | 0.000189 |
|    n_updates       | 819653   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    episodes        | 412      |
|    fps             | 8        |
|    time_elapsed    | 95853    |
|    total timesteps | 821654   |
---------------------------------
