warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:99:4: 'mjtDisableBit' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:114:4: 'mjtEnableBit' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:121:4: 'mjtJoint' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:127:4: 'mjtGeom' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:149:4: 'mjtCamLight' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:157:4: 'mjtTexture' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:163:4: 'mjtIntegrator' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:168:4: 'mjtCollision' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:174:4: 'mjtCone' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:179:4: 'mjtJacobian' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:185:4: 'mjtSolver' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:191:4: 'mjtImp' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:198:4: 'mjtRef' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:203:4: 'mjtEq' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:211:4: 'mjtWrap' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:220:4: 'mjtTrn' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:230:4: 'mjtDyn' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:237:4: 'mjtGain' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:242:4: 'mjtBias' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:248:4: 'mjtObj' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:274:4: 'mjtConstraint' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:285:4: 'mjtConstraintState' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:294:4: 'mjtSensor' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:338:4: 'mjtStage' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:345:4: 'mjtDataType' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:99:4: 'mjtDisableBit' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:114:4: 'mjtEnableBit' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:121:4: 'mjtJoint' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:127:4: 'mjtGeom' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:149:4: 'mjtCamLight' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:157:4: 'mjtTexture' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:163:4: 'mjtIntegrator' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:168:4: 'mjtCollision' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:174:4: 'mjtCone' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:179:4: 'mjtJacobian' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:185:4: 'mjtSolver' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:191:4: 'mjtImp' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:198:4: 'mjtRef' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:203:4: 'mjtEq' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:211:4: 'mjtWrap' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:220:4: 'mjtTrn' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:230:4: 'mjtDyn' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:237:4: 'mjtGain' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:242:4: 'mjtBias' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:248:4: 'mjtObj' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:274:4: 'mjtConstraint' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:285:4: 'mjtConstraintState' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:294:4: 'mjtSensor' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:338:4: 'mjtStage' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:345:4: 'mjtDataType' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:75:4: 'mjtNum' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:77:10: 'mjPI' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:78:10: 'mjMAXVAL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:79:10: 'mjMINMU' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:80:10: 'mjMINIMP' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:81:10: 'mjMAXIMP' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:82:10: 'mjMAXCONPAIR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:83:10: 'mjMAXVFS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:84:10: 'mjMAXVFSNAME' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:87:10: 'mjNEQDATA' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:88:10: 'mjNDYN' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:89:10: 'mjNGAIN' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:90:10: 'mjNBIAS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:91:10: 'mjNREF' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:92:10: 'mjNIMP' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:93:10: 'mjNSOLVER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:96:4: 'mjtByte' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:100:8: 'mjDSBL_CONSTRAINT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:101:8: 'mjDSBL_EQUALITY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:102:8: 'mjDSBL_FRICTIONLOSS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:103:8: 'mjDSBL_LIMIT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:104:8: 'mjDSBL_CONTACT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:105:8: 'mjDSBL_PASSIVE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:106:8: 'mjDSBL_GRAVITY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:107:8: 'mjDSBL_CLAMPCTRL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:108:8: 'mjDSBL_WARMSTART' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:109:8: 'mjDSBL_FILTERPARENT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:110:8: 'mjDSBL_ACTUATION' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:111:8: 'mjDSBL_REFSAFE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:112:10: 'mjNDISABLE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:115:8: 'mjENBL_OVERRIDE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:116:8: 'mjENBL_ENERGY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:117:8: 'mjENBL_FWDINV' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:118:8: 'mjENBL_SENSORNOISE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:119:10: 'mjNENABLE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:122:8: 'mjJNT_FREE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:123:8: 'mjJNT_BALL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:124:8: 'mjJNT_SLIDE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:125:8: 'mjJNT_HINGE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:129:8: 'mjGEOM_PLANE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:130:8: 'mjGEOM_HFIELD' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:131:8: 'mjGEOM_SPHERE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:132:8: 'mjGEOM_CAPSULE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:133:8: 'mjGEOM_ELLIPSOID' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:134:8: 'mjGEOM_CYLINDER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:135:8: 'mjGEOM_BOX' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:136:8: 'mjGEOM_MESH' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:138:8: 'mjNGEOMTYPES' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:141:8: 'mjGEOM_ARROW' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:142:8: 'mjGEOM_ARROW1' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:143:8: 'mjGEOM_ARROW2' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:144:8: 'mjGEOM_LABEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:146:8: 'mjGEOM_NONE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:150:8: 'mjCAMLIGHT_FIXED' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:151:8: 'mjCAMLIGHT_TRACK' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:152:8: 'mjCAMLIGHT_TRACKCOM' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:153:8: 'mjCAMLIGHT_TARGETBODY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:154:8: 'mjCAMLIGHT_TARGETBODYCOM' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:158:8: 'mjTEXTURE_2D' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:159:8: 'mjTEXTURE_CUBE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:160:8: 'mjTEXTURE_SKYBOX' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:164:8: 'mjINT_EULER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:165:8: 'mjINT_RK4' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:169:8: 'mjCOL_ALL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:170:8: 'mjCOL_PAIR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:171:8: 'mjCOL_DYNAMIC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:175:8: 'mjCONE_PYRAMIDAL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:176:8: 'mjCONE_ELLIPTIC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:180:8: 'mjJAC_DENSE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:181:8: 'mjJAC_SPARSE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:182:8: 'mjJAC_AUTO' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:186:8: 'mjSOL_PGS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:187:8: 'mjSOL_CG' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:188:8: 'mjSOL_NEWTON' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:192:8: 'mjIMP_CONSTANT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:193:8: 'mjIMP_SIGMOID' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:194:8: 'mjIMP_LINEAR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:195:8: 'mjIMP_USER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:199:8: 'mjREF_SPRING' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:200:8: 'mjREF_USER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:204:8: 'mjEQ_CONNECT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:205:8: 'mjEQ_WELD' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:206:8: 'mjEQ_JOINT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:207:8: 'mjEQ_TENDON' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:208:8: 'mjEQ_DISTANCE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:212:8: 'mjWRAP_NONE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:213:8: 'mjWRAP_JOINT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:214:8: 'mjWRAP_PULLEY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:215:8: 'mjWRAP_SITE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:216:8: 'mjWRAP_SPHERE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:217:8: 'mjWRAP_CYLINDER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:221:8: 'mjTRN_JOINT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:222:8: 'mjTRN_JOINTINPARENT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:223:8: 'mjTRN_SLIDERCRANK' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:224:8: 'mjTRN_TENDON' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:225:8: 'mjTRN_SITE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:227:8: 'mjTRN_UNDEFINED' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:231:8: 'mjDYN_NONE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:232:8: 'mjDYN_INTEGRATOR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:233:8: 'mjDYN_FILTER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:234:8: 'mjDYN_USER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:238:8: 'mjGAIN_FIXED' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:239:8: 'mjGAIN_USER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:243:8: 'mjBIAS_NONE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:244:8: 'mjBIAS_AFFINE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:245:8: 'mjBIAS_USER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:249:8: 'mjOBJ_UNKNOWN' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:250:8: 'mjOBJ_BODY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:251:8: 'mjOBJ_XBODY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:252:8: 'mjOBJ_JOINT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:253:8: 'mjOBJ_DOF' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:254:8: 'mjOBJ_GEOM' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:255:8: 'mjOBJ_SITE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:256:8: 'mjOBJ_CAMERA' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:257:8: 'mjOBJ_LIGHT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:258:8: 'mjOBJ_MESH' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:259:8: 'mjOBJ_HFIELD' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:260:8: 'mjOBJ_TEXTURE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:261:8: 'mjOBJ_MATERIAL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:262:8: 'mjOBJ_PAIR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:263:8: 'mjOBJ_EXCLUDE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:264:8: 'mjOBJ_EQUALITY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:265:8: 'mjOBJ_TENDON' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:266:8: 'mjOBJ_ACTUATOR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:267:8: 'mjOBJ_SENSOR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:268:8: 'mjOBJ_NUMERIC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:269:8: 'mjOBJ_TEXT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:270:8: 'mjOBJ_TUPLE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:271:8: 'mjOBJ_KEY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:275:8: 'mjCNSTR_EQUALITY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:276:8: 'mjCNSTR_FRICTION_DOF' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:277:8: 'mjCNSTR_FRICTION_TENDON' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:278:8: 'mjCNSTR_LIMIT_JOINT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:279:8: 'mjCNSTR_LIMIT_TENDON' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:280:8: 'mjCNSTR_CONTACT_FRICTIONLESS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:281:8: 'mjCNSTR_CONTACT_PYRAMIDAL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:282:8: 'mjCNSTR_CONTACT_ELLIPTIC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:286:8: 'mjCNSTRSTATE_SATISFIED' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:287:8: 'mjCNSTRSTATE_QUADRATIC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:288:8: 'mjCNSTRSTATE_LINEARNEG' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:289:8: 'mjCNSTRSTATE_LINEARPOS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:290:8: 'mjCNSTRSTATE_CONE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:296:8: 'mjSENS_TOUCH' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:297:8: 'mjSENS_ACCELEROMETER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:298:8: 'mjSENS_VELOCIMETER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:299:8: 'mjSENS_GYRO' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:300:8: 'mjSENS_FORCE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:301:8: 'mjSENS_TORQUE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:302:8: 'mjSENS_MAGNETOMETER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:303:8: 'mjSENS_RANGEFINDER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:306:8: 'mjSENS_JOINTPOS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:307:8: 'mjSENS_JOINTVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:308:8: 'mjSENS_TENDONPOS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:309:8: 'mjSENS_TENDONVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:310:8: 'mjSENS_ACTUATORPOS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:311:8: 'mjSENS_ACTUATORVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:312:8: 'mjSENS_ACTUATORFRC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:315:8: 'mjSENS_BALLQUAT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:316:8: 'mjSENS_BALLANGVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:319:8: 'mjSENS_FRAMEPOS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:320:8: 'mjSENS_FRAMEQUAT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:321:8: 'mjSENS_FRAMEXAXIS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:322:8: 'mjSENS_FRAMEYAXIS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:323:8: 'mjSENS_FRAMEZAXIS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:324:8: 'mjSENS_FRAMELINVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:325:8: 'mjSENS_FRAMEANGVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:326:8: 'mjSENS_FRAMELINACC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:327:8: 'mjSENS_FRAMEANGACC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:330:8: 'mjSENS_SUBTREECOM' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:331:8: 'mjSENS_SUBTREELINVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:332:8: 'mjSENS_SUBTREEANGMOM' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:335:8: 'mjSENS_USER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:339:8: 'mjSTAGE_NONE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:340:8: 'mjSTAGE_POS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:341:8: 'mjSTAGE_VEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:342:8: 'mjSTAGE_ACC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:346:8: 'mjDATATYPE_REAL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:347:8: 'mjDATATYPE_POSITIVE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:348:8: 'mjDATATYPE_AXIS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:349:8: 'mjDATATYPE_QUAT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:75:4: 'mjtNum' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:77:10: 'mjPI' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:78:10: 'mjMAXVAL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:79:10: 'mjMINMU' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:80:10: 'mjMINIMP' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:81:10: 'mjMAXIMP' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:82:10: 'mjMAXCONPAIR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:83:10: 'mjMAXVFS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:84:10: 'mjMAXVFSNAME' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:87:10: 'mjNEQDATA' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:88:10: 'mjNDYN' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:89:10: 'mjNGAIN' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:90:10: 'mjNBIAS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:91:10: 'mjNREF' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:92:10: 'mjNIMP' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:93:10: 'mjNSOLVER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:96:4: 'mjtByte' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:100:8: 'mjDSBL_CONSTRAINT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:101:8: 'mjDSBL_EQUALITY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:102:8: 'mjDSBL_FRICTIONLOSS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:103:8: 'mjDSBL_LIMIT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:104:8: 'mjDSBL_CONTACT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:105:8: 'mjDSBL_PASSIVE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:106:8: 'mjDSBL_GRAVITY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:107:8: 'mjDSBL_CLAMPCTRL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:108:8: 'mjDSBL_WARMSTART' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:109:8: 'mjDSBL_FILTERPARENT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:110:8: 'mjDSBL_ACTUATION' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:111:8: 'mjDSBL_REFSAFE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:112:10: 'mjNDISABLE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:115:8: 'mjENBL_OVERRIDE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:116:8: 'mjENBL_ENERGY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:117:8: 'mjENBL_FWDINV' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:118:8: 'mjENBL_SENSORNOISE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:119:10: 'mjNENABLE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:122:8: 'mjJNT_FREE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:123:8: 'mjJNT_BALL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:124:8: 'mjJNT_SLIDE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:125:8: 'mjJNT_HINGE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:129:8: 'mjGEOM_PLANE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:130:8: 'mjGEOM_HFIELD' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:131:8: 'mjGEOM_SPHERE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:132:8: 'mjGEOM_CAPSULE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:133:8: 'mjGEOM_ELLIPSOID' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:134:8: 'mjGEOM_CYLINDER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:135:8: 'mjGEOM_BOX' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:136:8: 'mjGEOM_MESH' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:138:8: 'mjNGEOMTYPES' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:141:8: 'mjGEOM_ARROW' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:142:8: 'mjGEOM_ARROW1' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:143:8: 'mjGEOM_ARROW2' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:144:8: 'mjGEOM_LABEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:146:8: 'mjGEOM_NONE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:150:8: 'mjCAMLIGHT_FIXED' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:151:8: 'mjCAMLIGHT_TRACK' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:152:8: 'mjCAMLIGHT_TRACKCOM' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:153:8: 'mjCAMLIGHT_TARGETBODY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:154:8: 'mjCAMLIGHT_TARGETBODYCOM' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:158:8: 'mjTEXTURE_2D' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:159:8: 'mjTEXTURE_CUBE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:160:8: 'mjTEXTURE_SKYBOX' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:164:8: 'mjINT_EULER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:165:8: 'mjINT_RK4' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:169:8: 'mjCOL_ALL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:170:8: 'mjCOL_PAIR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:171:8: 'mjCOL_DYNAMIC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:175:8: 'mjCONE_PYRAMIDAL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:176:8: 'mjCONE_ELLIPTIC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:180:8: 'mjJAC_DENSE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:181:8: 'mjJAC_SPARSE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:182:8: 'mjJAC_AUTO' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:186:8: 'mjSOL_PGS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:187:8: 'mjSOL_CG' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:188:8: 'mjSOL_NEWTON' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:192:8: 'mjIMP_CONSTANT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:193:8: 'mjIMP_SIGMOID' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:194:8: 'mjIMP_LINEAR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:195:8: 'mjIMP_USER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:199:8: 'mjREF_SPRING' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:200:8: 'mjREF_USER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:204:8: 'mjEQ_CONNECT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:205:8: 'mjEQ_WELD' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:206:8: 'mjEQ_JOINT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:207:8: 'mjEQ_TENDON' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:208:8: 'mjEQ_DISTANCE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:212:8: 'mjWRAP_NONE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:213:8: 'mjWRAP_JOINT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:214:8: 'mjWRAP_PULLEY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:215:8: 'mjWRAP_SITE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:216:8: 'mjWRAP_SPHERE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:217:8: 'mjWRAP_CYLINDER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:221:8: 'mjTRN_JOINT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:222:8: 'mjTRN_JOINTINPARENT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:223:8: 'mjTRN_SLIDERCRANK' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:224:8: 'mjTRN_TENDON' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:225:8: 'mjTRN_SITE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:227:8: 'mjTRN_UNDEFINED' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:231:8: 'mjDYN_NONE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:232:8: 'mjDYN_INTEGRATOR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:233:8: 'mjDYN_FILTER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:234:8: 'mjDYN_USER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:238:8: 'mjGAIN_FIXED' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:239:8: 'mjGAIN_USER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:243:8: 'mjBIAS_NONE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:244:8: 'mjBIAS_AFFINE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:245:8: 'mjBIAS_USER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:249:8: 'mjOBJ_UNKNOWN' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:250:8: 'mjOBJ_BODY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:251:8: 'mjOBJ_XBODY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:252:8: 'mjOBJ_JOINT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:253:8: 'mjOBJ_DOF' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:254:8: 'mjOBJ_GEOM' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:255:8: 'mjOBJ_SITE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:256:8: 'mjOBJ_CAMERA' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:257:8: 'mjOBJ_LIGHT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:258:8: 'mjOBJ_MESH' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:259:8: 'mjOBJ_HFIELD' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:260:8: 'mjOBJ_TEXTURE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:261:8: 'mjOBJ_MATERIAL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:262:8: 'mjOBJ_PAIR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:263:8: 'mjOBJ_EXCLUDE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:264:8: 'mjOBJ_EQUALITY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:265:8: 'mjOBJ_TENDON' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:266:8: 'mjOBJ_ACTUATOR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:267:8: 'mjOBJ_SENSOR' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:268:8: 'mjOBJ_NUMERIC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:269:8: 'mjOBJ_TEXT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:270:8: 'mjOBJ_TUPLE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:271:8: 'mjOBJ_KEY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:275:8: 'mjCNSTR_EQUALITY' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:276:8: 'mjCNSTR_FRICTION_DOF' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:277:8: 'mjCNSTR_FRICTION_TENDON' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:278:8: 'mjCNSTR_LIMIT_JOINT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:279:8: 'mjCNSTR_LIMIT_TENDON' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:280:8: 'mjCNSTR_CONTACT_FRICTIONLESS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:281:8: 'mjCNSTR_CONTACT_PYRAMIDAL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:282:8: 'mjCNSTR_CONTACT_ELLIPTIC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:286:8: 'mjCNSTRSTATE_SATISFIED' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:287:8: 'mjCNSTRSTATE_QUADRATIC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:288:8: 'mjCNSTRSTATE_LINEARNEG' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:289:8: 'mjCNSTRSTATE_LINEARPOS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:290:8: 'mjCNSTRSTATE_CONE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:296:8: 'mjSENS_TOUCH' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:297:8: 'mjSENS_ACCELEROMETER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:298:8: 'mjSENS_VELOCIMETER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:299:8: 'mjSENS_GYRO' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:300:8: 'mjSENS_FORCE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:301:8: 'mjSENS_TORQUE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:302:8: 'mjSENS_MAGNETOMETER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:303:8: 'mjSENS_RANGEFINDER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:306:8: 'mjSENS_JOINTPOS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:307:8: 'mjSENS_JOINTVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:308:8: 'mjSENS_TENDONPOS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:309:8: 'mjSENS_TENDONVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:310:8: 'mjSENS_ACTUATORPOS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:311:8: 'mjSENS_ACTUATORVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:312:8: 'mjSENS_ACTUATORFRC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:315:8: 'mjSENS_BALLQUAT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:316:8: 'mjSENS_BALLANGVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:319:8: 'mjSENS_FRAMEPOS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:320:8: 'mjSENS_FRAMEQUAT' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:321:8: 'mjSENS_FRAMEXAXIS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:322:8: 'mjSENS_FRAMEYAXIS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:323:8: 'mjSENS_FRAMEZAXIS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:324:8: 'mjSENS_FRAMELINVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:325:8: 'mjSENS_FRAMEANGVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:326:8: 'mjSENS_FRAMELINACC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:327:8: 'mjSENS_FRAMEANGACC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:330:8: 'mjSENS_SUBTREECOM' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:331:8: 'mjSENS_SUBTREELINVEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:332:8: 'mjSENS_SUBTREEANGMOM' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:335:8: 'mjSENS_USER' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:339:8: 'mjSTAGE_NONE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:340:8: 'mjSTAGE_POS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:341:8: 'mjSTAGE_VEL' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:342:8: 'mjSTAGE_ACC' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:346:8: 'mjDATATYPE_REAL' redeclared 
Compiling /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.pyx because it depends on /usr/local/lib/python3.6/site-packages/mujoco_py/pxd/mujoco.pxd.
[1/1] Cythonizing /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.pyx
running build_ext
building 'mujoco_py.cymj' extension
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o -fopenmp -w
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py
gcc -pthread -shared /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mjpro150/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mjpro150/bin -lmujoco150 -lglewegl -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:347:8: 'mjDATATYPE_POSITIVE' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:348:8: 'mjDATATYPE_AXIS' redeclared 
warning: /usr/local/lib/python3.6/site-packages/mujoco_py/generated/../pxd/mjmodel.pxd:349:8: 'mjDATATYPE_QUAT' redeclared 
Downloading: "https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth
/root/trainer
Using cuda device
  0%|          | 0.00/9.83M [00:00<?, ?B/s]  0%|          | 8.00k/9.83M [00:00<03:42, 46.3kB/s]  0%|          | 32.0k/9.83M [00:00<01:41, 101kB/s]   1%|          | 88.0k/9.83M [00:00<00:51, 199kB/s]  2%|         | 208k/9.83M [00:00<00:26, 387kB/s]   4%|         | 424k/9.83M [00:00<00:14, 688kB/s]  9%|         | 872k/9.83M [00:01<00:07, 1.30MB/s] 18%|        | 1.73M/9.83M [00:01<00:03, 2.42MB/s] 35%|      | 3.46M/9.83M [00:01<00:01, 4.82MB/s] 61%|    | 5.99M/9.83M [00:01<00:00, 8.04MB/s] 87%| | 8.51M/9.83M [00:01<00:00, 9.67MB/s]100%|| 9.83M/9.83M [00:01<00:00, 5.48MB/s]2021-11-30 16:18:25.380042: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-11-30 16:18:25.384434: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Logging to assets/out/models/exp6/TD3_16
Found 1 GPUs for rendering. Using device 0.
Terminated
2021-11-30 17:00:43.123504: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-11-30 17:00:43.123579: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp6/TD3_21
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -57.3    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 3        |
|    time_elapsed     | 1534     |
|    total timesteps  | 6004     |
| train/              |          |
|    actor_loss       | 0.213    |
|    critic_loss      | 63       |
|    learning_rate    | 0.0007   |
|    n_updates        | 1835     |
|    total_actor_loss | 0.968    |
|    value_loss       | 0.755    |
----------------------------------
Eval num_timesteps=10000, episode_reward=-46.38 +/- 105.30
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -46.4    |
| time/               |          |
|    total_timesteps  | 10000    |
| train/              |          |
|    actor_loss       | -0.0087  |
|    critic_loss      | 0.658    |
|    learning_rate    | 0.0007   |
|    n_updates        | 3165     |
|    total_actor_loss | 0.463    |
|    value_loss       | 0.471    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -65.5    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 3        |
|    time_elapsed     | 3725     |
|    total timesteps  | 12008    |
| train/              |          |
|    actor_loss       | 0.0755   |
|    critic_loss      | 1.66     |
|    learning_rate    | 0.0007   |
|    n_updates        | 3835     |
|    total_actor_loss | 1.33     |
|    value_loss       | 1.25     |
----------------------------------
[W python_anomaly_mode.cpp:104] Warning: Error detected in ExpBackward. Traceback of forward call that caused the error:
  File "train.py", line 39, in <module>
    model.learn(args.timesteps)
  File "/root/trainer/learning/explore.py", line 78, in learn
    callback = self.rl_callback
  File "/root/trainer/utils/td3_utils.py", line 507, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 369, in learn
    self.train(batch_size=self.batch_size, gradient_steps=gradient_steps)
  File "/root/trainer/utils/td3_utils.py", line 461, in train
    actions, vt = self.actor(replay_data.observations)
  File "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/trainer/utils/td3_utils.py", line 90, in forward
    return self.mu(features)
  File "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/trainer/bg/models.py", line 238, in forward
    bg_out, vt  = self.bg([stimulus_t, stimulus_t_1])
  File "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/trainer/bg/models.py", line 85, in forward
    lamd1 = 1 / (1 + torch.exp(-self.log_a1.exp() * (deltavf - self.thetad1)))
 (function _print_stack)
Traceback (most recent call last):
  File "train.py", line 39, in <module>
    model.learn(args.timesteps)
  File "/root/trainer/learning/explore.py", line 78, in learn
    callback = self.rl_callback
  File "/root/trainer/utils/td3_utils.py", line 507, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 369, in learn
    self.train(batch_size=self.batch_size, gradient_steps=gradient_steps)
  File "/root/trainer/utils/td3_utils.py", line 470, in train
    loss.backward()
  File "/usr/local/lib/python3.6/site-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 149, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
RuntimeError: Function 'ExpBackward' returned nan values in its 0th output.
2021-11-30 19:28:40.888908: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-11-30 19:28:40.888982: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp6/TD3_22
Found 1 GPUs for rendering. Using device 0.
Terminated
2021-11-30 19:46:22.502750: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-11-30 19:46:22.502801: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp6/TD3_23
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -1.53    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 3        |
|    time_elapsed     | 1533     |
|    total timesteps  | 6004     |
| train/              |          |
|    actor_loss       | 0.1      |
|    critic_loss      | 1.33     |
|    learning_rate    | 0.0007   |
|    n_updates        | 1835     |
|    total_actor_loss | 0.216    |
|    value_loss       | 0.116    |
----------------------------------
Eval num_timesteps=9000, episode_reward=-26.24 +/- 17.30
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -26.2    |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | 0.122    |
|    critic_loss      | 0.09     |
|    learning_rate    | 0.0007   |
|    n_updates        | 2830     |
|    total_actor_loss | 0.2      |
|    value_loss       | 0.078    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 19.5     |
| time/               |          |
|    episodes         | 8        |
|    fps              | 3        |
|    time_elapsed     | 3692     |
|    total timesteps  | 12008    |
| train/              |          |
|    actor_loss       | 0.057    |
|    critic_loss      | 0.116    |
|    learning_rate    | 0.0007   |
|    n_updates        | 3835     |
|    total_actor_loss | 0.0993   |
|    value_loss       | 0.0424   |
----------------------------------
Eval num_timesteps=18000, episode_reward=-29.74 +/- 7.66
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -29.7    |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | -0.0846  |
|    critic_loss      | 0.483    |
|    learning_rate    | 0.0007   |
|    n_updates        | 5830     |
|    total_actor_loss | 0.193    |
|    value_loss       | 0.278    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 34       |
| time/               |          |
|    episodes         | 12       |
|    fps              | 3        |
|    time_elapsed     | 5871     |
|    total timesteps  | 18012    |
| train/              |          |
|    actor_loss       | -0.08    |
|    critic_loss      | 0.988    |
|    learning_rate    | 0.0007   |
|    n_updates        | 5835     |
|    total_actor_loss | 0.362    |
|    value_loss       | 0.442    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 33.7     |
| time/               |          |
|    episodes         | 16       |
|    fps              | 3        |
|    time_elapsed     | 7528     |
|    total timesteps  | 24016    |
| train/              |          |
|    actor_loss       | -0.195   |
|    critic_loss      | 0.682    |
|    learning_rate    | 0.0007   |
|    n_updates        | 7840     |
|    total_actor_loss | 0.0702   |
|    value_loss       | 0.265    |
----------------------------------
Eval num_timesteps=27000, episode_reward=59.64 +/- 32.30
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 59.6     |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | -0.166   |
|    critic_loss      | 0.262    |
|    learning_rate    | 0.0007   |
|    n_updates        | 8830     |
|    total_actor_loss | -0.011   |
|    value_loss       | 0.155    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 33.3     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 3        |
|    time_elapsed     | 9697     |
|    total timesteps  | 30020    |
| train/              |          |
|    actor_loss       | -0.127   |
|    critic_loss      | 0.294    |
|    learning_rate    | 0.0007   |
|    n_updates        | 9840     |
|    total_actor_loss | 0.0436   |
|    value_loss       | 0.171    |
----------------------------------
Eval num_timesteps=36000, episode_reward=5.38 +/- 41.49
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 5.38     |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | -0.251   |
|    critic_loss      | 0.219    |
|    learning_rate    | 0.0007   |
|    n_updates        | 11830    |
|    total_actor_loss | -0.188   |
|    value_loss       | 0.0632   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 31.2     |
| time/               |          |
|    episodes         | 24       |
|    fps              | 3        |
|    time_elapsed     | 11849    |
|    total timesteps  | 36024    |
| train/              |          |
|    actor_loss       | -0.283   |
|    critic_loss      | 0.205    |
|    learning_rate    | 0.0007   |
|    n_updates        | 11840    |
|    total_actor_loss | -0.121   |
|    value_loss       | 0.162    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 33.2     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 3        |
|    time_elapsed     | 13506    |
|    total timesteps  | 42028    |
| train/              |          |
|    actor_loss       | -0.312   |
|    critic_loss      | 0.946    |
|    learning_rate    | 0.0007   |
|    n_updates        | 13840    |
|    total_actor_loss | 0.507    |
|    value_loss       | 0.819    |
----------------------------------
Eval num_timesteps=45000, episode_reward=17.08 +/- 36.29
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 17.1     |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | -0.382   |
|    critic_loss      | 0.881    |
|    learning_rate    | 0.0007   |
|    n_updates        | 14830    |
|    total_actor_loss | 0.455    |
|    value_loss       | 0.838    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 31.3     |
| time/               |          |
|    episodes         | 32       |
|    fps              | 3        |
|    time_elapsed     | 15615    |
|    total timesteps  | 47873    |
| train/              |          |
|    actor_loss       | -0.376   |
|    critic_loss      | 0.181    |
|    learning_rate    | 0.0007   |
|    n_updates        | 15790    |
|    total_actor_loss | -0.222   |
|    value_loss       | 0.154    |
----------------------------------
Terminated
2021-12-01 00:35:38.388154: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-01 00:35:38.388222: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp6/TD3_24
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -3.75    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 3        |
|    time_elapsed     | 1526     |
|    total timesteps  | 6004     |
| train/              |          |
|    actor_loss       | 0.0481   |
|    critic_loss      | 0.00151  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1835     |
|    total_actor_loss | 0.0509   |
|    value_loss       | 0.00286  |
----------------------------------
Eval num_timesteps=9000, episode_reward=-2.06 +/- 8.45
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -2.06    |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | 0.0628   |
|    critic_loss      | 0.00363  |
|    learning_rate    | 0.0007   |
|    n_updates        | 2830     |
|    total_actor_loss | 0.0672   |
|    value_loss       | 0.00433  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -8.09    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 3        |
|    time_elapsed     | 3708     |
|    total timesteps  | 12008    |
| train/              |          |
|    actor_loss       | 0.0864   |
|    critic_loss      | 0.0051   |
|    learning_rate    | 0.0007   |
|    n_updates        | 3835     |
|    total_actor_loss | 0.0905   |
|    value_loss       | 0.00407  |
----------------------------------
Terminated
2021-12-01 01:58:12.170856: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-01 01:58:12.170927: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp6/TD3_25
Found 1 GPUs for rendering. Using device 0.
Terminated
2021-12-01 02:44:09.178246: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-01 02:44:09.178320: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp6/TD3_26
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -6.4     |
| time/               |          |
|    episodes         | 4        |
|    fps              | 5        |
|    time_elapsed     | 1092     |
|    total timesteps  | 6004     |
| train/              |          |
|    actor_loss       | 0.0188   |
|    critic_loss      | 0.00511  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1101     |
|    total_actor_loss | 0.0224   |
|    value_loss       | 0.00355  |
----------------------------------
Eval num_timesteps=9000, episode_reward=1.86 +/- 3.56
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 1.86     |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | 0.0229   |
|    critic_loss      | 0.00657  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1698     |
|    total_actor_loss | 0.0347   |
|    value_loss       | 0.0119   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -8.95    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4        |
|    time_elapsed     | 2776     |
|    total timesteps  | 12008    |
| train/              |          |
|    actor_loss       | 0.0464   |
|    critic_loss      | 0.0108   |
|    learning_rate    | 0.0007   |
|    n_updates        | 2301     |
|    total_actor_loss | 0.0613   |
|    value_loss       | 0.0148   |
----------------------------------
Eval num_timesteps=18000, episode_reward=-5.12 +/- 10.32
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -5.12    |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | 0.0932   |
|    critic_loss      | 0.0163   |
|    learning_rate    | 0.0007   |
|    n_updates        | 3498     |
|    total_actor_loss | 0.119    |
|    value_loss       | 0.0257   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -9.5     |
| time/               |          |
|    episodes         | 12       |
|    fps              | 4        |
|    time_elapsed     | 4462     |
|    total timesteps  | 18012    |
| train/              |          |
|    actor_loss       | 0.0912   |
|    critic_loss      | 0.0115   |
|    learning_rate    | 0.0007   |
|    n_updates        | 3501     |
|    total_actor_loss | 0.102    |
|    value_loss       | 0.0103   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -10.2    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 4        |
|    time_elapsed     | 5624     |
|    total timesteps  | 24016    |
| train/              |          |
|    actor_loss       | 0.0952   |
|    critic_loss      | 0.0154   |
|    learning_rate    | 0.0007   |
|    n_updates        | 4704     |
|    total_actor_loss | 0.118    |
|    value_loss       | 0.0224   |
----------------------------------
Eval num_timesteps=27000, episode_reward=-10.78 +/- 8.32
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -10.8    |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | 0.0844   |
|    critic_loss      | 0.017    |
|    learning_rate    | 0.0007   |
|    n_updates        | 5298     |
|    total_actor_loss | 0.111    |
|    value_loss       | 0.0267   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -10.2    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 4        |
|    time_elapsed     | 7304     |
|    total timesteps  | 30020    |
| train/              |          |
|    actor_loss       | 0.106    |
|    critic_loss      | 0.0275   |
|    learning_rate    | 0.0007   |
|    n_updates        | 5904     |
|    total_actor_loss | 0.129    |
|    value_loss       | 0.0227   |
----------------------------------
Eval num_timesteps=36000, episode_reward=-6.76 +/- 6.95
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -6.76    |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | 0.0839   |
|    critic_loss      | 0.0124   |
|    learning_rate    | 0.0007   |
|    n_updates        | 7098     |
|    total_actor_loss | 0.0934   |
|    value_loss       | 0.00945  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -8.44    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 4        |
|    time_elapsed     | 8973     |
|    total timesteps  | 36024    |
| train/              |          |
|    actor_loss       | 0.109    |
|    critic_loss      | 0.0101   |
|    learning_rate    | 0.0007   |
|    n_updates        | 7104     |
|    total_actor_loss | 0.122    |
|    value_loss       | 0.0126   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -8.46    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 4        |
|    time_elapsed     | 10133    |
|    total timesteps  | 42028    |
| train/              |          |
|    actor_loss       | 0.107    |
|    critic_loss      | 0.016    |
|    learning_rate    | 0.0007   |
|    n_updates        | 8304     |
|    total_actor_loss | 0.118    |
|    value_loss       | 0.0113   |
----------------------------------
Eval num_timesteps=45000, episode_reward=-11.94 +/- 4.86
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -11.9    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | 0.121    |
|    critic_loss      | 0.0237   |
|    learning_rate    | 0.0007   |
|    n_updates        | 8898     |
|    total_actor_loss | 0.136    |
|    value_loss       | 0.015    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -8.44    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 4        |
|    time_elapsed     | 11823    |
|    total timesteps  | 48032    |
| train/              |          |
|    actor_loss       | 0.11     |
|    critic_loss      | 0.0103   |
|    learning_rate    | 0.0007   |
|    n_updates        | 9507     |
|    total_actor_loss | 0.118    |
|    value_loss       | 0.00818  |
----------------------------------
Eval num_timesteps=54000, episode_reward=-6.26 +/- 4.57
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -6.26    |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | 0.147    |
|    critic_loss      | 0.00777  |
|    learning_rate    | 0.0007   |
|    n_updates        | 10698    |
|    total_actor_loss | 0.152    |
|    value_loss       | 0.00483  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -8.74    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 4        |
|    time_elapsed     | 13496    |
|    total timesteps  | 54036    |
| train/              |          |
|    actor_loss       | 0.143    |
|    critic_loss      | 0.00618  |
|    learning_rate    | 0.0007   |
|    n_updates        | 10707    |
|    total_actor_loss | 0.145    |
|    value_loss       | 0.00274  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -8.24    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 4        |
|    time_elapsed     | 14666    |
|    total timesteps  | 60040    |
| train/              |          |
|    actor_loss       | 0.158    |
|    critic_loss      | 0.00707  |
|    learning_rate    | 0.0007   |
|    n_updates        | 11907    |
|    total_actor_loss | 0.164    |
|    value_loss       | 0.00669  |
----------------------------------
Eval num_timesteps=63000, episode_reward=-2.62 +/- 7.57
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -2.62    |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    actor_loss       | 0.126    |
|    critic_loss      | 0.0104   |
|    learning_rate    | 0.0007   |
|    n_updates        | 12498    |
|    total_actor_loss | 0.132    |
|    value_loss       | 0.00616  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -7.55    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 4        |
|    time_elapsed     | 16363    |
|    total timesteps  | 66044    |
| train/              |          |
|    actor_loss       | 0.122    |
|    critic_loss      | 0.00867  |
|    learning_rate    | 0.0007   |
|    n_updates        | 13107    |
|    total_actor_loss | 0.127    |
|    value_loss       | 0.00481  |
----------------------------------
Eval num_timesteps=72000, episode_reward=-4.84 +/- 10.03
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -4.84    |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    actor_loss       | 0.145    |
|    critic_loss      | 0.00567  |
|    learning_rate    | 0.0007   |
|    n_updates        | 14298    |
|    total_actor_loss | 0.149    |
|    value_loss       | 0.00407  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -7.72    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 3        |
|    time_elapsed     | 18043    |
|    total timesteps  | 72048    |
| train/              |          |
|    actor_loss       | 0.148    |
|    critic_loss      | 0.00739  |
|    learning_rate    | 0.0007   |
|    n_updates        | 14310    |
|    total_actor_loss | 0.154    |
|    value_loss       | 0.00579  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -7.8     |
| time/               |          |
|    episodes         | 52       |
|    fps              | 4        |
|    time_elapsed     | 19212    |
|    total timesteps  | 78052    |
| train/              |          |
|    actor_loss       | 0.171    |
|    critic_loss      | 0.00761  |
|    learning_rate    | 0.0007   |
|    n_updates        | 15510    |
|    total_actor_loss | 0.177    |
|    value_loss       | 0.00583  |
----------------------------------
Eval num_timesteps=81000, episode_reward=-14.90 +/- 6.47
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -14.9    |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    actor_loss       | 0.175    |
|    critic_loss      | 0.0109   |
|    learning_rate    | 0.0007   |
|    n_updates        | 16098    |
|    total_actor_loss | 0.181    |
|    value_loss       | 0.00585  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -7.41    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 4        |
|    time_elapsed     | 20901    |
|    total timesteps  | 84056    |
| train/              |          |
|    actor_loss       | 0.155    |
|    critic_loss      | 0.00669  |
|    learning_rate    | 0.0007   |
|    n_updates        | 16710    |
|    total_actor_loss | 0.158    |
|    value_loss       | 0.00278  |
----------------------------------
Eval num_timesteps=90000, episode_reward=-12.40 +/- 16.40
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -12.4    |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    actor_loss       | 0.167    |
|    critic_loss      | 0.00749  |
|    learning_rate    | 0.0007   |
|    n_updates        | 17898    |
|    total_actor_loss | 0.17     |
|    value_loss       | 0.00345  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -7.52    |
| time/               |          |
|    episodes         | 60       |
|    fps              | 3        |
|    time_elapsed     | 22598    |
|    total timesteps  | 90060    |
| train/              |          |
|    actor_loss       | 0.174    |
|    critic_loss      | 0.00915  |
|    learning_rate    | 0.0007   |
|    n_updates        | 17910    |
|    total_actor_loss | 0.18     |
|    value_loss       | 0.00536  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -7.23    |
| time/               |          |
|    episodes         | 64       |
|    fps              | 4        |
|    time_elapsed     | 23737    |
|    total timesteps  | 95981    |
| train/              |          |
|    actor_loss       | 0.168    |
|    critic_loss      | 0.00948  |
|    learning_rate    | 0.0007   |
|    n_updates        | 19095    |
|    total_actor_loss | 0.174    |
|    value_loss       | 0.00565  |
----------------------------------
Eval num_timesteps=99000, episode_reward=-1.56 +/- 2.50
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -1.56    |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    actor_loss       | 0.194    |
|    critic_loss      | 0.00598  |
|    learning_rate    | 0.0007   |
|    n_updates        | 19698    |
|    total_actor_loss | 0.198    |
|    value_loss       | 0.00396  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -7.03    |
| time/               |          |
|    episodes         | 68       |
|    fps              | 4        |
|    time_elapsed     | 25423    |
|    total timesteps  | 101985   |
| train/              |          |
|    actor_loss       | 0.202    |
|    critic_loss      | 0.00698  |
|    learning_rate    | 0.0007   |
|    n_updates        | 20295    |
|    total_actor_loss | 0.206    |
|    value_loss       | 0.00427  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -7.43    |
| time/               |          |
|    episodes         | 72       |
|    fps              | 4        |
|    time_elapsed     | 26589    |
|    total timesteps  | 107989   |
| train/              |          |
|    actor_loss       | 0.218    |
|    critic_loss      | 0.00743  |
|    learning_rate    | 0.0007   |
|    n_updates        | 21498    |
|    total_actor_loss | 0.222    |
|    value_loss       | 0.00361  |
----------------------------------
Eval num_timesteps=108000, episode_reward=-8.44 +/- 7.00
Episode length: 1501.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.5e+03  |
|    mean_reward     | -8.44    |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -7.19    |
| time/               |          |
|    episodes         | 76       |
|    fps              | 4        |
|    time_elapsed     | 28278    |
|    total timesteps  | 113993   |
| train/              |          |
|    actor_loss       | 0.229    |
|    critic_loss      | 0.00553  |
|    learning_rate    | 0.0007   |
|    n_updates        | 22698    |
|    total_actor_loss | 0.235    |
|    value_loss       | 0.00521  |
----------------------------------
Eval num_timesteps=117000, episode_reward=-11.26 +/- 10.03
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -11.3    |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    actor_loss       | 0.223    |
|    critic_loss      | 0.00538  |
|    learning_rate    | 0.0007   |
|    n_updates        | 23298    |
|    total_actor_loss | 0.227    |
|    value_loss       | 0.00482  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -7.26    |
| time/               |          |
|    episodes         | 80       |
|    fps              | 4        |
|    time_elapsed     | 29977    |
|    total timesteps  | 119997   |
| train/              |          |
|    actor_loss       | 0.234    |
|    critic_loss      | 0.00678  |
|    learning_rate    | 0.0007   |
|    n_updates        | 23898    |
|    total_actor_loss | 0.238    |
|    value_loss       | 0.00374  |
----------------------------------
Eval num_timesteps=126000, episode_reward=-7.64 +/- 7.82
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -7.64    |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    actor_loss       | 0.238    |
|    critic_loss      | 0.00365  |
|    learning_rate    | 0.0007   |
|    n_updates        | 25098    |
|    total_actor_loss | 0.24     |
|    value_loss       | 0.00127  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -7.18    |
| time/               |          |
|    episodes         | 84       |
|    fps              | 3        |
|    time_elapsed     | 31686    |
|    total timesteps  | 126001   |
| train/              |          |
|    actor_loss       | 0.251    |
|    critic_loss      | 0.00545  |
|    learning_rate    | 0.0007   |
|    n_updates        | 25101    |
|    total_actor_loss | 0.255    |
|    value_loss       | 0.0039   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -7.06    |
| time/               |          |
|    episodes         | 88       |
|    fps              | 4        |
|    time_elapsed     | 32873    |
|    total timesteps  | 132005   |
| train/              |          |
|    actor_loss       | 0.247    |
|    critic_loss      | 0.00417  |
|    learning_rate    | 0.0007   |
|    n_updates        | 26301    |
|    total_actor_loss | 0.247    |
|    value_loss       | 0.000304 |
----------------------------------
Eval num_timesteps=135000, episode_reward=-14.78 +/- 7.98
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -14.8    |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    actor_loss       | 0.248    |
|    critic_loss      | 0.00413  |
|    learning_rate    | 0.0007   |
|    n_updates        | 26898    |
|    total_actor_loss | 0.25     |
|    value_loss       | 0.00183  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -6.83    |
| time/               |          |
|    episodes         | 92       |
|    fps              | 3        |
|    time_elapsed     | 35007    |
|    total timesteps  | 138009   |
| train/              |          |
|    actor_loss       | 0.245    |
|    critic_loss      | 0.0089   |
|    learning_rate    | 0.0007   |
|    n_updates        | 27501    |
|    total_actor_loss | 0.249    |
|    value_loss       | 0.00421  |
----------------------------------
Terminated
2021-12-01 12:49:39.852751: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-01 12:49:39.852836: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp6/TD3_28
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -7.67    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 7        |
|    time_elapsed     | 764      |
|    total timesteps  | 6004     |
| train/              |          |
|    actor_loss       | 0.075    |
|    critic_loss      | 0.00245  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1101     |
|    total_actor_loss | 0.0773   |
|    value_loss       | 0.00235  |
----------------------------------
Eval num_timesteps=9000, episode_reward=-8.42 +/- 9.06
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -8.42    |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | 0.073    |
|    critic_loss      | 0.00415  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1698     |
|    total_actor_loss | 0.0754   |
|    value_loss       | 0.00239  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -8.74    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 6        |
|    time_elapsed     | 1984     |
|    total timesteps  | 12008    |
| train/              |          |
|    actor_loss       | 0.0676   |
|    critic_loss      | 0.00735  |
|    learning_rate    | 0.0007   |
|    n_updates        | 2301     |
|    total_actor_loss | 0.072    |
|    value_loss       | 0.00448  |
----------------------------------
Eval num_timesteps=18000, episode_reward=-1.84 +/- 6.43
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -1.84    |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | 0.0653   |
|    critic_loss      | 0.00432  |
|    learning_rate    | 0.0007   |
|    n_updates        | 3498     |
|    total_actor_loss | 0.0683   |
|    value_loss       | 0.00297  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -4.43    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 5        |
|    time_elapsed     | 3188     |
|    total timesteps  | 18012    |
| train/              |          |
|    actor_loss       | 0.0562   |
|    critic_loss      | 0.00823  |
|    learning_rate    | 0.0007   |
|    n_updates        | 3501     |
|    total_actor_loss | 0.0637   |
|    value_loss       | 0.00749  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -4.82    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 6        |
|    time_elapsed     | 3994     |
|    total timesteps  | 24016    |
| train/              |          |
|    actor_loss       | 0.0521   |
|    critic_loss      | 0.0104   |
|    learning_rate    | 0.0007   |
|    n_updates        | 4704     |
|    total_actor_loss | 0.0599   |
|    value_loss       | 0.00778  |
----------------------------------
Eval num_timesteps=27000, episode_reward=-4.22 +/- 11.20
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -4.22    |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | 0.083    |
|    critic_loss      | 0.00905  |
|    learning_rate    | 0.0007   |
|    n_updates        | 5298     |
|    total_actor_loss | 0.0902   |
|    value_loss       | 0.00718  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -5.37    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 5        |
|    time_elapsed     | 5211     |
|    total timesteps  | 30020    |
| train/              |          |
|    actor_loss       | 0.0789   |
|    critic_loss      | 0.0116   |
|    learning_rate    | 0.0007   |
|    n_updates        | 5904     |
|    total_actor_loss | 0.09     |
|    value_loss       | 0.0111   |
----------------------------------
Eval num_timesteps=36000, episode_reward=-13.92 +/- 5.74
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -13.9    |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | 0.0989   |
|    critic_loss      | 0.0107   |
|    learning_rate    | 0.0007   |
|    n_updates        | 7098     |
|    total_actor_loss | 0.113    |
|    value_loss       | 0.0141   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -5.17    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 5        |
|    time_elapsed     | 6436     |
|    total timesteps  | 36024    |
| train/              |          |
|    actor_loss       | 0.0862   |
|    critic_loss      | 0.0221   |
|    learning_rate    | 0.0007   |
|    n_updates        | 7104     |
|    total_actor_loss | 0.102    |
|    value_loss       | 0.0155   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -6.1     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 5        |
|    time_elapsed     | 7235     |
|    total timesteps  | 42028    |
| train/              |          |
|    actor_loss       | 0.154    |
|    critic_loss      | 0.0125   |
|    learning_rate    | 0.0007   |
|    n_updates        | 8304     |
|    total_actor_loss | 0.161    |
|    value_loss       | 0.00741  |
----------------------------------
Eval num_timesteps=45000, episode_reward=-9.50 +/- 10.94
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -9.5     |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | 0.131    |
|    critic_loss      | 0.0106   |
|    learning_rate    | 0.0007   |
|    n_updates        | 8898     |
|    total_actor_loss | 0.135    |
|    value_loss       | 0.00414  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -6.6     |
| time/               |          |
|    episodes         | 32       |
|    fps              | 5        |
|    time_elapsed     | 8460     |
|    total timesteps  | 48032    |
| train/              |          |
|    actor_loss       | 0.154    |
|    critic_loss      | 0.0087   |
|    learning_rate    | 0.0007   |
|    n_updates        | 9507     |
|    total_actor_loss | 0.158    |
|    value_loss       | 0.00417  |
----------------------------------
Eval num_timesteps=54000, episode_reward=2.48 +/- 9.15
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 2.48     |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | 0.154    |
|    critic_loss      | 0.00462  |
|    learning_rate    | 0.0007   |
|    n_updates        | 10698    |
|    total_actor_loss | 0.163    |
|    value_loss       | 0.00814  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -6.33    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 5        |
|    time_elapsed     | 9663     |
|    total timesteps  | 54036    |
| train/              |          |
|    actor_loss       | 0.159    |
|    critic_loss      | 0.00665  |
|    learning_rate    | 0.0007   |
|    n_updates        | 10707    |
|    total_actor_loss | 0.161    |
|    value_loss       | 0.00144  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -5.65    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 5        |
|    time_elapsed     | 10453    |
|    total timesteps  | 60040    |
| train/              |          |
|    actor_loss       | 0.181    |
|    critic_loss      | 0.0161   |
|    learning_rate    | 0.0007   |
|    n_updates        | 11907    |
|    total_actor_loss | 0.184    |
|    value_loss       | 0.00295  |
----------------------------------
Eval num_timesteps=63000, episode_reward=-7.04 +/- 4.63
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -7.04    |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    actor_loss       | 0.152    |
|    critic_loss      | 0.00623  |
|    learning_rate    | 0.0007   |
|    n_updates        | 12498    |
|    total_actor_loss | 0.154    |
|    value_loss       | 0.00254  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -4.85    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 5        |
|    time_elapsed     | 11679    |
|    total timesteps  | 66044    |
| train/              |          |
|    actor_loss       | 0.147    |
|    critic_loss      | 0.00869  |
|    learning_rate    | 0.0007   |
|    n_updates        | 13107    |
|    total_actor_loss | 0.149    |
|    value_loss       | 0.00185  |
----------------------------------
Eval num_timesteps=72000, episode_reward=11.26 +/- 15.53
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 11.3     |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    actor_loss       | 0.168    |
|    critic_loss      | 0.00992  |
|    learning_rate    | 0.0007   |
|    n_updates        | 14298    |
|    total_actor_loss | 0.174    |
|    value_loss       | 0.0058   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -3.93    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 5        |
|    time_elapsed     | 12885    |
|    total timesteps  | 72048    |
| train/              |          |
|    actor_loss       | 0.169    |
|    critic_loss      | 0.0052   |
|    learning_rate    | 0.0007   |
|    n_updates        | 14310    |
|    total_actor_loss | 0.174    |
|    value_loss       | 0.00465  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -4.1     |
| time/               |          |
|    episodes         | 52       |
|    fps              | 5        |
|    time_elapsed     | 13682    |
|    total timesteps  | 78052    |
| train/              |          |
|    actor_loss       | 0.186    |
|    critic_loss      | 0.00931  |
|    learning_rate    | 0.0007   |
|    n_updates        | 15510    |
|    total_actor_loss | 0.193    |
|    value_loss       | 0.00722  |
----------------------------------
Eval num_timesteps=81000, episode_reward=0.34 +/- 7.79
Episode length: 1481.40 +/- 39.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.48e+03 |
|    mean_reward      | 0.34     |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    actor_loss       | 0.21     |
|    critic_loss      | 0.00917  |
|    learning_rate    | 0.0007   |
|    n_updates        | 16098    |
|    total_actor_loss | 0.217    |
|    value_loss       | 0.00727  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -3.49    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 5        |
|    time_elapsed     | 14895    |
|    total timesteps  | 84056    |
| train/              |          |
|    actor_loss       | 0.211    |
|    critic_loss      | 0.0109   |
|    learning_rate    | 0.0007   |
|    n_updates        | 16710    |
|    total_actor_loss | 0.219    |
|    value_loss       | 0.00749  |
----------------------------------
Eval num_timesteps=90000, episode_reward=-7.92 +/- 19.27
Episode length: 1381.80 +/- 238.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.38e+03 |
|    mean_reward      | -7.92    |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    actor_loss       | 0.186    |
|    critic_loss      | 0.00906  |
|    learning_rate    | 0.0007   |
|    n_updates        | 17898    |
|    total_actor_loss | 0.193    |
|    value_loss       | 0.00675  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -3.6     |
| time/               |          |
|    episodes         | 60       |
|    fps              | 5        |
|    time_elapsed     | 16100    |
|    total timesteps  | 90060    |
| train/              |          |
|    actor_loss       | 0.2      |
|    critic_loss      | 0.00442  |
|    learning_rate    | 0.0007   |
|    n_updates        | 17910    |
|    total_actor_loss | 0.208    |
|    value_loss       | 0.00799  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -3.27    |
| time/               |          |
|    episodes         | 64       |
|    fps              | 5        |
|    time_elapsed     | 16911    |
|    total timesteps  | 96064    |
| train/              |          |
|    actor_loss       | 0.218    |
|    critic_loss      | 0.00367  |
|    learning_rate    | 0.0007   |
|    n_updates        | 19113    |
|    total_actor_loss | 0.219    |
|    value_loss       | 0.000467 |
----------------------------------
Eval num_timesteps=99000, episode_reward=-0.76 +/- 3.37
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -0.76    |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    actor_loss       | 0.221    |
|    critic_loss      | 0.00404  |
|    learning_rate    | 0.0007   |
|    n_updates        | 19698    |
|    total_actor_loss | 0.224    |
|    value_loss       | 0.00339  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -3.6     |
| time/               |          |
|    episodes         | 68       |
|    fps              | 5        |
|    time_elapsed     | 18137    |
|    total timesteps  | 102068   |
| train/              |          |
|    actor_loss       | 0.275    |
|    critic_loss      | 0.003    |
|    learning_rate    | 0.0007   |
|    n_updates        | 20313    |
|    total_actor_loss | 0.276    |
|    value_loss       | 0.00102  |
----------------------------------
Eval num_timesteps=108000, episode_reward=-4.78 +/- 1.22
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -4.78    |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    actor_loss       | 0.292    |
|    critic_loss      | 0.00598  |
|    learning_rate    | 0.0007   |
|    n_updates        | 21498    |
|    total_actor_loss | 0.295    |
|    value_loss       | 0.0029   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -3.84    |
| time/               |          |
|    episodes         | 72       |
|    fps              | 5        |
|    time_elapsed     | 19349    |
|    total timesteps  | 108072   |
| train/              |          |
|    actor_loss       | 0.28     |
|    critic_loss      | 0.00357  |
|    learning_rate    | 0.0007   |
|    n_updates        | 21513    |
|    total_actor_loss | 0.281    |
|    value_loss       | 0.000612 |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -3.83    |
| time/               |          |
|    episodes         | 76       |
|    fps              | 5        |
|    time_elapsed     | 20144    |
|    total timesteps  | 114076   |
| train/              |          |
|    actor_loss       | 0.294    |
|    critic_loss      | 0.00383  |
|    learning_rate    | 0.0007   |
|    n_updates        | 22716    |
|    total_actor_loss | 0.297    |
|    value_loss       | 0.00297  |
----------------------------------
Eval num_timesteps=117000, episode_reward=2.94 +/- 11.63
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 2.94     |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    actor_loss       | 0.273    |
|    critic_loss      | 0.00691  |
|    learning_rate    | 0.0007   |
|    n_updates        | 23298    |
|    total_actor_loss | 0.278    |
|    value_loss       | 0.00495  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -3.52    |
| time/               |          |
|    episodes         | 80       |
|    fps              | 5        |
|    time_elapsed     | 21352    |
|    total timesteps  | 120080   |
| train/              |          |
|    actor_loss       | 0.288    |
|    critic_loss      | 0.00467  |
|    learning_rate    | 0.0007   |
|    n_updates        | 23916    |
|    total_actor_loss | 0.289    |
|    value_loss       | 0.00159  |
----------------------------------
Eval num_timesteps=126000, episode_reward=-7.38 +/- 8.04
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -7.38    |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    actor_loss       | 0.287    |
|    critic_loss      | 0.00506  |
|    learning_rate    | 0.0007   |
|    n_updates        | 25098    |
|    total_actor_loss | 0.291    |
|    value_loss       | 0.00372  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -3.68    |
| time/               |          |
|    episodes         | 84       |
|    fps              | 5        |
|    time_elapsed     | 22564    |
|    total timesteps  | 126084   |
| train/              |          |
|    actor_loss       | 0.289    |
|    critic_loss      | 0.00823  |
|    learning_rate    | 0.0007   |
|    n_updates        | 25116    |
|    total_actor_loss | 0.294    |
|    value_loss       | 0.00463  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -2.4     |
| time/               |          |
|    episodes         | 88       |
|    fps              | 5        |
|    time_elapsed     | 23292    |
|    total timesteps  | 131606   |
| train/              |          |
|    actor_loss       | 0.228    |
|    critic_loss      | 0.0116   |
|    learning_rate    | 0.0007   |
|    n_updates        | 26220    |
|    total_actor_loss | 0.234    |
|    value_loss       | 0.00633  |
----------------------------------
Eval num_timesteps=135000, episode_reward=11.96 +/- 28.83
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 12       |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    actor_loss       | 0.221    |
|    critic_loss      | 0.0137   |
|    learning_rate    | 0.0007   |
|    n_updates        | 26898    |
|    total_actor_loss | 0.23     |
|    value_loss       | 0.00871  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -2.81    |
| time/               |          |
|    episodes         | 92       |
|    fps              | 5        |
|    time_elapsed     | 24504    |
|    total timesteps  | 137610   |
| train/              |          |
|    actor_loss       | 0.206    |
|    critic_loss      | 0.00561  |
|    learning_rate    | 0.0007   |
|    n_updates        | 27420    |
|    total_actor_loss | 0.212    |
|    value_loss       | 0.00562  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -2.62    |
| time/               |          |
|    episodes         | 96       |
|    fps              | 5        |
|    time_elapsed     | 25308    |
|    total timesteps  | 143614   |
| train/              |          |
|    actor_loss       | 0.275    |
|    critic_loss      | 0.0128   |
|    learning_rate    | 0.0007   |
|    n_updates        | 28623    |
|    total_actor_loss | 0.288    |
|    value_loss       | 0.0126   |
----------------------------------
Eval num_timesteps=144000, episode_reward=-0.16 +/- 24.57
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -0.16    |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    actor_loss       | 0.253    |
|    critic_loss      | 0.00662  |
|    learning_rate    | 0.0007   |
|    n_updates        | 28698    |
|    total_actor_loss | 0.263    |
|    value_loss       | 0.0102   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -2.28    |
| time/               |          |
|    episodes         | 100      |
|    fps              | 5        |
|    time_elapsed     | 26542    |
|    total timesteps  | 149618   |
| train/              |          |
|    actor_loss       | 0.248    |
|    critic_loss      | 0.0138   |
|    learning_rate    | 0.0007   |
|    n_updates        | 29823    |
|    total_actor_loss | 0.265    |
|    value_loss       | 0.0172   |
----------------------------------
Eval num_timesteps=153000, episode_reward=-11.96 +/- 3.64
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -12      |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    actor_loss       | 0.272    |
|    critic_loss      | 0.0146   |
|    learning_rate    | 0.0007   |
|    n_updates        | 30498    |
|    total_actor_loss | 0.289    |
|    value_loss       | 0.0169   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -1.39    |
| time/               |          |
|    episodes         | 104      |
|    fps              | 5        |
|    time_elapsed     | 27767    |
|    total timesteps  | 155622   |
| train/              |          |
|    actor_loss       | 0.212    |
|    critic_loss      | 0.00938  |
|    learning_rate    | 0.0007   |
|    n_updates        | 31023    |
|    total_actor_loss | 0.221    |
|    value_loss       | 0.00883  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -1.49    |
| time/               |          |
|    episodes         | 108      |
|    fps              | 5        |
|    time_elapsed     | 28574    |
|    total timesteps  | 161626   |
| train/              |          |
|    actor_loss       | 0.29     |
|    critic_loss      | 0.00619  |
|    learning_rate    | 0.0007   |
|    n_updates        | 32226    |
|    total_actor_loss | 0.296    |
|    value_loss       | 0.0059   |
----------------------------------
Eval num_timesteps=162000, episode_reward=-6.94 +/- 7.65
Episode length: 1485.00 +/- 32.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.48e+03 |
|    mean_reward      | -6.94    |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    actor_loss       | 0.302    |
|    critic_loss      | 0.00767  |
|    learning_rate    | 0.0007   |
|    n_updates        | 32298    |
|    total_actor_loss | 0.31     |
|    value_loss       | 0.00838  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -1.86    |
| time/               |          |
|    episodes         | 112      |
|    fps              | 5        |
|    time_elapsed     | 29776    |
|    total timesteps  | 167630   |
| train/              |          |
|    actor_loss       | 0.305    |
|    critic_loss      | 0.00586  |
|    learning_rate    | 0.0007   |
|    n_updates        | 33426    |
|    total_actor_loss | 0.314    |
|    value_loss       | 0.0082   |
----------------------------------
Eval num_timesteps=171000, episode_reward=-6.50 +/- 9.56
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -6.5     |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    actor_loss       | 0.338    |
|    critic_loss      | 0.00463  |
|    learning_rate    | 0.0007   |
|    n_updates        | 34098    |
|    total_actor_loss | 0.34     |
|    value_loss       | 0.00252  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -1.74    |
| time/               |          |
|    episodes         | 116      |
|    fps              | 5        |
|    time_elapsed     | 30992    |
|    total timesteps  | 173634   |
| train/              |          |
|    actor_loss       | 0.331    |
|    critic_loss      | 0.0113   |
|    learning_rate    | 0.0007   |
|    n_updates        | 34626    |
|    total_actor_loss | 0.339    |
|    value_loss       | 0.00858  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -1.84    |
| time/               |          |
|    episodes         | 120      |
|    fps              | 5        |
|    time_elapsed     | 31796    |
|    total timesteps  | 179638   |
| train/              |          |
|    actor_loss       | 0.337    |
|    critic_loss      | 0.0073   |
|    learning_rate    | 0.0007   |
|    n_updates        | 35826    |
|    total_actor_loss | 0.342    |
|    value_loss       | 0.00494  |
----------------------------------
Eval num_timesteps=180000, episode_reward=-1.02 +/- 10.30
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -1.02    |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    actor_loss       | 0.381    |
|    critic_loss      | 0.0044   |
|    learning_rate    | 0.0007   |
|    n_updates        | 35898    |
|    total_actor_loss | 0.384    |
|    value_loss       | 0.00314  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -1.85    |
| time/               |          |
|    episodes         | 124      |
|    fps              | 5        |
|    time_elapsed     | 33004    |
|    total timesteps  | 185642   |
| train/              |          |
|    actor_loss       | 0.373    |
|    critic_loss      | 0.00524  |
|    learning_rate    | 0.0007   |
|    n_updates        | 37029    |
|    total_actor_loss | 0.374    |
|    value_loss       | 0.000886 |
----------------------------------
Eval num_timesteps=189000, episode_reward=-4.92 +/- 3.49
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -4.92    |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    actor_loss       | 0.388    |
|    critic_loss      | 0.00613  |
|    learning_rate    | 0.0007   |
|    n_updates        | 37698    |
|    total_actor_loss | 0.397    |
|    value_loss       | 0.00834  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -1.36    |
| time/               |          |
|    episodes         | 128      |
|    fps              | 5        |
|    time_elapsed     | 34176    |
|    total timesteps  | 191307   |
| train/              |          |
|    actor_loss       | 0.39     |
|    critic_loss      | 0.00758  |
|    learning_rate    | 0.0007   |
|    n_updates        | 38160    |
|    total_actor_loss | 0.393    |
|    value_loss       | 0.00294  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -1.63    |
| time/               |          |
|    episodes         | 132      |
|    fps              | 5        |
|    time_elapsed     | 34975    |
|    total timesteps  | 197311   |
| train/              |          |
|    actor_loss       | 0.407    |
|    critic_loss      | 0.0107   |
|    learning_rate    | 0.0007   |
|    n_updates        | 39363    |
|    total_actor_loss | 0.415    |
|    value_loss       | 0.00758  |
----------------------------------
Eval num_timesteps=198000, episode_reward=28.76 +/- 13.30
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 28.8     |
| time/               |          |
|    total_timesteps  | 198000   |
| train/              |          |
|    actor_loss       | 0.467    |
|    critic_loss      | 0.0102   |
|    learning_rate    | 0.0007   |
|    n_updates        | 39498    |
|    total_actor_loss | 0.472    |
|    value_loss       | 0.00523  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -1.16    |
| time/               |          |
|    episodes         | 136      |
|    fps              | 5        |
|    time_elapsed     | 36187    |
|    total timesteps  | 203315   |
| train/              |          |
|    actor_loss       | 0.415    |
|    critic_loss      | 0.0107   |
|    learning_rate    | 0.0007   |
|    n_updates        | 40563    |
|    total_actor_loss | 0.425    |
|    value_loss       | 0.00954  |
----------------------------------
Eval num_timesteps=207000, episode_reward=1.24 +/- 19.97
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 1.24     |
| time/               |          |
|    total_timesteps  | 207000   |
| train/              |          |
|    actor_loss       | 0.418    |
|    critic_loss      | 0.0173   |
|    learning_rate    | 0.0007   |
|    n_updates        | 41298    |
|    total_actor_loss | 0.443    |
|    value_loss       | 0.0245   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -1.22    |
| time/               |          |
|    episodes         | 140      |
|    fps              | 5        |
|    time_elapsed     | 37386    |
|    total timesteps  | 209319   |
| train/              |          |
|    actor_loss       | 0.456    |
|    critic_loss      | 0.013    |
|    learning_rate    | 0.0007   |
|    n_updates        | 41763    |
|    total_actor_loss | 0.462    |
|    value_loss       | 0.0057   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -1.94    |
| time/               |          |
|    episodes         | 144      |
|    fps              | 5        |
|    time_elapsed     | 38186    |
|    total timesteps  | 215323   |
| train/              |          |
|    actor_loss       | 0.506    |
|    critic_loss      | 0.00829  |
|    learning_rate    | 0.0007   |
|    n_updates        | 42963    |
|    total_actor_loss | 0.512    |
|    value_loss       | 0.00568  |
----------------------------------
Eval num_timesteps=216000, episode_reward=4.98 +/- 15.36
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 4.98     |
| time/               |          |
|    total_timesteps  | 216000   |
| train/              |          |
|    actor_loss       | 0.525    |
|    critic_loss      | 0.00858  |
|    learning_rate    | 0.0007   |
|    n_updates        | 43098    |
|    total_actor_loss | 0.533    |
|    value_loss       | 0.00832  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -1.83    |
| time/               |          |
|    episodes         | 148      |
|    fps              | 5        |
|    time_elapsed     | 39398    |
|    total timesteps  | 221327   |
| train/              |          |
|    actor_loss       | 0.549    |
|    critic_loss      | 0.0125   |
|    learning_rate    | 0.0007   |
|    n_updates        | 44166    |
|    total_actor_loss | 0.553    |
|    value_loss       | 0.00379  |
----------------------------------
Eval num_timesteps=225000, episode_reward=-10.46 +/- 5.78
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -10.5    |
| time/               |          |
|    total_timesteps  | 225000   |
| train/              |          |
|    actor_loss       | 0.51     |
|    critic_loss      | 0.0187   |
|    learning_rate    | 0.0007   |
|    n_updates        | 44898    |
|    total_actor_loss | 0.521    |
|    value_loss       | 0.0109   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -1.87    |
| time/               |          |
|    episodes         | 152      |
|    fps              | 5        |
|    time_elapsed     | 40635    |
|    total timesteps  | 227331   |
| train/              |          |
|    actor_loss       | 0.494    |
|    critic_loss      | 0.00884  |
|    learning_rate    | 0.0007   |
|    n_updates        | 45366    |
|    total_actor_loss | 0.5      |
|    value_loss       | 0.00628  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -2.25    |
| time/               |          |
|    episodes         | 156      |
|    fps              | 5        |
|    time_elapsed     | 41435    |
|    total timesteps  | 233335   |
| train/              |          |
|    actor_loss       | 0.558    |
|    critic_loss      | 0.00893  |
|    learning_rate    | 0.0007   |
|    n_updates        | 46566    |
|    total_actor_loss | 0.565    |
|    value_loss       | 0.0068   |
----------------------------------
Eval num_timesteps=234000, episode_reward=15.32 +/- 18.01
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 15.3     |
| time/               |          |
|    total_timesteps  | 234000   |
| train/              |          |
|    actor_loss       | 0.538    |
|    critic_loss      | 0.00795  |
|    learning_rate    | 0.0007   |
|    n_updates        | 46698    |
|    total_actor_loss | 0.54     |
|    value_loss       | 0.00183  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -1.33    |
| time/               |          |
|    episodes         | 160      |
|    fps              | 5        |
|    time_elapsed     | 42645    |
|    total timesteps  | 239339   |
| train/              |          |
|    actor_loss       | 0.537    |
|    critic_loss      | 0.00875  |
|    learning_rate    | 0.0007   |
|    n_updates        | 47766    |
|    total_actor_loss | 0.541    |
|    value_loss       | 0.00418  |
----------------------------------
Eval num_timesteps=243000, episode_reward=3.80 +/- 10.72
Episode length: 1489.40 +/- 23.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.49e+03 |
|    mean_reward      | 3.8      |
| time/               |          |
|    total_timesteps  | 243000   |
| train/              |          |
|    actor_loss       | 0.498    |
|    critic_loss      | 0.00839  |
|    learning_rate    | 0.0007   |
|    n_updates        | 48498    |
|    total_actor_loss | 0.503    |
|    value_loss       | 0.00491  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -1.17    |
| time/               |          |
|    episodes         | 164      |
|    fps              | 5        |
|    time_elapsed     | 43854    |
|    total timesteps  | 245343   |
| train/              |          |
|    actor_loss       | 0.524    |
|    critic_loss      | 0.00721  |
|    learning_rate    | 0.0007   |
|    n_updates        | 48969    |
|    total_actor_loss | 0.529    |
|    value_loss       | 0.00417  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.32    |
| time/               |          |
|    episodes         | 168      |
|    fps              | 5        |
|    time_elapsed     | 44661    |
|    total timesteps  | 251347   |
| train/              |          |
|    actor_loss       | 0.513    |
|    critic_loss      | 0.0102   |
|    learning_rate    | 0.0007   |
|    n_updates        | 50169    |
|    total_actor_loss | 0.525    |
|    value_loss       | 0.0118   |
----------------------------------
Eval num_timesteps=252000, episode_reward=2.04 +/- 10.94
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 2.04     |
| time/               |          |
|    total_timesteps  | 252000   |
| train/              |          |
|    actor_loss       | 0.536    |
|    critic_loss      | 0.0166   |
|    learning_rate    | 0.0007   |
|    n_updates        | 50298    |
|    total_actor_loss | 0.542    |
|    value_loss       | 0.00578  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.408   |
| time/               |          |
|    episodes         | 172      |
|    fps              | 5        |
|    time_elapsed     | 45874    |
|    total timesteps  | 257351   |
| train/              |          |
|    actor_loss       | 0.576    |
|    critic_loss      | 0.00973  |
|    learning_rate    | 0.0007   |
|    n_updates        | 51369    |
|    total_actor_loss | 0.582    |
|    value_loss       | 0.00583  |
----------------------------------
Eval num_timesteps=261000, episode_reward=-6.74 +/- 14.10
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -6.74    |
| time/               |          |
|    total_timesteps  | 261000   |
| train/              |          |
|    actor_loss       | 0.589    |
|    critic_loss      | 0.0132   |
|    learning_rate    | 0.0007   |
|    n_updates        | 52098    |
|    total_actor_loss | 0.597    |
|    value_loss       | 0.00876  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.132   |
| time/               |          |
|    episodes         | 176      |
|    fps              | 5        |
|    time_elapsed     | 47081    |
|    total timesteps  | 263355   |
| train/              |          |
|    actor_loss       | 0.581    |
|    critic_loss      | 0.0126   |
|    learning_rate    | 0.0007   |
|    n_updates        | 52569    |
|    total_actor_loss | 0.589    |
|    value_loss       | 0.00858  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.093   |
| time/               |          |
|    episodes         | 180      |
|    fps              | 5        |
|    time_elapsed     | 47867    |
|    total timesteps  | 269359   |
| train/              |          |
|    actor_loss       | 0.605    |
|    critic_loss      | 0.00997  |
|    learning_rate    | 0.0007   |
|    n_updates        | 53772    |
|    total_actor_loss | 0.62     |
|    value_loss       | 0.0151   |
----------------------------------
Eval num_timesteps=270000, episode_reward=-12.04 +/- 10.59
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -12      |
| time/               |          |
|    total_timesteps  | 270000   |
| train/              |          |
|    actor_loss       | 0.591    |
|    critic_loss      | 0.00807  |
|    learning_rate    | 0.0007   |
|    n_updates        | 53898    |
|    total_actor_loss | 0.594    |
|    value_loss       | 0.00221  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 0.197    |
| time/               |          |
|    episodes         | 184      |
|    fps              | 5        |
|    time_elapsed     | 48965    |
|    total timesteps  | 274677   |
| train/              |          |
|    actor_loss       | 0.536    |
|    critic_loss      | 0.0136   |
|    learning_rate    | 0.0007   |
|    n_updates        | 54834    |
|    total_actor_loss | 0.537    |
|    value_loss       | 0.00132  |
----------------------------------
Eval num_timesteps=279000, episode_reward=-6.86 +/- 14.25
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -6.86    |
| time/               |          |
|    total_timesteps  | 279000   |
| train/              |          |
|    actor_loss       | 0.608    |
|    critic_loss      | 0.0089   |
|    learning_rate    | 0.0007   |
|    n_updates        | 55698    |
|    total_actor_loss | 0.613    |
|    value_loss       | 0.00508  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.926   |
| time/               |          |
|    episodes         | 188      |
|    fps              | 5        |
|    time_elapsed     | 50159    |
|    total timesteps  | 280681   |
| train/              |          |
|    actor_loss       | 0.606    |
|    critic_loss      | 0.0143   |
|    learning_rate    | 0.0007   |
|    n_updates        | 56037    |
|    total_actor_loss | 0.616    |
|    value_loss       | 0.0108   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.779   |
| time/               |          |
|    episodes         | 192      |
|    fps              | 5        |
|    time_elapsed     | 50960    |
|    total timesteps  | 286685   |
| train/              |          |
|    actor_loss       | 0.653    |
|    critic_loss      | 0.0169   |
|    learning_rate    | 0.0007   |
|    n_updates        | 57237    |
|    total_actor_loss | 0.681    |
|    value_loss       | 0.0283   |
----------------------------------
Eval num_timesteps=288000, episode_reward=-4.42 +/- 5.96
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -4.42    |
| time/               |          |
|    total_timesteps  | 288000   |
| train/              |          |
|    actor_loss       | 0.649    |
|    critic_loss      | 0.0166   |
|    learning_rate    | 0.0007   |
|    n_updates        | 57498    |
|    total_actor_loss | 0.664    |
|    value_loss       | 0.0145   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.769   |
| time/               |          |
|    episodes         | 196      |
|    fps              | 5        |
|    time_elapsed     | 52156    |
|    total timesteps  | 292689   |
| train/              |          |
|    actor_loss       | 0.654    |
|    critic_loss      | 0.0259   |
|    learning_rate    | 0.0007   |
|    n_updates        | 58437    |
|    total_actor_loss | 0.662    |
|    value_loss       | 0.0077   |
----------------------------------
Eval num_timesteps=297000, episode_reward=0.04 +/- 5.71
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 0.04     |
| time/               |          |
|    total_timesteps  | 297000   |
| train/              |          |
|    actor_loss       | 0.643    |
|    critic_loss      | 0.0124   |
|    learning_rate    | 0.0007   |
|    n_updates        | 59298    |
|    total_actor_loss | 0.649    |
|    value_loss       | 0.00571  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -1.14    |
| time/               |          |
|    episodes         | 200      |
|    fps              | 5        |
|    time_elapsed     | 53343    |
|    total timesteps  | 298693   |
| train/              |          |
|    actor_loss       | 0.65     |
|    critic_loss      | 0.0174   |
|    learning_rate    | 0.0007   |
|    n_updates        | 59637    |
|    total_actor_loss | 0.664    |
|    value_loss       | 0.0139   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -1.56    |
| time/               |          |
|    episodes         | 204      |
|    fps              | 5        |
|    time_elapsed     | 54136    |
|    total timesteps  | 304697   |
| train/              |          |
|    actor_loss       | 0.653    |
|    critic_loss      | 0.00693  |
|    learning_rate    | 0.0007   |
|    n_updates        | 60840    |
|    total_actor_loss | 0.656    |
|    value_loss       | 0.00239  |
----------------------------------
Eval num_timesteps=306000, episode_reward=-0.02 +/- 13.85
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -0.02    |
| time/               |          |
|    total_timesteps  | 306000   |
| train/              |          |
|    actor_loss       | 0.628    |
|    critic_loss      | 0.0161   |
|    learning_rate    | 0.0007   |
|    n_updates        | 61098    |
|    total_actor_loss | 0.633    |
|    value_loss       | 0.00489  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.51    |
| time/               |          |
|    episodes         | 208      |
|    fps              | 5        |
|    time_elapsed     | 55352    |
|    total timesteps  | 310701   |
| train/              |          |
|    actor_loss       | 0.625    |
|    critic_loss      | 0.0162   |
|    learning_rate    | 0.0007   |
|    n_updates        | 62040    |
|    total_actor_loss | 0.635    |
|    value_loss       | 0.0106   |
----------------------------------
Eval num_timesteps=315000, episode_reward=-7.96 +/- 8.49
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -7.96    |
| time/               |          |
|    total_timesteps  | 315000   |
| train/              |          |
|    actor_loss       | 0.682    |
|    critic_loss      | 0.0178   |
|    learning_rate    | 0.0007   |
|    n_updates        | 62898    |
|    total_actor_loss | 0.693    |
|    value_loss       | 0.0108   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.442   |
| time/               |          |
|    episodes         | 212      |
|    fps              | 5        |
|    time_elapsed     | 56548    |
|    total timesteps  | 316705   |
| train/              |          |
|    actor_loss       | 0.705    |
|    critic_loss      | 0.0387   |
|    learning_rate    | 0.0007   |
|    n_updates        | 63240    |
|    total_actor_loss | 0.722    |
|    value_loss       | 0.0175   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.57    |
| time/               |          |
|    episodes         | 216      |
|    fps              | 5        |
|    time_elapsed     | 57345    |
|    total timesteps  | 322709   |
| train/              |          |
|    actor_loss       | 0.728    |
|    critic_loss      | 0.011    |
|    learning_rate    | 0.0007   |
|    n_updates        | 64440    |
|    total_actor_loss | 0.736    |
|    value_loss       | 0.0087   |
----------------------------------
Eval num_timesteps=324000, episode_reward=-2.80 +/- 11.43
Episode length: 1457.60 +/- 86.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.46e+03 |
|    mean_reward      | -2.8     |
| time/               |          |
|    total_timesteps  | 324000   |
| train/              |          |
|    actor_loss       | 0.718    |
|    critic_loss      | 0.00915  |
|    learning_rate    | 0.0007   |
|    n_updates        | 64698    |
|    total_actor_loss | 0.723    |
|    value_loss       | 0.0045   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.294   |
| time/               |          |
|    episodes         | 220      |
|    fps              | 5        |
|    time_elapsed     | 58540    |
|    total timesteps  | 328713   |
| train/              |          |
|    actor_loss       | 0.765    |
|    critic_loss      | 0.0111   |
|    learning_rate    | 0.0007   |
|    n_updates        | 65643    |
|    total_actor_loss | 0.771    |
|    value_loss       | 0.00652  |
----------------------------------
Eval num_timesteps=333000, episode_reward=4.32 +/- 18.51
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 4.32     |
| time/               |          |
|    total_timesteps  | 333000   |
| train/              |          |
|    actor_loss       | 0.769    |
|    critic_loss      | 0.0121   |
|    learning_rate    | 0.0007   |
|    n_updates        | 66498    |
|    total_actor_loss | 0.772    |
|    value_loss       | 0.00271  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.877   |
| time/               |          |
|    episodes         | 224      |
|    fps              | 5        |
|    time_elapsed     | 59758    |
|    total timesteps  | 334717   |
| train/              |          |
|    actor_loss       | 0.844    |
|    critic_loss      | 0.0191   |
|    learning_rate    | 0.0007   |
|    n_updates        | 66843    |
|    total_actor_loss | 0.851    |
|    value_loss       | 0.00689  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -1.12    |
| time/               |          |
|    episodes         | 228      |
|    fps              | 5        |
|    time_elapsed     | 60552    |
|    total timesteps  | 340721   |
| train/              |          |
|    actor_loss       | 0.801    |
|    critic_loss      | 0.0287   |
|    learning_rate    | 0.0007   |
|    n_updates        | 68043    |
|    total_actor_loss | 0.806    |
|    value_loss       | 0.00429  |
----------------------------------
Eval num_timesteps=342000, episode_reward=-3.72 +/- 13.23
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -3.72    |
| time/               |          |
|    total_timesteps  | 342000   |
| train/              |          |
|    actor_loss       | 0.811    |
|    critic_loss      | 0.012    |
|    learning_rate    | 0.0007   |
|    n_updates        | 68298    |
|    total_actor_loss | 0.818    |
|    value_loss       | 0.00616  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.401   |
| time/               |          |
|    episodes         | 232      |
|    fps              | 5        |
|    time_elapsed     | 61759    |
|    total timesteps  | 346725   |
| train/              |          |
|    actor_loss       | 0.81     |
|    critic_loss      | 0.00963  |
|    learning_rate    | 0.0007   |
|    n_updates        | 69243    |
|    total_actor_loss | 0.812    |
|    value_loss       | 0.00189  |
----------------------------------
Eval num_timesteps=351000, episode_reward=-3.98 +/- 7.19
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -3.98    |
| time/               |          |
|    total_timesteps  | 351000   |
| train/              |          |
|    actor_loss       | 0.806    |
|    critic_loss      | 0.012    |
|    learning_rate    | 0.0007   |
|    n_updates        | 70098    |
|    total_actor_loss | 0.808    |
|    value_loss       | 0.00219  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.765   |
| time/               |          |
|    episodes         | 236      |
|    fps              | 5        |
|    time_elapsed     | 62965    |
|    total timesteps  | 352729   |
| train/              |          |
|    actor_loss       | 0.806    |
|    critic_loss      | 0.00551  |
|    learning_rate    | 0.0007   |
|    n_updates        | 70446    |
|    total_actor_loss | 0.809    |
|    value_loss       | 0.00293  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.381   |
| time/               |          |
|    episodes         | 240      |
|    fps              | 5        |
|    time_elapsed     | 63761    |
|    total timesteps  | 358733   |
| train/              |          |
|    actor_loss       | 0.804    |
|    critic_loss      | 0.0154   |
|    learning_rate    | 0.0007   |
|    n_updates        | 71646    |
|    total_actor_loss | 0.811    |
|    value_loss       | 0.00717  |
----------------------------------
Eval num_timesteps=360000, episode_reward=0.34 +/- 12.09
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 0.34     |
| time/               |          |
|    total_timesteps  | 360000   |
| train/              |          |
|    actor_loss       | 0.802    |
|    critic_loss      | 0.0121   |
|    learning_rate    | 0.0007   |
|    n_updates        | 71898    |
|    total_actor_loss | 0.815    |
|    value_loss       | 0.013    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 0.133    |
| time/               |          |
|    episodes         | 244      |
|    fps              | 5        |
|    time_elapsed     | 64974    |
|    total timesteps  | 364737   |
| train/              |          |
|    actor_loss       | 0.803    |
|    critic_loss      | 0.0161   |
|    learning_rate    | 0.0007   |
|    n_updates        | 72846    |
|    total_actor_loss | 0.809    |
|    value_loss       | 0.00582  |
----------------------------------
Eval num_timesteps=369000, episode_reward=22.48 +/- 23.51
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 22.5     |
| time/               |          |
|    total_timesteps  | 369000   |
| train/              |          |
|    actor_loss       | 0.789    |
|    critic_loss      | 0.0128   |
|    learning_rate    | 0.0007   |
|    n_updates        | 73698    |
|    total_actor_loss | 0.796    |
|    value_loss       | 0.00708  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 0.041    |
| time/               |          |
|    episodes         | 248      |
|    fps              | 5        |
|    time_elapsed     | 66184    |
|    total timesteps  | 370741   |
| train/              |          |
|    actor_loss       | 0.812    |
|    critic_loss      | 0.00829  |
|    learning_rate    | 0.0007   |
|    n_updates        | 74049    |
|    total_actor_loss | 0.812    |
|    value_loss       | 6.81e-05 |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 0.324    |
| time/               |          |
|    episodes         | 252      |
|    fps              | 5        |
|    time_elapsed     | 66986    |
|    total timesteps  | 376745   |
| train/              |          |
|    actor_loss       | 0.791    |
|    critic_loss      | 0.0165   |
|    learning_rate    | 0.0007   |
|    n_updates        | 75249    |
|    total_actor_loss | 0.801    |
|    value_loss       | 0.0104   |
----------------------------------
Eval num_timesteps=378000, episode_reward=24.72 +/- 29.28
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 24.7     |
| time/               |          |
|    total_timesteps  | 378000   |
| train/              |          |
|    actor_loss       | 0.783    |
|    critic_loss      | 0.0137   |
|    learning_rate    | 0.0007   |
|    n_updates        | 75498    |
|    total_actor_loss | 0.794    |
|    value_loss       | 0.0105   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 1.08     |
| time/               |          |
|    episodes         | 256      |
|    fps              | 5        |
|    time_elapsed     | 68190    |
|    total timesteps  | 382749   |
| train/              |          |
|    actor_loss       | 0.737    |
|    critic_loss      | 0.0143   |
|    learning_rate    | 0.0007   |
|    n_updates        | 76449    |
|    total_actor_loss | 0.761    |
|    value_loss       | 0.0242   |
----------------------------------
Eval num_timesteps=387000, episode_reward=-3.68 +/- 6.59
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -3.68    |
| time/               |          |
|    total_timesteps  | 387000   |
| train/              |          |
|    actor_loss       | 0.78     |
|    critic_loss      | 0.0178   |
|    learning_rate    | 0.0007   |
|    n_updates        | 77298    |
|    total_actor_loss | 0.793    |
|    value_loss       | 0.0131   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 0.661    |
| time/               |          |
|    episodes         | 260      |
|    fps              | 5        |
|    time_elapsed     | 69396    |
|    total timesteps  | 388753   |
| train/              |          |
|    actor_loss       | 0.729    |
|    critic_loss      | 0.0181   |
|    learning_rate    | 0.0007   |
|    n_updates        | 77649    |
|    total_actor_loss | 0.735    |
|    value_loss       | 0.00606  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 0.391    |
| time/               |          |
|    episodes         | 264      |
|    fps              | 5        |
|    time_elapsed     | 70191    |
|    total timesteps  | 394757   |
| train/              |          |
|    actor_loss       | 0.771    |
|    critic_loss      | 0.0134   |
|    learning_rate    | 0.0007   |
|    n_updates        | 78852    |
|    total_actor_loss | 0.787    |
|    value_loss       | 0.0156   |
----------------------------------
Eval num_timesteps=396000, episode_reward=-7.98 +/- 10.79
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -7.98    |
| time/               |          |
|    total_timesteps  | 396000   |
| train/              |          |
|    actor_loss       | 0.791    |
|    critic_loss      | 0.008    |
|    learning_rate    | 0.0007   |
|    n_updates        | 79098    |
|    total_actor_loss | 0.797    |
|    value_loss       | 0.00618  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | -0.184   |
| time/               |          |
|    episodes         | 268      |
|    fps              | 5        |
|    time_elapsed     | 71399    |
|    total timesteps  | 400761   |
| train/              |          |
|    actor_loss       | 0.815    |
|    critic_loss      | 0.0108   |
|    learning_rate    | 0.0007   |
|    n_updates        | 80052    |
|    total_actor_loss | 0.823    |
|    value_loss       | 0.00813  |
----------------------------------
Eval num_timesteps=405000, episode_reward=0.48 +/- 21.62
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 0.48     |
| time/               |          |
|    total_timesteps  | 405000   |
| train/              |          |
|    actor_loss       | 0.791    |
|    critic_loss      | 0.0122   |
|    learning_rate    | 0.0007   |
|    n_updates        | 80898    |
|    total_actor_loss | 0.806    |
|    value_loss       | 0.0154   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 0.73     |
| time/               |          |
|    episodes         | 272      |
|    fps              | 5        |
|    time_elapsed     | 72606    |
|    total timesteps  | 406765   |
| train/              |          |
|    actor_loss       | 0.78     |
|    critic_loss      | 0.0119   |
|    learning_rate    | 0.0007   |
|    n_updates        | 81252    |
|    total_actor_loss | 0.792    |
|    value_loss       | 0.0116   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 0.084    |
| time/               |          |
|    episodes         | 276      |
|    fps              | 5        |
|    time_elapsed     | 73406    |
|    total timesteps  | 412769   |
| train/              |          |
|    actor_loss       | 0.841    |
|    critic_loss      | 0.00676  |
|    learning_rate    | 0.0007   |
|    n_updates        | 82452    |
|    total_actor_loss | 0.845    |
|    value_loss       | 0.00324  |
----------------------------------
Eval num_timesteps=414000, episode_reward=3.10 +/- 27.90
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 3.1      |
| time/               |          |
|    total_timesteps  | 414000   |
| train/              |          |
|    actor_loss       | 0.834    |
|    critic_loss      | 0.023    |
|    learning_rate    | 0.0007   |
|    n_updates        | 82698    |
|    total_actor_loss | 0.837    |
|    value_loss       | 0.00306  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 0.682    |
| time/               |          |
|    episodes         | 280      |
|    fps              | 5        |
|    time_elapsed     | 74614    |
|    total timesteps  | 418773   |
| train/              |          |
|    actor_loss       | 0.773    |
|    critic_loss      | 0.0159   |
|    learning_rate    | 0.0007   |
|    n_updates        | 83655    |
|    total_actor_loss | 0.778    |
|    value_loss       | 0.00476  |
----------------------------------
Eval num_timesteps=423000, episode_reward=38.62 +/- 44.09
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 38.6     |
| time/               |          |
|    total_timesteps  | 423000   |
| train/              |          |
|    actor_loss       | 0.771    |
|    critic_loss      | 0.012    |
|    learning_rate    | 0.0007   |
|    n_updates        | 84498    |
|    total_actor_loss | 0.778    |
|    value_loss       | 0.00658  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 1.56     |
| time/               |          |
|    episodes         | 284      |
|    fps              | 5        |
|    time_elapsed     | 75818    |
|    total timesteps  | 424777   |
| train/              |          |
|    actor_loss       | 0.657    |
|    critic_loss      | 0.021    |
|    learning_rate    | 0.0007   |
|    n_updates        | 84855    |
|    total_actor_loss | 0.668    |
|    value_loss       | 0.0116   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 2.53     |
| time/               |          |
|    episodes         | 288      |
|    fps              | 5        |
|    time_elapsed     | 76616    |
|    total timesteps  | 430781   |
| train/              |          |
|    actor_loss       | 0.764    |
|    critic_loss      | 0.00865  |
|    learning_rate    | 0.0007   |
|    n_updates        | 86055    |
|    total_actor_loss | 0.77     |
|    value_loss       | 0.00656  |
----------------------------------
Eval num_timesteps=432000, episode_reward=7.88 +/- 12.06
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 7.88     |
| time/               |          |
|    total_timesteps  | 432000   |
| train/              |          |
|    actor_loss       | 0.69     |
|    critic_loss      | 0.00667  |
|    learning_rate    | 0.0007   |
|    n_updates        | 86298    |
|    total_actor_loss | 0.695    |
|    value_loss       | 0.00484  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 3.4      |
| time/               |          |
|    episodes         | 292      |
|    fps              | 5        |
|    time_elapsed     | 77823    |
|    total timesteps  | 436785   |
| train/              |          |
|    actor_loss       | 0.609    |
|    critic_loss      | 0.0162   |
|    learning_rate    | 0.0007   |
|    n_updates        | 87255    |
|    total_actor_loss | 0.628    |
|    value_loss       | 0.0194   |
----------------------------------
Eval num_timesteps=441000, episode_reward=0.28 +/- 11.97
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 0.28     |
| time/               |          |
|    total_timesteps  | 441000   |
| train/              |          |
|    actor_loss       | 0.689    |
|    critic_loss      | 0.0136   |
|    learning_rate    | 0.0007   |
|    n_updates        | 88098    |
|    total_actor_loss | 0.692    |
|    value_loss       | 0.00313  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 3.46     |
| time/               |          |
|    episodes         | 296      |
|    fps              | 5        |
|    time_elapsed     | 79040    |
|    total timesteps  | 442789   |
| train/              |          |
|    actor_loss       | 0.631    |
|    critic_loss      | 0.0141   |
|    learning_rate    | 0.0007   |
|    n_updates        | 88458    |
|    total_actor_loss | 0.64     |
|    value_loss       | 0.00885  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 4.51     |
| time/               |          |
|    episodes         | 300      |
|    fps              | 5        |
|    time_elapsed     | 79837    |
|    total timesteps  | 448793   |
| train/              |          |
|    actor_loss       | 0.581    |
|    critic_loss      | 0.0441   |
|    learning_rate    | 0.0007   |
|    n_updates        | 89658    |
|    total_actor_loss | 0.609    |
|    value_loss       | 0.0282   |
----------------------------------
Eval num_timesteps=450000, episode_reward=4.76 +/- 8.98
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 4.76     |
| time/               |          |
|    total_timesteps  | 450000   |
| train/              |          |
|    actor_loss       | 0.541    |
|    critic_loss      | 0.0208   |
|    learning_rate    | 0.0007   |
|    n_updates        | 89898    |
|    total_actor_loss | 0.555    |
|    value_loss       | 0.014    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 4.47     |
| time/               |          |
|    episodes         | 304      |
|    fps              | 5        |
|    time_elapsed     | 81055    |
|    total timesteps  | 454797   |
| train/              |          |
|    actor_loss       | 0.52     |
|    critic_loss      | 0.0261   |
|    learning_rate    | 0.0007   |
|    n_updates        | 90858    |
|    total_actor_loss | 0.54     |
|    value_loss       | 0.0203   |
----------------------------------
Eval num_timesteps=459000, episode_reward=9.08 +/- 24.54
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 9.08     |
| time/               |          |
|    total_timesteps  | 459000   |
| train/              |          |
|    actor_loss       | 0.671    |
|    critic_loss      | 0.00459  |
|    learning_rate    | 0.0007   |
|    n_updates        | 91698    |
|    total_actor_loss | 0.676    |
|    value_loss       | 0.0053   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 4.39     |
| time/               |          |
|    episodes         | 308      |
|    fps              | 5        |
|    time_elapsed     | 82263    |
|    total timesteps  | 460801   |
| train/              |          |
|    actor_loss       | 0.674    |
|    critic_loss      | 0.0483   |
|    learning_rate    | 0.0007   |
|    n_updates        | 92061    |
|    total_actor_loss | 0.676    |
|    value_loss       | 0.00148  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 4.7      |
| time/               |          |
|    episodes         | 312      |
|    fps              | 5        |
|    time_elapsed     | 83078    |
|    total timesteps  | 466805   |
| train/              |          |
|    actor_loss       | 0.677    |
|    critic_loss      | 0.0199   |
|    learning_rate    | 0.0007   |
|    n_updates        | 93261    |
|    total_actor_loss | 0.681    |
|    value_loss       | 0.00396  |
----------------------------------
Eval num_timesteps=468000, episode_reward=19.10 +/- 20.66
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 19.1     |
| time/               |          |
|    total_timesteps  | 468000   |
| train/              |          |
|    actor_loss       | 0.703    |
|    critic_loss      | 0.0138   |
|    learning_rate    | 0.0007   |
|    n_updates        | 93498    |
|    total_actor_loss | 0.743    |
|    value_loss       | 0.0399   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 4.61     |
| time/               |          |
|    episodes         | 316      |
|    fps              | 5        |
|    time_elapsed     | 84302    |
|    total timesteps  | 472809   |
| train/              |          |
|    actor_loss       | 0.777    |
|    critic_loss      | 0.00642  |
|    learning_rate    | 0.0007   |
|    n_updates        | 94461    |
|    total_actor_loss | 0.779    |
|    value_loss       | 0.00144  |
----------------------------------
Eval num_timesteps=477000, episode_reward=39.06 +/- 35.20
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 39.1     |
| time/               |          |
|    total_timesteps  | 477000   |
| train/              |          |
|    actor_loss       | 0.518    |
|    critic_loss      | 0.0136   |
|    learning_rate    | 0.0007   |
|    n_updates        | 95298    |
|    total_actor_loss | 0.525    |
|    value_loss       | 0.00783  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 6.38     |
| time/               |          |
|    episodes         | 320      |
|    fps              | 5        |
|    time_elapsed     | 85532    |
|    total timesteps  | 478813   |
| train/              |          |
|    actor_loss       | 0.546    |
|    critic_loss      | 0.0353   |
|    learning_rate    | 0.0007   |
|    n_updates        | 95661    |
|    total_actor_loss | 0.551    |
|    value_loss       | 0.00568  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 9.06     |
| time/               |          |
|    episodes         | 324      |
|    fps              | 5        |
|    time_elapsed     | 86336    |
|    total timesteps  | 484817   |
| train/              |          |
|    actor_loss       | 0.198    |
|    critic_loss      | 0.0397   |
|    learning_rate    | 0.0007   |
|    n_updates        | 96864    |
|    total_actor_loss | 0.228    |
|    value_loss       | 0.0296   |
----------------------------------
Eval num_timesteps=486000, episode_reward=-4.66 +/- 13.83
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -4.66    |
| time/               |          |
|    total_timesteps  | 486000   |
| train/              |          |
|    actor_loss       | 0.162    |
|    critic_loss      | 0.0132   |
|    learning_rate    | 0.0007   |
|    n_updates        | 97098    |
|    total_actor_loss | 0.178    |
|    value_loss       | 0.0169   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 11.9     |
| time/               |          |
|    episodes         | 328      |
|    fps              | 5        |
|    time_elapsed     | 87551    |
|    total timesteps  | 490821   |
| train/              |          |
|    actor_loss       | -0.197   |
|    critic_loss      | 0.0343   |
|    learning_rate    | 0.0007   |
|    n_updates        | 98064    |
|    total_actor_loss | -0.151   |
|    value_loss       | 0.0465   |
----------------------------------
Eval num_timesteps=495000, episode_reward=35.38 +/- 48.09
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 35.4     |
| time/               |          |
|    total_timesteps  | 495000   |
| train/              |          |
|    actor_loss       | 0.182    |
|    critic_loss      | 0.0169   |
|    learning_rate    | 0.0007   |
|    n_updates        | 98898    |
|    total_actor_loss | 0.189    |
|    value_loss       | 0.00646  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 11.6     |
| time/               |          |
|    episodes         | 332      |
|    fps              | 5        |
|    time_elapsed     | 88769    |
|    total timesteps  | 496825   |
| train/              |          |
|    actor_loss       | 0.376    |
|    critic_loss      | 0.0135   |
|    learning_rate    | 0.0007   |
|    n_updates        | 99264    |
|    total_actor_loss | 0.382    |
|    value_loss       | 0.00681  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 11.6     |
| time/               |          |
|    episodes         | 336      |
|    fps              | 5        |
|    time_elapsed     | 89573    |
|    total timesteps  | 502829   |
| train/              |          |
|    actor_loss       | 0.79     |
|    critic_loss      | 0.0137   |
|    learning_rate    | 0.0007   |
|    n_updates        | 100464   |
|    total_actor_loss | 0.797    |
|    value_loss       | 0.00728  |
----------------------------------
Eval num_timesteps=504000, episode_reward=24.50 +/- 47.73
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 24.5     |
| time/               |          |
|    total_timesteps  | 504000   |
| train/              |          |
|    actor_loss       | 0.778    |
|    critic_loss      | 0.00503  |
|    learning_rate    | 0.0007   |
|    n_updates        | 100698   |
|    total_actor_loss | 0.779    |
|    value_loss       | 0.000729 |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 11.5     |
| time/               |          |
|    episodes         | 340      |
|    fps              | 5        |
|    time_elapsed     | 90801    |
|    total timesteps  | 508833   |
| train/              |          |
|    actor_loss       | 0.697    |
|    critic_loss      | 0.00538  |
|    learning_rate    | 0.0007   |
|    n_updates        | 101667   |
|    total_actor_loss | 0.7      |
|    value_loss       | 0.00369  |
----------------------------------
Eval num_timesteps=513000, episode_reward=3.58 +/- 51.08
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 3.58     |
| time/               |          |
|    total_timesteps  | 513000   |
| train/              |          |
|    actor_loss       | 0.62     |
|    critic_loss      | 0.0186   |
|    learning_rate    | 0.0007   |
|    n_updates        | 102498   |
|    total_actor_loss | 0.622    |
|    value_loss       | 0.0019   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 11.3     |
| time/               |          |
|    episodes         | 344      |
|    fps              | 5        |
|    time_elapsed     | 92021    |
|    total timesteps  | 514837   |
| train/              |          |
|    actor_loss       | 0.737    |
|    critic_loss      | 0.0565   |
|    learning_rate    | 0.0007   |
|    n_updates        | 102867   |
|    total_actor_loss | 0.738    |
|    value_loss       | 0.00129  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 12.4     |
| time/               |          |
|    episodes         | 348      |
|    fps              | 5        |
|    time_elapsed     | 92829    |
|    total timesteps  | 520841   |
| train/              |          |
|    actor_loss       | 0.0788   |
|    critic_loss      | 0.0229   |
|    learning_rate    | 0.0007   |
|    n_updates        | 104067   |
|    total_actor_loss | 0.0906   |
|    value_loss       | 0.0117   |
----------------------------------
Eval num_timesteps=522000, episode_reward=19.14 +/- 57.17
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 19.1     |
| time/               |          |
|    total_timesteps  | 522000   |
| train/              |          |
|    actor_loss       | 0.222    |
|    critic_loss      | 0.0248   |
|    learning_rate    | 0.0007   |
|    n_updates        | 104298   |
|    total_actor_loss | 0.236    |
|    value_loss       | 0.0132   |
----------------------------------
