running build_ext
building 'mujoco_py.cymj' extension
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o -fopenmp -w
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py
gcc -pthread -shared /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mjpro150/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mjpro150/bin -lmujoco150 -lglewegl -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp
2021-12-07 23:10:31.139016: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-07 23:10:31.139086: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp10/TD3_6
Found 1 GPUs for rendering. Using device 0.
Terminated
2021-12-07 23:19:35.507189: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-07 23:19:35.507301: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp10/TD3_9
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.89e+03 |
|    ep_rew_mean     | 66.3     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 16       |
|    time_elapsed    | 465      |
|    total timesteps | 7563     |
| train/             |          |
|    actor_loss      | 2.09     |
|    critic_loss     | 19.1     |
|    learning_rate   | 0.0005   |
|    n_updates       | 5562     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-28.93 +/- 56.97
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -28.9    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.68     |
|    critic_loss     | 19       |
|    learning_rate   | 0.0005   |
|    n_updates       | 7999     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.95e+03 |
|    ep_rew_mean     | 44.2     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 12       |
|    time_elapsed    | 1280     |
|    total timesteps | 15567    |
| train/             |          |
|    actor_loss      | 1.46     |
|    critic_loss     | 19.4     |
|    learning_rate   | 0.0005   |
|    n_updates       | 13566    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-64.48 +/- 113.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -64.5    |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 0.874    |
|    critic_loss     | 0.321    |
|    learning_rate   | 0.0005   |
|    n_updates       | 17999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.96e+03 |
|    ep_rew_mean     | -30.6    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 11       |
|    time_elapsed    | 2086     |
|    total timesteps | 23571    |
| train/             |          |
|    actor_loss      | 1.11     |
|    critic_loss     | 0.403    |
|    learning_rate   | 0.0005   |
|    n_updates       | 21570    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-72.34 +/- 95.41
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -72.3    |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 1.01     |
|    critic_loss     | 0.377    |
|    learning_rate   | 0.0005   |
|    n_updates       | 27999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.97e+03 |
|    ep_rew_mean     | -38.2    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 10       |
|    time_elapsed    | 2928     |
|    total timesteps | 31575    |
| train/             |          |
|    actor_loss      | 0.96     |
|    critic_loss     | 19.7     |
|    learning_rate   | 0.0005   |
|    n_updates       | 29574    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -45.1    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 11       |
|    time_elapsed    | 3563     |
|    total timesteps | 39579    |
| train/             |          |
|    actor_loss      | 1.3      |
|    critic_loss     | 19.3     |
|    learning_rate   | 0.0005   |
|    n_updates       | 37578    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-183.11 +/- 60.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 1.21     |
|    critic_loss     | 19.5     |
|    learning_rate   | 0.0005   |
|    n_updates       | 37999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.98e+03 |
|    ep_rew_mean     | -63.7    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 11       |
|    time_elapsed    | 4316     |
|    total timesteps | 47583    |
| train/             |          |
|    actor_loss      | 1.52     |
|    critic_loss     | 18.8     |
|    learning_rate   | 0.0005   |
|    n_updates       | 45582    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-85.30 +/- 89.97
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -85.3    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 1.59     |
|    critic_loss     | 0.34     |
|    learning_rate   | 0.0005   |
|    n_updates       | 47999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -60.8    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 10       |
|    time_elapsed    | 5070     |
|    total timesteps | 55587    |
| train/             |          |
|    actor_loss      | 1.63     |
|    critic_loss     | 0.56     |
|    learning_rate   | 0.0005   |
|    n_updates       | 53586    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-159.38 +/- 71.17
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 1.85     |
|    critic_loss     | 0.599    |
|    learning_rate   | 0.0005   |
|    n_updates       | 57999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -50.5    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 10       |
|    time_elapsed    | 5819     |
|    total timesteps | 63591    |
| train/             |          |
|    actor_loss      | 2.05     |
|    critic_loss     | 0.399    |
|    learning_rate   | 0.0005   |
|    n_updates       | 61590    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-176.74 +/- 135.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 1.89     |
|    critic_loss     | 37.7     |
|    learning_rate   | 0.0005   |
|    n_updates       | 67999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -54      |
| time/              |          |
|    episodes        | 36       |
|    fps             | 10       |
|    time_elapsed    | 6575     |
|    total timesteps | 71595    |
| train/             |          |
|    actor_loss      | 2.18     |
|    critic_loss     | 19.4     |
|    learning_rate   | 0.0005   |
|    n_updates       | 69594    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -65.1    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 11       |
|    time_elapsed    | 7232     |
|    total timesteps | 79599    |
| train/             |          |
|    actor_loss      | 2.16     |
|    critic_loss     | 0.388    |
|    learning_rate   | 0.0005   |
|    n_updates       | 77598    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-76.83 +/- 185.79
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -76.8    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 2.27     |
|    critic_loss     | 0.482    |
|    learning_rate   | 0.0005   |
|    n_updates       | 77999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -55.3    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 10       |
|    time_elapsed    | 7988     |
|    total timesteps | 87603    |
| train/             |          |
|    actor_loss      | 2.41     |
|    critic_loss     | 0.756    |
|    learning_rate   | 0.0005   |
|    n_updates       | 85602    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-104.30 +/- 142.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 2.28     |
|    critic_loss     | 0.63     |
|    learning_rate   | 0.0005   |
|    n_updates       | 87999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -59.8    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 10       |
|    time_elapsed    | 8740     |
|    total timesteps | 95607    |
| train/             |          |
|    actor_loss      | 2.51     |
|    critic_loss     | 19.1     |
|    learning_rate   | 0.0005   |
|    n_updates       | 93606    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-130.13 +/- 70.56
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 1.91     |
|    critic_loss     | 38.2     |
|    learning_rate   | 0.0005   |
|    n_updates       | 97999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -65.2    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 10       |
|    time_elapsed    | 9492     |
|    total timesteps | 103611   |
| train/             |          |
|    actor_loss      | 2.16     |
|    critic_loss     | 0.567    |
|    learning_rate   | 0.0005   |
|    n_updates       | 101610   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-74.99 +/- 129.47
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -75      |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 2.34     |
|    critic_loss     | 37.5     |
|    learning_rate   | 0.0005   |
|    n_updates       | 107999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -65.8    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 10       |
|    time_elapsed    | 10241    |
|    total timesteps | 111615   |
| train/             |          |
|    actor_loss      | 2.04     |
|    critic_loss     | 0.614    |
|    learning_rate   | 0.0005   |
|    n_updates       | 109614   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -63.3    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 11       |
|    time_elapsed    | 10873    |
|    total timesteps | 119619   |
| train/             |          |
|    actor_loss      | 2.44     |
|    critic_loss     | 0.414    |
|    learning_rate   | 0.0005   |
|    n_updates       | 117618   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-110.91 +/- 90.47
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 2.38     |
|    critic_loss     | 0.337    |
|    learning_rate   | 0.0005   |
|    n_updates       | 117999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -73.3    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 10       |
|    time_elapsed    | 11630    |
|    total timesteps | 127623   |
| train/             |          |
|    actor_loss      | 2.81     |
|    critic_loss     | 37.9     |
|    learning_rate   | 0.0005   |
|    n_updates       | 125622   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-17.85 +/- 62.76
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -17.8    |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 2.27     |
|    critic_loss     | 19.4     |
|    learning_rate   | 0.0005   |
|    n_updates       | 127999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -79.5    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 10       |
|    time_elapsed    | 12388    |
|    total timesteps | 135627   |
| train/             |          |
|    actor_loss      | 2.82     |
|    critic_loss     | 19.2     |
|    learning_rate   | 0.0005   |
|    n_updates       | 133626   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-67.30 +/- 107.43
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -67.3    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 2.61     |
|    critic_loss     | 19.1     |
|    learning_rate   | 0.0005   |
|    n_updates       | 137999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -84.6    |
| time/              |          |
|    episodes        | 72       |
|    fps             | 10       |
|    time_elapsed    | 13141    |
|    total timesteps | 143631   |
| train/             |          |
|    actor_loss      | 2.86     |
|    critic_loss     | 19       |
|    learning_rate   | 0.0005   |
|    n_updates       | 141630   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-43.52 +/- 101.07
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -43.5    |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 2.78     |
|    critic_loss     | 0.56     |
|    learning_rate   | 0.0005   |
|    n_updates       | 147999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -87.4    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 10       |
|    time_elapsed    | 13896    |
|    total timesteps | 151635   |
| train/             |          |
|    actor_loss      | 2.51     |
|    critic_loss     | 0.521    |
|    learning_rate   | 0.0005   |
|    n_updates       | 149634   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -96.4    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 10       |
|    time_elapsed    | 14531    |
|    total timesteps | 159639   |
| train/             |          |
|    actor_loss      | 3.13     |
|    critic_loss     | 0.509    |
|    learning_rate   | 0.0005   |
|    n_updates       | 157638   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-95.95 +/- 102.13
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -95.9    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 3.31     |
|    critic_loss     | 19.2     |
|    learning_rate   | 0.0005   |
|    n_updates       | 157999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -104     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 10       |
|    time_elapsed    | 15290    |
|    total timesteps | 167643   |
| train/             |          |
|    actor_loss      | 3.14     |
|    critic_loss     | 0.56     |
|    learning_rate   | 0.0005   |
|    n_updates       | 165642   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-31.84 +/- 124.69
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -31.8    |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 3.07     |
|    critic_loss     | 19.2     |
|    learning_rate   | 0.0005   |
|    n_updates       | 167999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -102     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 10       |
|    time_elapsed    | 16067    |
|    total timesteps | 175647   |
| train/             |          |
|    actor_loss      | 3.28     |
|    critic_loss     | 0.901    |
|    learning_rate   | 0.0005   |
|    n_updates       | 173646   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-51.59 +/- 64.84
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -51.6    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 3.46     |
|    critic_loss     | 18.4     |
|    learning_rate   | 0.0005   |
|    n_updates       | 177999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -108     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 10       |
|    time_elapsed    | 16819    |
|    total timesteps | 183651   |
| train/             |          |
|    actor_loss      | 3.61     |
|    critic_loss     | 0.551    |
|    learning_rate   | 0.0005   |
|    n_updates       | 181650   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-235.63 +/- 110.25
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 3.65     |
|    critic_loss     | 18.3     |
|    learning_rate   | 0.0005   |
|    n_updates       | 187999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 10       |
|    time_elapsed    | 17574    |
|    total timesteps | 191655   |
| train/             |          |
|    actor_loss      | 3.38     |
|    critic_loss     | 0.696    |
|    learning_rate   | 0.0005   |
|    n_updates       | 189654   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -101     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 10       |
|    time_elapsed    | 18208    |
|    total timesteps | 199659   |
| train/             |          |
|    actor_loss      | 3.64     |
|    critic_loss     | 18.6     |
|    learning_rate   | 0.0005   |
|    n_updates       | 197658   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-69.38 +/- 123.03
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -69.4    |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 3.71     |
|    critic_loss     | 0.705    |
|    learning_rate   | 0.0005   |
|    n_updates       | 197999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 104      |
|    fps             | 10       |
|    time_elapsed    | 18966    |
|    total timesteps | 207663   |
| train/             |          |
|    actor_loss      | 3.67     |
|    critic_loss     | 18.5     |
|    learning_rate   | 0.0005   |
|    n_updates       | 205662   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-31.40 +/- 147.69
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -31.4    |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | 3.43     |
|    critic_loss     | 37.2     |
|    learning_rate   | 0.0005   |
|    n_updates       | 207999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -113     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 10       |
|    time_elapsed    | 19718    |
|    total timesteps | 215667   |
| train/             |          |
|    actor_loss      | 3.58     |
|    critic_loss     | 0.607    |
|    learning_rate   | 0.0005   |
|    n_updates       | 213666   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-48.13 +/- 82.82
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -48.1    |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 3.62     |
|    critic_loss     | 18.3     |
|    learning_rate   | 0.0005   |
|    n_updates       | 217999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -107     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 10       |
|    time_elapsed    | 20470    |
|    total timesteps | 223671   |
| train/             |          |
|    actor_loss      | 3.75     |
|    critic_loss     | 0.882    |
|    learning_rate   | 0.0005   |
|    n_updates       | 221670   |
---------------------------------
Eval num_timesteps=230000, episode_reward=-87.63 +/- 121.10
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -87.6    |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 3.47     |
|    critic_loss     | 18.7     |
|    learning_rate   | 0.0005   |
|    n_updates       | 227999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 10       |
|    time_elapsed    | 21259    |
|    total timesteps | 231675   |
| train/             |          |
|    actor_loss      | 3.64     |
|    critic_loss     | 0.607    |
|    learning_rate   | 0.0005   |
|    n_updates       | 229674   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -104     |
| time/              |          |
|    episodes        | 120      |
|    fps             | 10       |
|    time_elapsed    | 21893    |
|    total timesteps | 239679   |
| train/             |          |
|    actor_loss      | 3.42     |
|    critic_loss     | 0.3      |
|    learning_rate   | 0.0005   |
|    n_updates       | 237678   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-98.82 +/- 131.06
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -98.8    |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 3.59     |
|    critic_loss     | 36.6     |
|    learning_rate   | 0.0005   |
|    n_updates       | 237999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -99.7    |
| time/              |          |
|    episodes        | 124      |
|    fps             | 10       |
|    time_elapsed    | 22647    |
|    total timesteps | 247683   |
| train/             |          |
|    actor_loss      | 3.41     |
|    critic_loss     | 0.527    |
|    learning_rate   | 0.0005   |
|    n_updates       | 245682   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-104.60 +/- 122.44
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -105     |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | 3.52     |
|    critic_loss     | 18.8     |
|    learning_rate   | 0.0005   |
|    n_updates       | 247999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -99.8    |
| time/              |          |
|    episodes        | 128      |
|    fps             | 10       |
|    time_elapsed    | 23427    |
|    total timesteps | 255687   |
| train/             |          |
|    actor_loss      | 3.96     |
|    critic_loss     | 0.624    |
|    learning_rate   | 0.0005   |
|    n_updates       | 253686   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-64.48 +/- 117.30
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -64.5    |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 3.43     |
|    critic_loss     | 0.588    |
|    learning_rate   | 0.0005   |
|    n_updates       | 257999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -99.4    |
| time/              |          |
|    episodes        | 132      |
|    fps             | 10       |
|    time_elapsed    | 24179    |
|    total timesteps | 263691   |
| train/             |          |
|    actor_loss      | 3.68     |
|    critic_loss     | 0.472    |
|    learning_rate   | 0.0005   |
|    n_updates       | 261690   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-126.68 +/- 107.03
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | 3.36     |
|    critic_loss     | 18.9     |
|    learning_rate   | 0.0005   |
|    n_updates       | 267999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -92.7    |
| time/              |          |
|    episodes        | 136      |
|    fps             | 10       |
|    time_elapsed    | 24936    |
|    total timesteps | 271695   |
| train/             |          |
|    actor_loss      | 3.65     |
|    critic_loss     | 18.5     |
|    learning_rate   | 0.0005   |
|    n_updates       | 269694   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -91.8    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 10       |
|    time_elapsed    | 25590    |
|    total timesteps | 279699   |
| train/             |          |
|    actor_loss      | 3.14     |
|    critic_loss     | 1.09     |
|    learning_rate   | 0.0005   |
|    n_updates       | 277698   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-56.05 +/- 130.07
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -56.1    |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 3.81     |
|    critic_loss     | 36.6     |
|    learning_rate   | 0.0005   |
|    n_updates       | 277999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -102     |
| time/              |          |
|    episodes        | 144      |
|    fps             | 10       |
|    time_elapsed    | 26342    |
|    total timesteps | 287703   |
| train/             |          |
|    actor_loss      | 3.55     |
|    critic_loss     | 0.659    |
|    learning_rate   | 0.0005   |
|    n_updates       | 285702   |
---------------------------------
Eval num_timesteps=290000, episode_reward=41.62 +/- 53.39
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 41.6     |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | 3.56     |
|    critic_loss     | 18.6     |
|    learning_rate   | 0.0005   |
|    n_updates       | 287999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -101     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 10       |
|    time_elapsed    | 27093    |
|    total timesteps | 295707   |
| train/             |          |
|    actor_loss      | 3.86     |
|    critic_loss     | 36       |
|    learning_rate   | 0.0005   |
|    n_updates       | 293706   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-149.65 +/- 77.03
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 3.77     |
|    critic_loss     | 0.687    |
|    learning_rate   | 0.0005   |
|    n_updates       | 297999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -95.6    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 10       |
|    time_elapsed    | 27849    |
|    total timesteps | 303711   |
| train/             |          |
|    actor_loss      | 3.65     |
|    critic_loss     | 18.7     |
|    learning_rate   | 0.0005   |
|    n_updates       | 301710   |
---------------------------------
Eval num_timesteps=310000, episode_reward=-78.41 +/- 126.88
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -78.4    |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | 3.83     |
|    critic_loss     | 18.5     |
|    learning_rate   | 0.0005   |
|    n_updates       | 307999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -94.7    |
| time/              |          |
|    episodes        | 156      |
|    fps             | 10       |
|    time_elapsed    | 28603    |
|    total timesteps | 311715   |
| train/             |          |
|    actor_loss      | 3.97     |
|    critic_loss     | 0.464    |
|    learning_rate   | 0.0005   |
|    n_updates       | 309714   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -92.2    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 10       |
|    time_elapsed    | 29236    |
|    total timesteps | 319719   |
| train/             |          |
|    actor_loss      | 3.17     |
|    critic_loss     | 0.802    |
|    learning_rate   | 0.0005   |
|    n_updates       | 317718   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-165.31 +/- 121.74
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 3.69     |
|    critic_loss     | 0.409    |
|    learning_rate   | 0.0005   |
|    n_updates       | 317999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -79.9    |
| time/              |          |
|    episodes        | 164      |
|    fps             | 10       |
|    time_elapsed    | 29996    |
|    total timesteps | 327723   |
| train/             |          |
|    actor_loss      | 3.66     |
|    critic_loss     | 0.589    |
|    learning_rate   | 0.0005   |
|    n_updates       | 325722   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-167.52 +/- 85.76
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | 3.16     |
|    critic_loss     | 37.2     |
|    learning_rate   | 0.0005   |
|    n_updates       | 327999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -69.9    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 10       |
|    time_elapsed    | 30752    |
|    total timesteps | 335727   |
| train/             |          |
|    actor_loss      | 3.79     |
|    critic_loss     | 19.1     |
|    learning_rate   | 0.0005   |
|    n_updates       | 333726   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-29.74 +/- 139.51
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29.7    |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | 3.55     |
|    critic_loss     | 0.613    |
|    learning_rate   | 0.0005   |
|    n_updates       | 337999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -63.3    |
| time/              |          |
|    episodes        | 172      |
|    fps             | 10       |
|    time_elapsed    | 31505    |
|    total timesteps | 343731   |
| train/             |          |
|    actor_loss      | 3.45     |
|    critic_loss     | 0.691    |
|    learning_rate   | 0.0005   |
|    n_updates       | 341730   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-140.39 +/- 195.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | 3.28     |
|    critic_loss     | 36.9     |
|    learning_rate   | 0.0005   |
|    n_updates       | 347999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -59.6    |
| time/              |          |
|    episodes        | 176      |
|    fps             | 10       |
|    time_elapsed    | 32260    |
|    total timesteps | 351735   |
| train/             |          |
|    actor_loss      | 3.16     |
|    critic_loss     | 18.6     |
|    learning_rate   | 0.0005   |
|    n_updates       | 349734   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -53.7    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 10       |
|    time_elapsed    | 32895    |
|    total timesteps | 359739   |
| train/             |          |
|    actor_loss      | 3.02     |
|    critic_loss     | 0.5      |
|    learning_rate   | 0.0005   |
|    n_updates       | 357738   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-6.58 +/- 50.26
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -6.58    |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | 3.51     |
|    critic_loss     | 0.446    |
|    learning_rate   | 0.0005   |
|    n_updates       | 357999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -51.3    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 10       |
|    time_elapsed    | 33653    |
|    total timesteps | 367743   |
| train/             |          |
|    actor_loss      | 2.98     |
|    critic_loss     | 0.399    |
|    learning_rate   | 0.0005   |
|    n_updates       | 365742   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-164.34 +/- 107.96
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | 3.42     |
|    critic_loss     | 0.272    |
|    learning_rate   | 0.0005   |
|    n_updates       | 367999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -49      |
| time/              |          |
|    episodes        | 188      |
|    fps             | 10       |
|    time_elapsed    | 34406    |
|    total timesteps | 375747   |
| train/             |          |
|    actor_loss      | 3.73     |
|    critic_loss     | 18.8     |
|    learning_rate   | 0.0005   |
|    n_updates       | 373746   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-47.29 +/- 146.39
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -47.3    |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | 3.3      |
|    critic_loss     | 0.656    |
|    learning_rate   | 0.0005   |
|    n_updates       | 377999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -44.3    |
| time/              |          |
|    episodes        | 192      |
|    fps             | 10       |
|    time_elapsed    | 35160    |
|    total timesteps | 383751   |
| train/             |          |
|    actor_loss      | 3.47     |
|    critic_loss     | 0.446    |
|    learning_rate   | 0.0005   |
|    n_updates       | 381750   |
---------------------------------
Eval num_timesteps=390000, episode_reward=-124.55 +/- 108.16
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | 3.59     |
|    critic_loss     | 36.6     |
|    learning_rate   | 0.0005   |
|    n_updates       | 387999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -44.2    |
| time/              |          |
|    episodes        | 196      |
|    fps             | 10       |
|    time_elapsed    | 35913    |
|    total timesteps | 391755   |
| train/             |          |
|    actor_loss      | 3.58     |
|    critic_loss     | 0.398    |
|    learning_rate   | 0.0005   |
|    n_updates       | 389754   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -51.7    |
| time/              |          |
|    episodes        | 200      |
|    fps             | 10       |
|    time_elapsed    | 36547    |
|    total timesteps | 399759   |
| train/             |          |
|    actor_loss      | 3.83     |
|    critic_loss     | 0.664    |
|    learning_rate   | 0.0005   |
|    n_updates       | 397758   |
---------------------------------
Eval num_timesteps=400000, episode_reward=-120.45 +/- 107.62
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | 3.33     |
|    critic_loss     | 36.8     |
|    learning_rate   | 0.0005   |
|    n_updates       | 397999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -51      |
| time/              |          |
|    episodes        | 204      |
|    fps             | 10       |
|    time_elapsed    | 37308    |
|    total timesteps | 407763   |
| train/             |          |
|    actor_loss      | 3.46     |
|    critic_loss     | 0.422    |
|    learning_rate   | 0.0005   |
|    n_updates       | 405762   |
---------------------------------
Eval num_timesteps=410000, episode_reward=-223.99 +/- 66.00
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -224     |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | 3.66     |
|    critic_loss     | 0.629    |
|    learning_rate   | 0.0005   |
|    n_updates       | 407999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -49.4    |
| time/              |          |
|    episodes        | 208      |
|    fps             | 10       |
|    time_elapsed    | 38063    |
|    total timesteps | 415767   |
| train/             |          |
|    actor_loss      | 3.38     |
|    critic_loss     | 0.308    |
|    learning_rate   | 0.0005   |
|    n_updates       | 413766   |
---------------------------------
Eval num_timesteps=420000, episode_reward=-137.78 +/- 134.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | 3.36     |
|    critic_loss     | 18.9     |
|    learning_rate   | 0.0005   |
|    n_updates       | 417999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -47.1    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 10       |
|    time_elapsed    | 38819    |
|    total timesteps | 423771   |
| train/             |          |
|    actor_loss      | 3.47     |
|    critic_loss     | 36.6     |
|    learning_rate   | 0.0005   |
|    n_updates       | 421770   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-36.35 +/- 61.82
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -36.3    |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | 3.78     |
|    critic_loss     | 0.586    |
|    learning_rate   | 0.0005   |
|    n_updates       | 427999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -53.9    |
| time/              |          |
|    episodes        | 216      |
|    fps             | 10       |
|    time_elapsed    | 39584    |
|    total timesteps | 431775   |
| train/             |          |
|    actor_loss      | 3.48     |
|    critic_loss     | 0.355    |
|    learning_rate   | 0.0005   |
|    n_updates       | 429774   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -55.2    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 10       |
|    time_elapsed    | 40219    |
|    total timesteps | 439779   |
| train/             |          |
|    actor_loss      | 3.47     |
|    critic_loss     | 0.559    |
|    learning_rate   | 0.0005   |
|    n_updates       | 437778   |
---------------------------------
Eval num_timesteps=440000, episode_reward=-213.38 +/- 109.65
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | 3.31     |
|    critic_loss     | 0.563    |
|    learning_rate   | 0.0005   |
|    n_updates       | 437999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -55.3    |
| time/              |          |
|    episodes        | 224      |
|    fps             | 10       |
|    time_elapsed    | 40976    |
|    total timesteps | 447783   |
| train/             |          |
|    actor_loss      | 3.47     |
|    critic_loss     | 18.7     |
|    learning_rate   | 0.0005   |
|    n_updates       | 445782   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-95.48 +/- 64.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -95.5    |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | 3.51     |
|    critic_loss     | 0.429    |
|    learning_rate   | 0.0005   |
|    n_updates       | 447999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -57.4    |
| time/              |          |
|    episodes        | 228      |
|    fps             | 10       |
|    time_elapsed    | 41701    |
|    total timesteps | 454994   |
| train/             |          |
|    actor_loss      | 3.44     |
|    critic_loss     | 0.308    |
|    learning_rate   | 0.0005   |
|    n_updates       | 452993   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-57.38 +/- 49.89
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -57.4    |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | 3.66     |
|    critic_loss     | 0.424    |
|    learning_rate   | 0.0005   |
|    n_updates       | 457999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -64.2    |
| time/              |          |
|    episodes        | 232      |
|    fps             | 10       |
|    time_elapsed    | 42460    |
|    total timesteps | 462998   |
| train/             |          |
|    actor_loss      | 3.74     |
|    critic_loss     | 0.662    |
|    learning_rate   | 0.0005   |
|    n_updates       | 460997   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-102.39 +/- 103.62
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -102     |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | 3.76     |
|    critic_loss     | 0.382    |
|    learning_rate   | 0.0005   |
|    n_updates       | 467999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -78.2    |
| time/              |          |
|    episodes        | 236      |
|    fps             | 10       |
|    time_elapsed    | 43220    |
|    total timesteps | 471002   |
| train/             |          |
|    actor_loss      | 3.46     |
|    critic_loss     | 18.6     |
|    learning_rate   | 0.0005   |
|    n_updates       | 469001   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -76      |
| time/              |          |
|    episodes        | 240      |
|    fps             | 10       |
|    time_elapsed    | 43856    |
|    total timesteps | 479006   |
| train/             |          |
|    actor_loss      | 3.67     |
|    critic_loss     | 0.53     |
|    learning_rate   | 0.0005   |
|    n_updates       | 477005   |
---------------------------------
Eval num_timesteps=480000, episode_reward=-27.14 +/- 87.49
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27.1    |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | 3.7      |
|    critic_loss     | 0.567    |
|    learning_rate   | 0.0005   |
|    n_updates       | 477999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -70.4    |
| time/              |          |
|    episodes        | 244      |
|    fps             | 10       |
|    time_elapsed    | 44617    |
|    total timesteps | 487010   |
| train/             |          |
|    actor_loss      | 3.88     |
|    critic_loss     | 0.597    |
|    learning_rate   | 0.0005   |
|    n_updates       | 485009   |
---------------------------------
Eval num_timesteps=490000, episode_reward=-90.62 +/- 94.86
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -90.6    |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | 3.76     |
|    critic_loss     | 0.359    |
|    learning_rate   | 0.0005   |
|    n_updates       | 487999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -67.8    |
| time/              |          |
|    episodes        | 248      |
|    fps             | 10       |
|    time_elapsed    | 45374    |
|    total timesteps | 495014   |
| train/             |          |
|    actor_loss      | 4.01     |
|    critic_loss     | 36.9     |
|    learning_rate   | 0.0005   |
|    n_updates       | 493013   |
---------------------------------
Eval num_timesteps=500000, episode_reward=-75.91 +/- 125.51
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -75.9    |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | 3.65     |
|    critic_loss     | 0.534    |
|    learning_rate   | 0.0005   |
|    n_updates       | 497999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -66.6    |
| time/              |          |
|    episodes        | 252      |
|    fps             | 10       |
|    time_elapsed    | 46188    |
|    total timesteps | 503018   |
| train/             |          |
|    actor_loss      | 3.6      |
|    critic_loss     | 0.374    |
|    learning_rate   | 0.0005   |
|    n_updates       | 501017   |
---------------------------------
Eval num_timesteps=510000, episode_reward=-155.69 +/- 85.96
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | 3.51     |
|    critic_loss     | 0.371    |
|    learning_rate   | 0.0005   |
|    n_updates       | 507999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -66.8    |
| time/              |          |
|    episodes        | 256      |
|    fps             | 10       |
|    time_elapsed    | 46945    |
|    total timesteps | 511022   |
| train/             |          |
|    actor_loss      | 3.76     |
|    critic_loss     | 0.648    |
|    learning_rate   | 0.0005   |
|    n_updates       | 509021   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -71.3    |
| time/              |          |
|    episodes        | 260      |
|    fps             | 10       |
|    time_elapsed    | 47582    |
|    total timesteps | 519026   |
| train/             |          |
|    actor_loss      | 3.68     |
|    critic_loss     | 0.41     |
|    learning_rate   | 0.0005   |
|    n_updates       | 517025   |
---------------------------------
Eval num_timesteps=520000, episode_reward=-47.72 +/- 104.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -47.7    |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | 3.49     |
|    critic_loss     | 0.435    |
|    learning_rate   | 0.0005   |
|    n_updates       | 517999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -76.5    |
| time/              |          |
|    episodes        | 264      |
|    fps             | 10       |
|    time_elapsed    | 48343    |
|    total timesteps | 527030   |
| train/             |          |
|    actor_loss      | 3.61     |
|    critic_loss     | 0.495    |
|    learning_rate   | 0.0005   |
|    n_updates       | 525029   |
---------------------------------
Eval num_timesteps=530000, episode_reward=-86.49 +/- 107.10
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -86.5    |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | 3.75     |
|    critic_loss     | 18.5     |
|    learning_rate   | 0.0005   |
|    n_updates       | 527999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -81      |
| time/              |          |
|    episodes        | 268      |
|    fps             | 10       |
|    time_elapsed    | 49104    |
|    total timesteps | 535034   |
| train/             |          |
|    actor_loss      | 3.75     |
|    critic_loss     | 0.444    |
|    learning_rate   | 0.0005   |
|    n_updates       | 533033   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-11.85 +/- 125.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -11.9    |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | 3.84     |
|    critic_loss     | 0.494    |
|    learning_rate   | 0.0005   |
|    n_updates       | 537999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -90.2    |
| time/              |          |
|    episodes        | 272      |
|    fps             | 10       |
|    time_elapsed    | 49872    |
|    total timesteps | 543038   |
| train/             |          |
|    actor_loss      | 3.75     |
|    critic_loss     | 0.41     |
|    learning_rate   | 0.0005   |
|    n_updates       | 541037   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-70.84 +/- 92.30
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -70.8    |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | 3.79     |
|    critic_loss     | 0.664    |
|    learning_rate   | 0.0005   |
|    n_updates       | 547999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -94.3    |
| time/              |          |
|    episodes        | 276      |
|    fps             | 10       |
|    time_elapsed    | 50637    |
|    total timesteps | 551042   |
| train/             |          |
|    actor_loss      | 4.01     |
|    critic_loss     | 0.909    |
|    learning_rate   | 0.0005   |
|    n_updates       | 549041   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -98.3    |
| time/              |          |
|    episodes        | 280      |
|    fps             | 10       |
|    time_elapsed    | 51281    |
|    total timesteps | 559046   |
| train/             |          |
|    actor_loss      | 3.54     |
|    critic_loss     | 0.454    |
|    learning_rate   | 0.0005   |
|    n_updates       | 557045   |
---------------------------------
Eval num_timesteps=560000, episode_reward=-122.56 +/- 98.48
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | 3.64     |
|    critic_loss     | 0.394    |
|    learning_rate   | 0.0005   |
|    n_updates       | 557999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -91.9    |
| time/              |          |
|    episodes        | 284      |
|    fps             | 10       |
|    time_elapsed    | 52052    |
|    total timesteps | 567050   |
| train/             |          |
|    actor_loss      | 3.61     |
|    critic_loss     | 18.8     |
|    learning_rate   | 0.0005   |
|    n_updates       | 565049   |
---------------------------------
Eval num_timesteps=570000, episode_reward=-94.21 +/- 188.54
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -94.2    |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | 4.01     |
|    critic_loss     | 36.8     |
|    learning_rate   | 0.0005   |
|    n_updates       | 567999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -90.6    |
| time/              |          |
|    episodes        | 288      |
|    fps             | 10       |
|    time_elapsed    | 52826    |
|    total timesteps | 575054   |
| train/             |          |
|    actor_loss      | 3.72     |
|    critic_loss     | 0.497    |
|    learning_rate   | 0.0005   |
|    n_updates       | 573053   |
---------------------------------
