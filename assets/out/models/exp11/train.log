2021-12-08 21:16:01.257138: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-08 21:16:01.257218: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp11/TD3_26
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 222      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 22       |
|    time_elapsed    | 360      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 0.0525   |
|    learning_rate   | 0.0005   |
|    n_updates       | 7003     |
---------------------------------
Eval num_timesteps=10000, episode_reward=163.99 +/- 3.12
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 164      |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -1.74    |
|    critic_loss     | 0.117    |
|    learning_rate   | 0.0005   |
|    n_updates       | 8999     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 211      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 18       |
|    time_elapsed    | 871      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | -2.34    |
|    critic_loss     | 0.073    |
|    learning_rate   | 0.0005   |
|    n_updates       | 15007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=164.81 +/- 3.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -2.64    |
|    critic_loss     | 0.0755   |
|    learning_rate   | 0.0005   |
|    n_updates       | 18999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 207      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 17       |
|    time_elapsed    | 1378     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | -3.1     |
|    critic_loss     | 0.0978   |
|    learning_rate   | 0.0005   |
|    n_updates       | 23011    |
---------------------------------
Eval num_timesteps=30000, episode_reward=165.20 +/- 2.08
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -3.36    |
|    critic_loss     | 0.0541   |
|    learning_rate   | 0.0005   |
|    n_updates       | 28999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 209      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 16       |
|    time_elapsed    | 1887     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | -3.49    |
|    critic_loss     | 0.0518   |
|    learning_rate   | 0.0005   |
|    n_updates       | 31015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=160.61 +/- 1.50
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -3.65    |
|    critic_loss     | 0.0526   |
|    learning_rate   | 0.0005   |
|    n_updates       | 38999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 203      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 16       |
|    time_elapsed    | 2398     |
|    total timesteps | 40020    |
| train/             |          |
|    actor_loss      | -3.7     |
|    critic_loss     | 0.0453   |
|    learning_rate   | 0.0005   |
|    n_updates       | 39019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 204      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 17       |
|    time_elapsed    | 2795     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | -3.99    |
|    critic_loss     | 0.0406   |
|    learning_rate   | 0.0005   |
|    n_updates       | 47023    |
---------------------------------
Eval num_timesteps=50000, episode_reward=166.33 +/- 3.78
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -3.94    |
|    critic_loss     | 0.0833   |
|    learning_rate   | 0.0005   |
|    n_updates       | 48999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 205      |
| time/              |          |
|    episodes        | 28       |
|    fps             | 16       |
|    time_elapsed    | 3304     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | -4.08    |
|    critic_loss     | 0.0404   |
|    learning_rate   | 0.0005   |
|    n_updates       | 55027    |
---------------------------------
Eval num_timesteps=60000, episode_reward=161.79 +/- 3.17
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -4.21    |
|    critic_loss     | 0.0365   |
|    learning_rate   | 0.0005   |
|    n_updates       | 58999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 202      |
| time/              |          |
|    episodes        | 32       |
|    fps             | 16       |
|    time_elapsed    | 3812     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | -4.16    |
|    critic_loss     | 0.0322   |
|    learning_rate   | 0.0005   |
|    n_updates       | 63031    |
---------------------------------
Eval num_timesteps=70000, episode_reward=164.73 +/- 2.60
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -4.12    |
|    critic_loss     | 0.0686   |
|    learning_rate   | 0.0005   |
|    n_updates       | 68999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 202      |
| time/              |          |
|    episodes        | 36       |
|    fps             | 16       |
|    time_elapsed    | 4321     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | -4.19    |
|    critic_loss     | 0.0497   |
|    learning_rate   | 0.0005   |
|    n_updates       | 71035    |
---------------------------------
Eval num_timesteps=80000, episode_reward=167.07 +/- 1.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -4.37    |
|    critic_loss     | 0.0232   |
|    learning_rate   | 0.0005   |
|    n_updates       | 78999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 202      |
| time/              |          |
|    episodes        | 40       |
|    fps             | 16       |
|    time_elapsed    | 4833     |
|    total timesteps | 80040    |
| train/             |          |
|    actor_loss      | -4.37    |
|    critic_loss     | 0.0254   |
|    learning_rate   | 0.0005   |
|    n_updates       | 79039    |
---------------------------------
Terminated
2021-12-08 22:43:58.834677: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-08 22:43:58.834742: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp11/TD3_27
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.5     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 39       |
|    time_elapsed    | 202      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 1.65     |
|    critic_loss     | 0.0509   |
|    learning_rate   | 0.0005   |
|    n_updates       | 3003     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-2.08 +/- 0.53
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.57     |
|    critic_loss     | 0.054    |
|    learning_rate   | 0.0005   |
|    n_updates       | 4999     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.6     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 22       |
|    time_elapsed    | 715      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 1.34     |
|    critic_loss     | 0.039    |
|    learning_rate   | 0.0005   |
|    n_updates       | 11007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-2.64 +/- 1.33
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.64    |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 1.11     |
|    critic_loss     | 0.0254   |
|    learning_rate   | 0.0005   |
|    n_updates       | 14999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.57    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 19       |
|    time_elapsed    | 1227     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 1.01     |
|    critic_loss     | 0.00763  |
|    learning_rate   | 0.0005   |
|    n_updates       | 19011    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-2.48 +/- 0.59
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.48    |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 0.708    |
|    critic_loss     | 0.0295   |
|    learning_rate   | 0.0005   |
|    n_updates       | 24999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.08    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 18       |
|    time_elapsed    | 1733     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 0.661    |
|    critic_loss     | 0.0224   |
|    learning_rate   | 0.0005   |
|    n_updates       | 27015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-3.20 +/- 1.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.695    |
|    critic_loss     | 0.00946  |
|    learning_rate   | 0.0005   |
|    n_updates       | 34999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.9     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 17       |
|    time_elapsed    | 2241     |
|    total timesteps | 40020    |
| train/             |          |
|    actor_loss      | 0.682    |
|    critic_loss     | 0.0074   |
|    learning_rate   | 0.0005   |
|    n_updates       | 35019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.57    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 18       |
|    time_elapsed    | 2634     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.395    |
|    critic_loss     | 0.022    |
|    learning_rate   | 0.0005   |
|    n_updates       | 43023    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-2.08 +/- 0.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.388    |
|    critic_loss     | 0.0199   |
|    learning_rate   | 0.0005   |
|    n_updates       | 44999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.69    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 17       |
|    time_elapsed    | 3141     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 0.394    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0005   |
|    n_updates       | 51027    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-2.24 +/- 0.93
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.24    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.43     |
|    critic_loss     | 0.00178  |
|    learning_rate   | 0.0005   |
|    n_updates       | 54999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.56    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 17       |
|    time_elapsed    | 3648     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 0.501    |
|    critic_loss     | 0.00788  |
|    learning_rate   | 0.0005   |
|    n_updates       | 59031    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-2.08 +/- 0.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 0.335    |
|    critic_loss     | 0.00992  |
|    learning_rate   | 0.0005   |
|    n_updates       | 64999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.46    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 17       |
|    time_elapsed    | 4154     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 0.43     |
|    critic_loss     | 0.024    |
|    learning_rate   | 0.0005   |
|    n_updates       | 67035    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-2.48 +/- 0.69
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.48    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.511    |
|    critic_loss     | 0.0203   |
|    learning_rate   | 0.0005   |
|    n_updates       | 74999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.4     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 17       |
|    time_elapsed    | 4663     |
|    total timesteps | 80040    |
| train/             |          |
|    actor_loss      | 0.44     |
|    critic_loss     | 0.00606  |
|    learning_rate   | 0.0005   |
|    n_updates       | 75039    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.41    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 17       |
|    time_elapsed    | 5055     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 0.268    |
|    critic_loss     | 0.00457  |
|    learning_rate   | 0.0005   |
|    n_updates       | 83043    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-2.24 +/- 0.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.24    |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 0.324    |
|    critic_loss     | 0.00994  |
|    learning_rate   | 0.0005   |
|    n_updates       | 84999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.34    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 17       |
|    time_elapsed    | 5560     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 0.407    |
|    critic_loss     | 0.0233   |
|    learning_rate   | 0.0005   |
|    n_updates       | 91047    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-2.88 +/- 0.85
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.88    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.403    |
|    critic_loss     | 0.0121   |
|    learning_rate   | 0.0005   |
|    n_updates       | 94999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.39    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 17       |
|    time_elapsed    | 6061     |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | 0.349    |
|    critic_loss     | 0.00983  |
|    learning_rate   | 0.0005   |
|    n_updates       | 99051    |
---------------------------------
Eval num_timesteps=110000, episode_reward=-2.64 +/- 0.60
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.64    |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 0.279    |
|    critic_loss     | 0.00481  |
|    learning_rate   | 0.0005   |
|    n_updates       | 104999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.39    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 17       |
|    time_elapsed    | 6568     |
|    total timesteps | 112056   |
| train/             |          |
|    actor_loss      | 0.299    |
|    critic_loss     | 0.00118  |
|    learning_rate   | 0.0005   |
|    n_updates       | 107055   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-2.72 +/- 1.02
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.72    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 0.35     |
|    critic_loss     | 0.00801  |
|    learning_rate   | 0.0005   |
|    n_updates       | 114999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.37    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 16       |
|    time_elapsed    | 7067     |
|    total timesteps | 120060   |
| train/             |          |
|    actor_loss      | 0.326    |
|    critic_loss     | 0.00275  |
|    learning_rate   | 0.0005   |
|    n_updates       | 115059   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.38    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 17       |
|    time_elapsed    | 7455     |
|    total timesteps | 128064   |
| train/             |          |
|    actor_loss      | 0.303    |
|    critic_loss     | 0.00124  |
|    learning_rate   | 0.0005   |
|    n_updates       | 123063   |
---------------------------------
Terminated
2021-12-09 00:50:35.782690: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-09 00:50:35.782758: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp11/TD3_30
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.19    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 38       |
|    time_elapsed    | 209      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 1.16     |
|    critic_loss     | 0.0673   |
|    learning_rate   | 0.0005   |
|    n_updates       | 3003     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-1.88 +/- 0.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.88    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.36     |
|    critic_loss     | 0.0721   |
|    learning_rate   | 0.0005   |
|    n_updates       | 4999     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.13    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 21       |
|    time_elapsed    | 733      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 0.863    |
|    critic_loss     | 0.0434   |
|    learning_rate   | 0.0005   |
|    n_updates       | 11007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-1.63 +/- 0.11
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.63    |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 0.884    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.0005   |
|    n_updates       | 14999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 12       |
|    fps             | 19       |
|    time_elapsed    | 1256     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 0.783    |
|    critic_loss     | 0.00437  |
|    learning_rate   | 0.0005   |
|    n_updates       | 19011    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-1.78 +/- 0.24
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.78    |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 0.678    |
|    critic_loss     | 0.0177   |
|    learning_rate   | 0.0005   |
|    n_updates       | 24999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.93    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 17       |
|    time_elapsed    | 1787     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 0.706    |
|    critic_loss     | 0.0161   |
|    learning_rate   | 0.0005   |
|    n_updates       | 27015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-2.08 +/- 0.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.602    |
|    critic_loss     | 0.00965  |
|    learning_rate   | 0.0005   |
|    n_updates       | 34999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.97    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 17       |
|    time_elapsed    | 2318     |
|    total timesteps | 40020    |
| train/             |          |
|    actor_loss      | 0.882    |
|    critic_loss     | 0.0713   |
|    learning_rate   | 0.0005   |
|    n_updates       | 35019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 17       |
|    time_elapsed    | 2720     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.517    |
|    critic_loss     | 0.00181  |
|    learning_rate   | 0.0005   |
|    n_updates       | 43023    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-1.82 +/- 0.13
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.82    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.719    |
|    critic_loss     | 0.0433   |
|    learning_rate   | 0.0005   |
|    n_updates       | 44999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.97    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 17       |
|    time_elapsed    | 3253     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 0.508    |
|    critic_loss     | 0.00281  |
|    learning_rate   | 0.0005   |
|    n_updates       | 51027    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-2.11 +/- 0.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.11    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.778    |
|    critic_loss     | 0.0926   |
|    learning_rate   | 0.0005   |
|    n_updates       | 54999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.97    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 16       |
|    time_elapsed    | 3780     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 0.476    |
|    critic_loss     | 0.000796 |
|    learning_rate   | 0.0005   |
|    n_updates       | 59031    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-1.96 +/- 0.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.96    |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 0.439    |
|    critic_loss     | 0.0237   |
|    learning_rate   | 0.0005   |
|    n_updates       | 64999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 16       |
|    time_elapsed    | 4302     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 0.437    |
|    critic_loss     | 0.00914  |
|    learning_rate   | 0.0005   |
|    n_updates       | 67035    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-2.04 +/- 0.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.04    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.387    |
|    critic_loss     | 0.00182  |
|    learning_rate   | 0.0005   |
|    n_updates       | 74999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 40       |
|    fps             | 16       |
|    time_elapsed    | 4824     |
|    total timesteps | 80040    |
| train/             |          |
|    actor_loss      | 0.392    |
|    critic_loss     | 0.0239   |
|    learning_rate   | 0.0005   |
|    n_updates       | 75039    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.99    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 16       |
|    time_elapsed    | 5227     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 0.362    |
|    critic_loss     | 0.00317  |
|    learning_rate   | 0.0005   |
|    n_updates       | 83043    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-1.90 +/- 0.38
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.9     |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 0.339    |
|    critic_loss     | 0.00718  |
|    learning_rate   | 0.0005   |
|    n_updates       | 84999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.95    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 16       |
|    time_elapsed    | 5749     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 0.401    |
|    critic_loss     | 0.00708  |
|    learning_rate   | 0.0005   |
|    n_updates       | 91047    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-1.86 +/- 0.40
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.86    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.39     |
|    critic_loss     | 0.00739  |
|    learning_rate   | 0.0005   |
|    n_updates       | 94999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 16       |
|    time_elapsed    | 6267     |
|    total timesteps | 103835   |
| train/             |          |
|    actor_loss      | 0.219    |
|    critic_loss     | 0.0189   |
|    learning_rate   | 0.0005   |
|    n_updates       | 98834    |
---------------------------------
Eval num_timesteps=110000, episode_reward=-1.98 +/- 0.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.98    |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 0.401    |
|    critic_loss     | 0.0606   |
|    learning_rate   | 0.0005   |
|    n_updates       | 104999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.95    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 16       |
|    time_elapsed    | 6795     |
|    total timesteps | 111839   |
| train/             |          |
|    actor_loss      | 0.319    |
|    critic_loss     | 0.000675 |
|    learning_rate   | 0.0005   |
|    n_updates       | 106838   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 16       |
|    time_elapsed    | 7199     |
|    total timesteps | 119843   |
| train/             |          |
|    actor_loss      | 0.333    |
|    critic_loss     | 0.00333  |
|    learning_rate   | 0.0005   |
|    n_updates       | 114842   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-1.82 +/- 0.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.82    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 0.281    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.0005   |
|    n_updates       | 114999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.89    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 16       |
|    time_elapsed    | 7723     |
|    total timesteps | 127847   |
| train/             |          |
|    actor_loss      | 0.313    |
|    critic_loss     | 0.0167   |
|    learning_rate   | 0.0005   |
|    n_updates       | 122846   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-1.98 +/- 0.18
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.98    |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 0.31     |
|    critic_loss     | 0.00568  |
|    learning_rate   | 0.0005   |
|    n_updates       | 124999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 16       |
|    time_elapsed    | 8245     |
|    total timesteps | 135851   |
| train/             |          |
|    actor_loss      | 0.198    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.0005   |
|    n_updates       | 130850   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-1.90 +/- 0.24
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.9     |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 0.505    |
|    critic_loss     | 0.0415   |
|    learning_rate   | 0.0005   |
|    n_updates       | 134999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.83    |
| time/              |          |
|    episodes        | 72       |
|    fps             | 16       |
|    time_elapsed    | 8737     |
|    total timesteps | 143232   |
| train/             |          |
|    actor_loss      | 0.306    |
|    critic_loss     | 0.0017   |
|    learning_rate   | 0.0005   |
|    n_updates       | 138231   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-1.84 +/- 0.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.84    |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 0.281    |
|    critic_loss     | 0.00121  |
|    learning_rate   | 0.0005   |
|    n_updates       | 144999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.84    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 16       |
|    time_elapsed    | 9260     |
|    total timesteps | 151236   |
| train/             |          |
|    actor_loss      | 0.21     |
|    critic_loss     | 0.022    |
|    learning_rate   | 0.0005   |
|    n_updates       | 146235   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 16       |
|    time_elapsed    | 9659     |
|    total timesteps | 159240   |
| train/             |          |
|    actor_loss      | 0.246    |
|    critic_loss     | 0.00538  |
|    learning_rate   | 0.0005   |
|    n_updates       | 154239   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-1.84 +/- 0.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.84    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 0.353    |
|    critic_loss     | 0.00129  |
|    learning_rate   | 0.0005   |
|    n_updates       | 154999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 84       |
|    fps             | 16       |
|    time_elapsed    | 10178    |
|    total timesteps | 167244   |
| train/             |          |
|    actor_loss      | 0.272    |
|    critic_loss     | 0.0359   |
|    learning_rate   | 0.0005   |
|    n_updates       | 162243   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-1.95 +/- 0.28
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.95    |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 0.307    |
|    critic_loss     | 0.000302 |
|    learning_rate   | 0.0005   |
|    n_updates       | 164999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.88    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 16       |
|    time_elapsed    | 10707    |
|    total timesteps | 175248   |
| train/             |          |
|    actor_loss      | 0.309    |
|    critic_loss     | 0.000588 |
|    learning_rate   | 0.0005   |
|    n_updates       | 170247   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-2.02 +/- 0.33
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.02    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 0.524    |
|    critic_loss     | 0.0541   |
|    learning_rate   | 0.0005   |
|    n_updates       | 174999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.89    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 16       |
|    time_elapsed    | 11236    |
|    total timesteps | 183252   |
| train/             |          |
|    actor_loss      | 0.372    |
|    critic_loss     | 0.00541  |
|    learning_rate   | 0.0005   |
|    n_updates       | 178251   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-2.22 +/- 0.13
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.22    |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 0.248    |
|    critic_loss     | 0.00915  |
|    learning_rate   | 0.0005   |
|    n_updates       | 184999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 16       |
|    time_elapsed    | 11766    |
|    total timesteps | 191256   |
| train/             |          |
|    actor_loss      | 0.504    |
|    critic_loss     | 0.0589   |
|    learning_rate   | 0.0005   |
|    n_updates       | 186255   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.93    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 16       |
|    time_elapsed    | 12171    |
|    total timesteps | 199260   |
| train/             |          |
|    actor_loss      | 0.327    |
|    critic_loss     | 0.027    |
|    learning_rate   | 0.0005   |
|    n_updates       | 194259   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-2.00 +/- 0.16
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2       |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 0.428    |
|    critic_loss     | 0.0148   |
|    learning_rate   | 0.0005   |
|    n_updates       | 194999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 104      |
|    fps             | 16       |
|    time_elapsed    | 12700    |
|    total timesteps | 207264   |
| train/             |          |
|    actor_loss      | 0.33     |
|    critic_loss     | 0.0192   |
|    learning_rate   | 0.0005   |
|    n_updates       | 202263   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-1.94 +/- 0.44
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.94    |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | 0.349    |
|    critic_loss     | 0.00361  |
|    learning_rate   | 0.0005   |
|    n_updates       | 204999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 108      |
|    fps             | 16       |
|    time_elapsed    | 13230    |
|    total timesteps | 215268   |
| train/             |          |
|    actor_loss      | 0.317    |
|    critic_loss     | 0.00538  |
|    learning_rate   | 0.0005   |
|    n_updates       | 210267   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-1.90 +/- 0.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.9     |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 0.232    |
|    critic_loss     | 0.00287  |
|    learning_rate   | 0.0005   |
|    n_updates       | 214999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 16       |
|    time_elapsed    | 13760    |
|    total timesteps | 223272   |
| train/             |          |
|    actor_loss      | 0.258    |
|    critic_loss     | 0.000945 |
|    learning_rate   | 0.0005   |
|    n_updates       | 218271   |
---------------------------------
Eval num_timesteps=230000, episode_reward=-2.16 +/- 0.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.16    |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 0.335    |
|    critic_loss     | 0.00948  |
|    learning_rate   | 0.0005   |
|    n_updates       | 224999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 16       |
|    time_elapsed    | 14288    |
|    total timesteps | 231276   |
| train/             |          |
|    actor_loss      | 0.225    |
|    critic_loss     | 0.00299  |
|    learning_rate   | 0.0005   |
|    n_updates       | 226275   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.89    |
| time/              |          |
|    episodes        | 120      |
|    fps             | 16       |
|    time_elapsed    | 14692    |
|    total timesteps | 239280   |
| train/             |          |
|    actor_loss      | 0.375    |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.0005   |
|    n_updates       | 234279   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-1.82 +/- 0.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.82    |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.277    |
|    critic_loss     | 0.00235  |
|    learning_rate   | 0.0005   |
|    n_updates       | 234999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.89    |
| time/              |          |
|    episodes        | 124      |
|    fps             | 16       |
|    time_elapsed    | 15214    |
|    total timesteps | 247284   |
| train/             |          |
|    actor_loss      | 0.281    |
|    critic_loss     | 0.000474 |
|    learning_rate   | 0.0005   |
|    n_updates       | 242283   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-1.78 +/- 0.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.78    |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | 0.287    |
|    critic_loss     | 0.00426  |
|    learning_rate   | 0.0005   |
|    n_updates       | 244999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 128      |
|    fps             | 16       |
|    time_elapsed    | 15736    |
|    total timesteps | 255288   |
| train/             |          |
|    actor_loss      | 0.333    |
|    critic_loss     | 0.00718  |
|    learning_rate   | 0.0005   |
|    n_updates       | 250287   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-2.02 +/- 0.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.02    |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 0.335    |
|    critic_loss     | 0.00611  |
|    learning_rate   | 0.0005   |
|    n_updates       | 254999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.88    |
| time/              |          |
|    episodes        | 132      |
|    fps             | 16       |
|    time_elapsed    | 16257    |
|    total timesteps | 263292   |
| train/             |          |
|    actor_loss      | 0.226    |
|    critic_loss     | 0.00296  |
|    learning_rate   | 0.0005   |
|    n_updates       | 258291   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-1.90 +/- 0.20
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.9     |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | 0.274    |
|    critic_loss     | 0.000517 |
|    learning_rate   | 0.0005   |
|    n_updates       | 264999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 136      |
|    fps             | 16       |
|    time_elapsed    | 16780    |
|    total timesteps | 271296   |
| train/             |          |
|    actor_loss      | 0.293    |
|    critic_loss     | 0.00184  |
|    learning_rate   | 0.0005   |
|    n_updates       | 266295   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.86    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 16       |
|    time_elapsed    | 17178    |
|    total timesteps | 279254   |
| train/             |          |
|    actor_loss      | 0.253    |
|    critic_loss     | 0.000284 |
|    learning_rate   | 0.0005   |
|    n_updates       | 274253   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-1.94 +/- 0.24
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.94    |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 0.226    |
|    critic_loss     | 0.0045   |
|    learning_rate   | 0.0005   |
|    n_updates       | 274999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 144      |
|    fps             | 16       |
|    time_elapsed    | 17700    |
|    total timesteps | 287258   |
| train/             |          |
|    actor_loss      | 0.27     |
|    critic_loss     | 0.00446  |
|    learning_rate   | 0.0005   |
|    n_updates       | 282257   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-1.88 +/- 0.18
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.88    |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | 0.26     |
|    critic_loss     | 0.00672  |
|    learning_rate   | 0.0005   |
|    n_updates       | 284999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 148      |
|    fps             | 16       |
|    time_elapsed    | 18231    |
|    total timesteps | 295262   |
| train/             |          |
|    actor_loss      | 0.271    |
|    critic_loss     | 0.000234 |
|    learning_rate   | 0.0005   |
|    n_updates       | 290261   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-1.94 +/- 0.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.94    |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 0.262    |
|    critic_loss     | 0.000359 |
|    learning_rate   | 0.0005   |
|    n_updates       | 294999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.93    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 16       |
|    time_elapsed    | 18764    |
|    total timesteps | 303266   |
| train/             |          |
|    actor_loss      | 0.307    |
|    critic_loss     | 0.00133  |
|    learning_rate   | 0.0005   |
|    n_updates       | 298265   |
---------------------------------
Eval num_timesteps=310000, episode_reward=-2.04 +/- 0.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.04    |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | 0.412    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.0005   |
|    n_updates       | 304999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 156      |
|    fps             | 16       |
|    time_elapsed    | 19294    |
|    total timesteps | 311270   |
| train/             |          |
|    actor_loss      | 0.387    |
|    critic_loss     | 0.00678  |
|    learning_rate   | 0.0005   |
|    n_updates       | 306269   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 16       |
|    time_elapsed    | 19698    |
|    total timesteps | 319274   |
| train/             |          |
|    actor_loss      | 0.294    |
|    critic_loss     | 0.018    |
|    learning_rate   | 0.0005   |
|    n_updates       | 314273   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-1.98 +/- 0.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.98    |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 0.286    |
|    critic_loss     | 0.00436  |
|    learning_rate   | 0.0005   |
|    n_updates       | 314999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 164      |
|    fps             | 16       |
|    time_elapsed    | 20226    |
|    total timesteps | 327278   |
| train/             |          |
|    actor_loss      | 0.244    |
|    critic_loss     | 0.00262  |
|    learning_rate   | 0.0005   |
|    n_updates       | 322277   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-1.66 +/- 0.28
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.66    |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | 0.309    |
|    critic_loss     | 0.00301  |
|    learning_rate   | 0.0005   |
|    n_updates       | 324999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 16       |
|    time_elapsed    | 20750    |
|    total timesteps | 335282   |
| train/             |          |
|    actor_loss      | 0.436    |
|    critic_loss     | 0.0254   |
|    learning_rate   | 0.0005   |
|    n_updates       | 330281   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-1.80 +/- 0.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.8     |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | 0.545    |
|    critic_loss     | 0.0567   |
|    learning_rate   | 0.0005   |
|    n_updates       | 334999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 172      |
|    fps             | 16       |
|    time_elapsed    | 21279    |
|    total timesteps | 343286   |
| train/             |          |
|    actor_loss      | 0.312    |
|    critic_loss     | 0.00271  |
|    learning_rate   | 0.0005   |
|    n_updates       | 338285   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-1.88 +/- 0.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.88    |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | 0.422    |
|    critic_loss     | 0.00591  |
|    learning_rate   | 0.0005   |
|    n_updates       | 344999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 176      |
|    fps             | 16       |
|    time_elapsed    | 21810    |
|    total timesteps | 351290   |
| train/             |          |
|    actor_loss      | 0.262    |
|    critic_loss     | 0.0284   |
|    learning_rate   | 0.0005   |
|    n_updates       | 346289   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 16       |
|    time_elapsed    | 22216    |
|    total timesteps | 359294   |
| train/             |          |
|    actor_loss      | 0.331    |
|    critic_loss     | 0.00208  |
|    learning_rate   | 0.0005   |
|    n_updates       | 354293   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-1.84 +/- 0.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.84    |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | 0.356    |
|    critic_loss     | 0.00441  |
|    learning_rate   | 0.0005   |
|    n_updates       | 354999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 16       |
|    time_elapsed    | 22742    |
|    total timesteps | 367298   |
| train/             |          |
|    actor_loss      | 0.399    |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.0005   |
|    n_updates       | 362297   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-1.92 +/- 0.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.92    |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | 0.378    |
|    critic_loss     | 0.00683  |
|    learning_rate   | 0.0005   |
|    n_updates       | 364999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 188      |
|    fps             | 16       |
|    time_elapsed    | 23266    |
|    total timesteps | 375302   |
| train/             |          |
|    actor_loss      | 0.262    |
|    critic_loss     | 0.00163  |
|    learning_rate   | 0.0005   |
|    n_updates       | 370301   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-1.79 +/- 0.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.79    |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | 0.465    |
|    critic_loss     | 0.0458   |
|    learning_rate   | 0.0005   |
|    n_updates       | 374999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 192      |
|    fps             | 16       |
|    time_elapsed    | 23795    |
|    total timesteps | 383306   |
| train/             |          |
|    actor_loss      | 0.225    |
|    critic_loss     | 0.00112  |
|    learning_rate   | 0.0005   |
|    n_updates       | 378305   |
---------------------------------
Eval num_timesteps=390000, episode_reward=-1.92 +/- 0.36
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.92    |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | 0.257    |
|    critic_loss     | 0.000348 |
|    learning_rate   | 0.0005   |
|    n_updates       | 384999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.88    |
| time/              |          |
|    episodes        | 196      |
|    fps             | 16       |
|    time_elapsed    | 24326    |
|    total timesteps | 391310   |
| train/             |          |
|    actor_loss      | 0.291    |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.0005   |
|    n_updates       | 386309   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 200      |
|    fps             | 16       |
|    time_elapsed    | 24733    |
|    total timesteps | 399314   |
| train/             |          |
|    actor_loss      | 0.274    |
|    critic_loss     | 0.000506 |
|    learning_rate   | 0.0005   |
|    n_updates       | 394313   |
---------------------------------
Eval num_timesteps=400000, episode_reward=-1.88 +/- 0.40
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.88    |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | 0.359    |
|    critic_loss     | 0.00879  |
|    learning_rate   | 0.0005   |
|    n_updates       | 394999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 204      |
|    fps             | 16       |
|    time_elapsed    | 25265    |
|    total timesteps | 407318   |
| train/             |          |
|    actor_loss      | 0.0953   |
|    critic_loss     | 0.0222   |
|    learning_rate   | 0.0005   |
|    n_updates       | 402317   |
---------------------------------
Eval num_timesteps=410000, episode_reward=-2.04 +/- 0.20
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.04    |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | 0.25     |
|    critic_loss     | 0.000961 |
|    learning_rate   | 0.0005   |
|    n_updates       | 404999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 208      |
|    fps             | 16       |
|    time_elapsed    | 25788    |
|    total timesteps | 415322   |
| train/             |          |
|    actor_loss      | 0.26     |
|    critic_loss     | 0.00647  |
|    learning_rate   | 0.0005   |
|    n_updates       | 410321   |
---------------------------------
Eval num_timesteps=420000, episode_reward=-1.99 +/- 0.26
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.99    |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | 0.204    |
|    critic_loss     | 0.0025   |
|    learning_rate   | 0.0005   |
|    n_updates       | 414999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 16       |
|    time_elapsed    | 26314    |
|    total timesteps | 423326   |
| train/             |          |
|    actor_loss      | 0.26     |
|    critic_loss     | 0.00659  |
|    learning_rate   | 0.0005   |
|    n_updates       | 418325   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-1.82 +/- 0.15
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.82    |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | 0.205    |
|    critic_loss     | 0.00549  |
|    learning_rate   | 0.0005   |
|    n_updates       | 424999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 216      |
|    fps             | 16       |
|    time_elapsed    | 26839    |
|    total timesteps | 431330   |
| train/             |          |
|    actor_loss      | 0.285    |
|    critic_loss     | 0.00928  |
|    learning_rate   | 0.0005   |
|    n_updates       | 426329   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.93    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 16       |
|    time_elapsed    | 27244    |
|    total timesteps | 439334   |
| train/             |          |
|    actor_loss      | 0.241    |
|    critic_loss     | 0.000848 |
|    learning_rate   | 0.0005   |
|    n_updates       | 434333   |
---------------------------------
Eval num_timesteps=440000, episode_reward=-1.84 +/- 0.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.84    |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | 0.208    |
|    critic_loss     | 0.00365  |
|    learning_rate   | 0.0005   |
|    n_updates       | 434999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.96    |
| time/              |          |
|    episodes        | 224      |
|    fps             | 16       |
|    time_elapsed    | 27765    |
|    total timesteps | 447338   |
| train/             |          |
|    actor_loss      | 0.244    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.0005   |
|    n_updates       | 442337   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-2.04 +/- 0.42
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.04    |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | 0.204    |
|    critic_loss     | 0.00257  |
|    learning_rate   | 0.0005   |
|    n_updates       | 444999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.96    |
| time/              |          |
|    episodes        | 228      |
|    fps             | 16       |
|    time_elapsed    | 28292    |
|    total timesteps | 455342   |
| train/             |          |
|    actor_loss      | 0.284    |
|    critic_loss     | 0.00699  |
|    learning_rate   | 0.0005   |
|    n_updates       | 450341   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-1.72 +/- 0.18
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.72    |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | 0.229    |
|    critic_loss     | 0.00435  |
|    learning_rate   | 0.0005   |
|    n_updates       | 454999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 232      |
|    fps             | 16       |
|    time_elapsed    | 28823    |
|    total timesteps | 463346   |
| train/             |          |
|    actor_loss      | 0.211    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.0005   |
|    n_updates       | 458345   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-1.76 +/- 0.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.76    |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | 0.247    |
|    critic_loss     | 0.000343 |
|    learning_rate   | 0.0005   |
|    n_updates       | 464999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 236      |
|    fps             | 16       |
|    time_elapsed    | 29351    |
|    total timesteps | 471350   |
| train/             |          |
|    actor_loss      | 0.283    |
|    critic_loss     | 0.00271  |
|    learning_rate   | 0.0005   |
|    n_updates       | 466349   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.96    |
| time/              |          |
|    episodes        | 240      |
|    fps             | 16       |
|    time_elapsed    | 29754    |
|    total timesteps | 479354   |
| train/             |          |
|    actor_loss      | 0.234    |
|    critic_loss     | 0.00575  |
|    learning_rate   | 0.0005   |
|    n_updates       | 474353   |
---------------------------------
Eval num_timesteps=480000, episode_reward=-1.82 +/- 0.26
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.82    |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | 0.279    |
|    critic_loss     | 0.0063   |
|    learning_rate   | 0.0005   |
|    n_updates       | 474999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 244      |
|    fps             | 16       |
|    time_elapsed    | 30283    |
|    total timesteps | 487358   |
| train/             |          |
|    actor_loss      | 0.33     |
|    critic_loss     | 0.0582   |
|    learning_rate   | 0.0005   |
|    n_updates       | 482357   |
---------------------------------
Eval num_timesteps=490000, episode_reward=-2.04 +/- 0.24
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.04    |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | 0.3      |
|    critic_loss     | 0.00337  |
|    learning_rate   | 0.0005   |
|    n_updates       | 484999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 248      |
|    fps             | 16       |
|    time_elapsed    | 30814    |
|    total timesteps | 495362   |
| train/             |          |
|    actor_loss      | 0.314    |
|    critic_loss     | 0.00122  |
|    learning_rate   | 0.0005   |
|    n_updates       | 490361   |
---------------------------------
Eval num_timesteps=500000, episode_reward=-1.72 +/- 0.24
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.72    |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | 0.254    |
|    critic_loss     | 0.000302 |
|    learning_rate   | 0.0005   |
|    n_updates       | 494999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 252      |
|    fps             | 16       |
|    time_elapsed    | 31343    |
|    total timesteps | 503366   |
| train/             |          |
|    actor_loss      | 0.434    |
|    critic_loss     | 0.157    |
|    learning_rate   | 0.0005   |
|    n_updates       | 498365   |
---------------------------------
Eval num_timesteps=510000, episode_reward=-1.90 +/- 0.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.9     |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | 0.252    |
|    critic_loss     | 0.00532  |
|    learning_rate   | 0.0005   |
|    n_updates       | 504999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.93    |
| time/              |          |
|    episodes        | 256      |
|    fps             | 16       |
|    time_elapsed    | 31870    |
|    total timesteps | 511370   |
| train/             |          |
|    actor_loss      | 0.227    |
|    critic_loss     | 0.00443  |
|    learning_rate   | 0.0005   |
|    n_updates       | 506369   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 260      |
|    fps             | 16       |
|    time_elapsed    | 32274    |
|    total timesteps | 519374   |
| train/             |          |
|    actor_loss      | 0.273    |
|    critic_loss     | 0.0038   |
|    learning_rate   | 0.0005   |
|    n_updates       | 514373   |
---------------------------------
Eval num_timesteps=520000, episode_reward=-1.86 +/- 0.48
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.86    |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | 0.284    |
|    critic_loss     | 0.00794  |
|    learning_rate   | 0.0005   |
|    n_updates       | 514999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.99    |
| time/              |          |
|    episodes        | 264      |
|    fps             | 16       |
|    time_elapsed    | 32805    |
|    total timesteps | 527378   |
| train/             |          |
|    actor_loss      | 0.0032   |
|    critic_loss     | 0.0636   |
|    learning_rate   | 0.0005   |
|    n_updates       | 522377   |
---------------------------------
Eval num_timesteps=530000, episode_reward=-1.62 +/- 0.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.62    |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | 0.178    |
|    critic_loss     | 0.0025   |
|    learning_rate   | 0.0005   |
|    n_updates       | 524999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 268      |
|    fps             | 16       |
|    time_elapsed    | 33328    |
|    total timesteps | 535382   |
| train/             |          |
|    actor_loss      | 0.251    |
|    critic_loss     | 0.0051   |
|    learning_rate   | 0.0005   |
|    n_updates       | 530381   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-2.00 +/- 0.11
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2       |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | 0.126    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.0005   |
|    n_updates       | 534999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 272      |
|    fps             | 16       |
|    time_elapsed    | 33850    |
|    total timesteps | 543386   |
| train/             |          |
|    actor_loss      | 0.17     |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.0005   |
|    n_updates       | 538385   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-1.86 +/- 0.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.86    |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | 0.224    |
|    critic_loss     | 0.000175 |
|    learning_rate   | 0.0005   |
|    n_updates       | 544999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 276      |
|    fps             | 16       |
|    time_elapsed    | 34379    |
|    total timesteps | 551390   |
| train/             |          |
|    actor_loss      | 0.259    |
|    critic_loss     | 0.000335 |
|    learning_rate   | 0.0005   |
|    n_updates       | 546389   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 280      |
|    fps             | 16       |
|    time_elapsed    | 34784    |
|    total timesteps | 559394   |
| train/             |          |
|    actor_loss      | 0.129    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.0005   |
|    n_updates       | 554393   |
---------------------------------
Eval num_timesteps=560000, episode_reward=-2.14 +/- 0.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.14    |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | 0.257    |
|    critic_loss     | 0.00342  |
|    learning_rate   | 0.0005   |
|    n_updates       | 554999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.03    |
| time/              |          |
|    episodes        | 284      |
|    fps             | 16       |
|    time_elapsed    | 35311    |
|    total timesteps | 567398   |
| train/             |          |
|    actor_loss      | 0.226    |
|    critic_loss     | 0.00278  |
|    learning_rate   | 0.0005   |
|    n_updates       | 562397   |
---------------------------------
Eval num_timesteps=570000, episode_reward=-2.16 +/- 0.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.16    |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | 0.209    |
|    critic_loss     | 0.000214 |
|    learning_rate   | 0.0005   |
|    n_updates       | 564999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.04    |
| time/              |          |
|    episodes        | 288      |
|    fps             | 16       |
|    time_elapsed    | 35843    |
|    total timesteps | 575402   |
| train/             |          |
|    actor_loss      | 0.359    |
|    critic_loss     | 0.0206   |
|    learning_rate   | 0.0005   |
|    n_updates       | 570401   |
---------------------------------
Eval num_timesteps=580000, episode_reward=-2.08 +/- 0.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | 0.31     |
|    critic_loss     | 0.00689  |
|    learning_rate   | 0.0005   |
|    n_updates       | 574999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.03    |
| time/              |          |
|    episodes        | 292      |
|    fps             | 16       |
|    time_elapsed    | 36370    |
|    total timesteps | 583406   |
| train/             |          |
|    actor_loss      | 0.327    |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.0005   |
|    n_updates       | 578405   |
---------------------------------
Eval num_timesteps=590000, episode_reward=-1.98 +/- 0.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.98    |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | 0.182    |
|    critic_loss     | 0.000825 |
|    learning_rate   | 0.0005   |
|    n_updates       | 584999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.03    |
| time/              |          |
|    episodes        | 296      |
|    fps             | 16       |
|    time_elapsed    | 36902    |
|    total timesteps | 591410   |
| train/             |          |
|    actor_loss      | 0.183    |
|    critic_loss     | 0.000994 |
|    learning_rate   | 0.0005   |
|    n_updates       | 586409   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 300      |
|    fps             | 16       |
|    time_elapsed    | 37307    |
|    total timesteps | 599414   |
| train/             |          |
|    actor_loss      | 0.174    |
|    critic_loss     | 0.002    |
|    learning_rate   | 0.0005   |
|    n_updates       | 594413   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-2.25 +/- 0.25
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.25    |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | 0.205    |
|    critic_loss     | 0.00418  |
|    learning_rate   | 0.0005   |
|    n_updates       | 594999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 304      |
|    fps             | 16       |
|    time_elapsed    | 37838    |
|    total timesteps | 607418   |
| train/             |          |
|    actor_loss      | 0.162    |
|    critic_loss     | 0.000986 |
|    learning_rate   | 0.0005   |
|    n_updates       | 602417   |
---------------------------------
Eval num_timesteps=610000, episode_reward=-1.93 +/- 0.30
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.93    |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | 0.21     |
|    critic_loss     | 0.000258 |
|    learning_rate   | 0.0005   |
|    n_updates       | 604999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.03    |
| time/              |          |
|    episodes        | 308      |
|    fps             | 16       |
|    time_elapsed    | 38372    |
|    total timesteps | 615422   |
| train/             |          |
|    actor_loss      | 0.211    |
|    critic_loss     | 0.0206   |
|    learning_rate   | 0.0005   |
|    n_updates       | 610421   |
---------------------------------
Eval num_timesteps=620000, episode_reward=-1.84 +/- 0.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.84    |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | 0.225    |
|    critic_loss     | 0.00454  |
|    learning_rate   | 0.0005   |
|    n_updates       | 614999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.01    |
| time/              |          |
|    episodes        | 312      |
|    fps             | 16       |
|    time_elapsed    | 38903    |
|    total timesteps | 623426   |
| train/             |          |
|    actor_loss      | 0.183    |
|    critic_loss     | 0.00014  |
|    learning_rate   | 0.0005   |
|    n_updates       | 618425   |
---------------------------------
Eval num_timesteps=630000, episode_reward=-1.82 +/- 0.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.82    |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | 0.181    |
|    critic_loss     | 0.00559  |
|    learning_rate   | 0.0005   |
|    n_updates       | 624999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 316      |
|    fps             | 16       |
|    time_elapsed    | 39431    |
|    total timesteps | 631430   |
| train/             |          |
|    actor_loss      | 0.23     |
|    critic_loss     | 0.00418  |
|    learning_rate   | 0.0005   |
|    n_updates       | 626429   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.05    |
| time/              |          |
|    episodes        | 320      |
|    fps             | 16       |
|    time_elapsed    | 39836    |
|    total timesteps | 639434   |
| train/             |          |
|    actor_loss      | 0.0725   |
|    critic_loss     | 0.0483   |
|    learning_rate   | 0.0005   |
|    n_updates       | 634433   |
---------------------------------
Eval num_timesteps=640000, episode_reward=-2.14 +/- 0.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.14    |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | 0.356    |
|    critic_loss     | 0.0153   |
|    learning_rate   | 0.0005   |
|    n_updates       | 634999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.01    |
| time/              |          |
|    episodes        | 324      |
|    fps             | 16       |
|    time_elapsed    | 40366    |
|    total timesteps | 647438   |
| train/             |          |
|    actor_loss      | 0.253    |
|    critic_loss     | 0.0173   |
|    learning_rate   | 0.0005   |
|    n_updates       | 642437   |
---------------------------------
Eval num_timesteps=650000, episode_reward=-1.92 +/- 0.38
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.92    |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | 0.268    |
|    critic_loss     | 0.00708  |
|    learning_rate   | 0.0005   |
|    n_updates       | 644999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 328      |
|    fps             | 16       |
|    time_elapsed    | 40898    |
|    total timesteps | 655442   |
| train/             |          |
|    actor_loss      | 0.168    |
|    critic_loss     | 0.00321  |
|    learning_rate   | 0.0005   |
|    n_updates       | 650441   |
---------------------------------
Eval num_timesteps=660000, episode_reward=-1.94 +/- 0.14
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.94    |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | 0.208    |
|    critic_loss     | 0.00277  |
|    learning_rate   | 0.0005   |
|    n_updates       | 654999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.01    |
| time/              |          |
|    episodes        | 332      |
|    fps             | 16       |
|    time_elapsed    | 41427    |
|    total timesteps | 663446   |
| train/             |          |
|    actor_loss      | 0.254    |
|    critic_loss     | 0.000329 |
|    learning_rate   | 0.0005   |
|    n_updates       | 658445   |
---------------------------------
Eval num_timesteps=670000, episode_reward=-2.06 +/- 0.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.06    |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | 0.202    |
|    critic_loss     | 0.0124   |
|    learning_rate   | 0.0005   |
|    n_updates       | 664999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.01    |
| time/              |          |
|    episodes        | 336      |
|    fps             | 16       |
|    time_elapsed    | 41945    |
|    total timesteps | 671450   |
| train/             |          |
|    actor_loss      | 0.284    |
|    critic_loss     | 0.00375  |
|    learning_rate   | 0.0005   |
|    n_updates       | 666449   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 340      |
|    fps             | 16       |
|    time_elapsed    | 42347    |
|    total timesteps | 679454   |
| train/             |          |
|    actor_loss      | 0.219    |
|    critic_loss     | 0.00586  |
|    learning_rate   | 0.0005   |
|    n_updates       | 674453   |
---------------------------------
Eval num_timesteps=680000, episode_reward=-2.04 +/- 0.15
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.04    |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | 0.404    |
|    critic_loss     | 0.0273   |
|    learning_rate   | 0.0005   |
|    n_updates       | 674999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 344      |
|    fps             | 16       |
|    time_elapsed    | 42867    |
|    total timesteps | 687458   |
| train/             |          |
|    actor_loss      | 0.399    |
|    critic_loss     | 0.0189   |
|    learning_rate   | 0.0005   |
|    n_updates       | 682457   |
---------------------------------
Eval num_timesteps=690000, episode_reward=-2.00 +/- 0.43
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2       |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | 0.319    |
|    critic_loss     | 0.00327  |
|    learning_rate   | 0.0005   |
|    n_updates       | 684999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 348      |
|    fps             | 16       |
|    time_elapsed    | 43390    |
|    total timesteps | 695462   |
| train/             |          |
|    actor_loss      | 0.0973   |
|    critic_loss     | 0.0243   |
|    learning_rate   | 0.0005   |
|    n_updates       | 690461   |
---------------------------------
Eval num_timesteps=700000, episode_reward=-2.00 +/- 0.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2       |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | 0.242    |
|    critic_loss     | 0.00183  |
|    learning_rate   | 0.0005   |
|    n_updates       | 694999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.99    |
| time/              |          |
|    episodes        | 352      |
|    fps             | 16       |
|    time_elapsed    | 43922    |
|    total timesteps | 703466   |
| train/             |          |
|    actor_loss      | 0.21     |
|    critic_loss     | 0.000773 |
|    learning_rate   | 0.0005   |
|    n_updates       | 698465   |
---------------------------------
Eval num_timesteps=710000, episode_reward=-1.94 +/- 0.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.94    |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | 0.229    |
|    critic_loss     | 0.00134  |
|    learning_rate   | 0.0005   |
|    n_updates       | 704999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.01    |
| time/              |          |
|    episodes        | 356      |
|    fps             | 16       |
|    time_elapsed    | 44450    |
|    total timesteps | 711470   |
| train/             |          |
|    actor_loss      | 0.111    |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.0005   |
|    n_updates       | 706469   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.98    |
| time/              |          |
|    episodes        | 360      |
|    fps             | 16       |
|    time_elapsed    | 44838    |
|    total timesteps | 719264   |
| train/             |          |
|    actor_loss      | 0.301    |
|    critic_loss     | 0.00306  |
|    learning_rate   | 0.0005   |
|    n_updates       | 714263   |
---------------------------------
Eval num_timesteps=720000, episode_reward=-1.88 +/- 0.33
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.88    |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.0005   |
|    n_updates       | 714999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.95    |
| time/              |          |
|    episodes        | 364      |
|    fps             | 16       |
|    time_elapsed    | 45361    |
|    total timesteps | 727268   |
| train/             |          |
|    actor_loss      | 0.282    |
|    critic_loss     | 0.00128  |
|    learning_rate   | 0.0005   |
|    n_updates       | 722267   |
---------------------------------
Eval num_timesteps=730000, episode_reward=-1.85 +/- 0.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.85    |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | 0.227    |
|    critic_loss     | 0.00192  |
|    learning_rate   | 0.0005   |
|    n_updates       | 724999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.97    |
| time/              |          |
|    episodes        | 368      |
|    fps             | 16       |
|    time_elapsed    | 45882    |
|    total timesteps | 735272   |
| train/             |          |
|    actor_loss      | 0.273    |
|    critic_loss     | 0.00247  |
|    learning_rate   | 0.0005   |
|    n_updates       | 730271   |
---------------------------------
Eval num_timesteps=740000, episode_reward=-2.11 +/- 0.17
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.11    |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | 0.287    |
|    critic_loss     | 0.000839 |
|    learning_rate   | 0.0005   |
|    n_updates       | 734999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.98    |
| time/              |          |
|    episodes        | 372      |
|    fps             | 16       |
|    time_elapsed    | 46412    |
|    total timesteps | 743276   |
| train/             |          |
|    actor_loss      | 0.238    |
|    critic_loss     | 0.00202  |
|    learning_rate   | 0.0005   |
|    n_updates       | 738275   |
---------------------------------
Eval num_timesteps=750000, episode_reward=-2.08 +/- 0.15
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | 0.176    |
|    critic_loss     | 0.00893  |
|    learning_rate   | 0.0005   |
|    n_updates       | 744999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.95    |
| time/              |          |
|    episodes        | 376      |
|    fps             | 16       |
|    time_elapsed    | 46942    |
|    total timesteps | 751280   |
| train/             |          |
|    actor_loss      | 0.294    |
|    critic_loss     | 0.0034   |
|    learning_rate   | 0.0005   |
|    n_updates       | 746279   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.96    |
| time/              |          |
|    episodes        | 380      |
|    fps             | 16       |
|    time_elapsed    | 47345    |
|    total timesteps | 759284   |
| train/             |          |
|    actor_loss      | 0.16     |
|    critic_loss     | 0.0251   |
|    learning_rate   | 0.0005   |
|    n_updates       | 754283   |
---------------------------------
Eval num_timesteps=760000, episode_reward=-2.08 +/- 0.15
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | 0.318    |
|    critic_loss     | 0.055    |
|    learning_rate   | 0.0005   |
|    n_updates       | 754999   |
---------------------------------
Terminated
2021-12-09 14:08:10.830194: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-09 14:08:10.830260: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp11/TD3_33
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 38       |
|    time_elapsed    | 206      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 1.03     |
|    critic_loss     | 3.9      |
|    learning_rate   | 0.0005   |
|    n_updates       | 3003     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-27.84 +/- 18.08
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27.8    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.34     |
|    critic_loss     | 2.02     |
|    learning_rate   | 0.0005   |
|    n_updates       | 4999     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -55.7    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 22       |
|    time_elapsed    | 727      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 1.17     |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 11007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-19.05 +/- 37.50
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19      |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 0.962    |
|    critic_loss     | 1.98     |
|    learning_rate   | 0.0005   |
|    n_updates       | 14999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -27.9    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 19       |
|    time_elapsed    | 1251     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 1.16     |
|    critic_loss     | 1.34     |
|    learning_rate   | 0.0005   |
|    n_updates       | 19011    |
---------------------------------
Eval num_timesteps=30000, episode_reward=3.39 +/- 12.07
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 3.39     |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 1.01     |
|    critic_loss     | 0.501    |
|    learning_rate   | 0.0005   |
|    n_updates       | 24999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -23      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 18       |
|    time_elapsed    | 1777     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 0.768    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 27015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-36.35 +/- 23.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -36.4    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.778    |
|    critic_loss     | 1.27     |
|    learning_rate   | 0.0005   |
|    n_updates       | 34999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -16.7    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 17       |
|    time_elapsed    | 2302     |
|    total timesteps | 40020    |
| train/             |          |
|    actor_loss      | 0.746    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 35019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -23.6    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 17       |
|    time_elapsed    | 2703     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.877    |
|    critic_loss     | 2.02     |
|    learning_rate   | 0.0005   |
|    n_updates       | 43023    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-27.65 +/- 44.87
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27.7    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.645    |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.0005   |
|    n_updates       | 44999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -23      |
| time/              |          |
|    episodes        | 28       |
|    fps             | 17       |
|    time_elapsed    | 3228     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 0.679    |
|    critic_loss     | 2.32     |
|    learning_rate   | 0.0005   |
|    n_updates       | 51027    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-11.57 +/- 38.02
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.638    |
|    critic_loss     | 1.18     |
|    learning_rate   | 0.0005   |
|    n_updates       | 54999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -14.8    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 17       |
|    time_elapsed    | 3752     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 0.492    |
|    critic_loss     | 1.99     |
|    learning_rate   | 0.0005   |
|    n_updates       | 59031    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-28.16 +/- 14.89
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -28.2    |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 0.585    |
|    critic_loss     | 1.32     |
|    learning_rate   | 0.0005   |
|    n_updates       | 64999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.64    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 16       |
|    time_elapsed    | 4273     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 0.385    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 67035    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-22.07 +/- 30.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -22.1    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.378    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 74999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.2     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 16       |
|    time_elapsed    | 4795     |
|    total timesteps | 80040    |
| train/             |          |
|    actor_loss      | 0.428    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 75039    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.14    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 16       |
|    time_elapsed    | 5195     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 0.377    |
|    critic_loss     | 0.427    |
|    learning_rate   | 0.0005   |
|    n_updates       | 83043    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-13.28 +/- 29.73
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -13.3    |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 0.473    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 84999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.71    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 16       |
|    time_elapsed    | 5715     |
|    total timesteps | 96024    |
| train/             |          |
|    actor_loss      | 0.512    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 91023    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-20.04 +/- 26.36
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -20      |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.315    |
|    critic_loss     | 0.765    |
|    learning_rate   | 0.0005   |
|    n_updates       | 94999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.97    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 16       |
|    time_elapsed    | 6240     |
|    total timesteps | 104028   |
| train/             |          |
|    actor_loss      | 0.211    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 99027    |
---------------------------------
