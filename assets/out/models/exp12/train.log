running build_ext
building 'mujoco_py.cymj' extension
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o -fopenmp -w
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py
gcc -pthread -shared /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mjpro150/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mjpro150/bin -lmujoco150 -lglewegl -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp
2021-12-10 12:44:46.309827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-10 12:44:46.309897: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp12/TD3_6
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -60.8    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 36       |
|    time_elapsed    | 217      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 1.3      |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 3003     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-7.53 +/- 28.57
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -7.53    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.38     |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.0005   |
|    n_updates       | 4999     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -16.4    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 20       |
|    time_elapsed    | 766      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 1.36     |
|    critic_loss     | 0.453    |
|    learning_rate   | 0.0005   |
|    n_updates       | 11007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=12.79 +/- 26.09
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 1.09     |
|    critic_loss     | 0.791    |
|    learning_rate   | 0.0005   |
|    n_updates       | 14999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.03    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 18       |
|    time_elapsed    | 1319     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 1.06     |
|    critic_loss     | 0.817    |
|    learning_rate   | 0.0005   |
|    n_updates       | 19011    |
---------------------------------
Eval num_timesteps=30000, episode_reward=10.64 +/- 19.60
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 0.754    |
|    critic_loss     | 2.36     |
|    learning_rate   | 0.0005   |
|    n_updates       | 24999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -12.8    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 17       |
|    time_elapsed    | 1869     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 0.702    |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.0005   |
|    n_updates       | 27015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=39.09 +/- 8.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 39.1     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.61     |
|    critic_loss     | 1.24     |
|    learning_rate   | 0.0005   |
|    n_updates       | 34999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.174   |
| time/              |          |
|    episodes        | 20       |
|    fps             | 16       |
|    time_elapsed    | 2418     |
|    total timesteps | 40020    |
| train/             |          |
|    actor_loss      | 0.595    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 35019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 16       |
|    time_elapsed    | 2853     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.444    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 43023    |
---------------------------------
Eval num_timesteps=50000, episode_reward=4.66 +/- 21.49
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 4.66     |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.434    |
|    critic_loss     | 0.0914   |
|    learning_rate   | 0.0005   |
|    n_updates       | 44999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 16       |
|    time_elapsed    | 3416     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 0.336    |
|    critic_loss     | 0.0682   |
|    learning_rate   | 0.0005   |
|    n_updates       | 51027    |
---------------------------------
Eval num_timesteps=60000, episode_reward=16.64 +/- 21.97
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 16.6     |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.296    |
|    critic_loss     | 0.0819   |
|    learning_rate   | 0.0005   |
|    n_updates       | 54999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.51    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 16       |
|    time_elapsed    | 3963     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 0.177    |
|    critic_loss     | 1.14     |
|    learning_rate   | 0.0005   |
|    n_updates       | 59031    |
---------------------------------
Eval num_timesteps=70000, episode_reward=23.46 +/- 45.85
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 23.5     |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 0.401    |
|    critic_loss     | 0.543    |
|    learning_rate   | 0.0005   |
|    n_updates       | 64999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.77    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 15       |
|    time_elapsed    | 4504     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 0.261    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 67035    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-12.04 +/- 46.12
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -12      |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.287    |
|    critic_loss     | 0.841    |
|    learning_rate   | 0.0005   |
|    n_updates       | 74999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.15    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 15       |
|    time_elapsed    | 5047     |
|    total timesteps | 80040    |
| train/             |          |
|    actor_loss      | 0.0988   |
|    critic_loss     | 1.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 75039    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.57    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 16       |
|    time_elapsed    | 5477     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 0.313    |
|    critic_loss     | 0.0833   |
|    learning_rate   | 0.0005   |
|    n_updates       | 83043    |
---------------------------------
Eval num_timesteps=90000, episode_reward=12.72 +/- 17.67
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 0.0842   |
|    critic_loss     | 0.848    |
|    learning_rate   | 0.0005   |
|    n_updates       | 84999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.43    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 15       |
|    time_elapsed    | 6028     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 0.251    |
|    critic_loss     | 1.56     |
|    learning_rate   | 0.0005   |
|    n_updates       | 91047    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-14.20 +/- 25.45
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.2    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.259    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 94999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.95    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 15       |
|    time_elapsed    | 6567     |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | 0.156    |
|    critic_loss     | 0.432    |
|    learning_rate   | 0.0005   |
|    n_updates       | 99051    |
---------------------------------
Eval num_timesteps=110000, episode_reward=12.22 +/- 25.37
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.2     |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -0.0837  |
|    critic_loss     | 0.979    |
|    learning_rate   | 0.0005   |
|    n_updates       | 104999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.12    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 15       |
|    time_elapsed    | 7107     |
|    total timesteps | 112056   |
| train/             |          |
|    actor_loss      | 0.353    |
|    critic_loss     | 0.822    |
|    learning_rate   | 0.0005   |
|    n_updates       | 107055   |
---------------------------------
Eval num_timesteps=120000, episode_reward=7.41 +/- 33.57
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 7.41     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 0.104    |
|    critic_loss     | 0.443    |
|    learning_rate   | 0.0005   |
|    n_updates       | 114999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.54    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 15       |
|    time_elapsed    | 7645     |
|    total timesteps | 120060   |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 115059   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.66    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 15       |
|    time_elapsed    | 8074     |
|    total timesteps | 128064   |
| train/             |          |
|    actor_loss      | 0.265    |
|    critic_loss     | 1.15     |
|    learning_rate   | 0.0005   |
|    n_updates       | 123063   |
---------------------------------
Eval num_timesteps=130000, episode_reward=5.56 +/- 26.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 5.56     |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 0.407    |
|    critic_loss     | 0.865    |
|    learning_rate   | 0.0005   |
|    n_updates       | 124999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.71    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 15       |
|    time_elapsed    | 8611     |
|    total timesteps | 136068   |
| train/             |          |
|    actor_loss      | 0.204    |
|    critic_loss     | 0.786    |
|    learning_rate   | 0.0005   |
|    n_updates       | 131067   |
---------------------------------
Eval num_timesteps=140000, episode_reward=12.87 +/- 29.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 12.9     |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 0.294    |
|    critic_loss     | 0.425    |
|    learning_rate   | 0.0005   |
|    n_updates       | 134999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.1     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 15       |
|    time_elapsed    | 9150     |
|    total timesteps | 144072   |
| train/             |          |
|    actor_loss      | 0.304    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 139071   |
---------------------------------
Eval num_timesteps=150000, episode_reward=11.68 +/- 45.69
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 0.182    |
|    critic_loss     | 0.817    |
|    learning_rate   | 0.0005   |
|    n_updates       | 144999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.23    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 15       |
|    time_elapsed    | 9690     |
|    total timesteps | 152076   |
| train/             |          |
|    actor_loss      | 0.239    |
|    critic_loss     | 0.792    |
|    learning_rate   | 0.0005   |
|    n_updates       | 147075   |
---------------------------------
Eval num_timesteps=160000, episode_reward=41.53 +/- 9.87
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 41.5     |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 0.168    |
|    critic_loss     | 2.81     |
|    learning_rate   | 0.0005   |
|    n_updates       | 154999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.24    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 15       |
|    time_elapsed    | 10236    |
|    total timesteps | 160080   |
| train/             |          |
|    actor_loss      | 0.135    |
|    critic_loss     | 0.0318   |
|    learning_rate   | 0.0005   |
|    n_updates       | 155079   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 84       |
|    fps             | 15       |
|    time_elapsed    | 10668    |
|    total timesteps | 168084   |
| train/             |          |
|    actor_loss      | 0.122    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 163083   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-1.32 +/- 16.36
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.32    |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 0.203    |
|    critic_loss     | 1.19     |
|    learning_rate   | 0.0005   |
|    n_updates       | 164999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.05    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 15       |
|    time_elapsed    | 11213    |
|    total timesteps | 176088   |
| train/             |          |
|    actor_loss      | 0.0774   |
|    critic_loss     | 0.83     |
|    learning_rate   | 0.0005   |
|    n_updates       | 171087   |
---------------------------------
Eval num_timesteps=180000, episode_reward=1.79 +/- 39.62
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 1.79     |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 0.262    |
|    critic_loss     | 0.832    |
|    learning_rate   | 0.0005   |
|    n_updates       | 174999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.96    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 15       |
|    time_elapsed    | 11755    |
|    total timesteps | 184092   |
| train/             |          |
|    actor_loss      | 0.124    |
|    critic_loss     | 0.419    |
|    learning_rate   | 0.0005   |
|    n_updates       | 179091   |
---------------------------------
Eval num_timesteps=190000, episode_reward=18.36 +/- 32.43
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 18.4     |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 1.53     |
|    learning_rate   | 0.0005   |
|    n_updates       | 184999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.87    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 15       |
|    time_elapsed    | 12299    |
|    total timesteps | 192096   |
| train/             |          |
|    actor_loss      | 0.177    |
|    critic_loss     | 0.0267   |
|    learning_rate   | 0.0005   |
|    n_updates       | 187095   |
---------------------------------
Eval num_timesteps=200000, episode_reward=2.23 +/- 21.25
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.23     |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 0.154    |
|    critic_loss     | 0.368    |
|    learning_rate   | 0.0005   |
|    n_updates       | 194999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.58    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 15       |
|    time_elapsed    | 12847    |
|    total timesteps | 200100   |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 0.819    |
|    learning_rate   | 0.0005   |
|    n_updates       | 195099   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.98    |
| time/              |          |
|    episodes        | 104      |
|    fps             | 15       |
|    time_elapsed    | 13277    |
|    total timesteps | 208104   |
| train/             |          |
|    actor_loss      | 0.161    |
|    critic_loss     | 1.92     |
|    learning_rate   | 0.0005   |
|    n_updates       | 203103   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-3.41 +/- 14.25
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -3.41    |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | 0.0874   |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.0005   |
|    n_updates       | 204999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.22    |
| time/              |          |
|    episodes        | 108      |
|    fps             | 15       |
|    time_elapsed    | 13817    |
|    total timesteps | 216108   |
| train/             |          |
|    actor_loss      | 0.305    |
|    critic_loss     | 1.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 211107   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-10.53 +/- 26.86
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.5    |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 0.228    |
|    critic_loss     | 0.421    |
|    learning_rate   | 0.0005   |
|    n_updates       | 214999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.09    |
| time/              |          |
|    episodes        | 112      |
|    fps             | 15       |
|    time_elapsed    | 14365    |
|    total timesteps | 224112   |
| train/             |          |
|    actor_loss      | 0.281    |
|    critic_loss     | 0.81     |
|    learning_rate   | 0.0005   |
|    n_updates       | 219111   |
---------------------------------
Eval num_timesteps=230000, episode_reward=-3.04 +/- 34.71
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -3.04    |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 0.182    |
|    critic_loss     | 0.418    |
|    learning_rate   | 0.0005   |
|    n_updates       | 224999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.71    |
| time/              |          |
|    episodes        | 116      |
|    fps             | 15       |
|    time_elapsed    | 14915    |
|    total timesteps | 232116   |
| train/             |          |
|    actor_loss      | 0.117    |
|    critic_loss     | 1.68     |
|    learning_rate   | 0.0005   |
|    n_updates       | 227115   |
---------------------------------
Eval num_timesteps=240000, episode_reward=27.70 +/- 48.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 27.7     |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.211    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 234999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5       |
| time/              |          |
|    episodes        | 120      |
|    fps             | 15       |
|    time_elapsed    | 15460    |
|    total timesteps | 240120   |
| train/             |          |
|    actor_loss      | 0.23     |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 235119   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.26    |
| time/              |          |
|    episodes        | 124      |
|    fps             | 15       |
|    time_elapsed    | 15889    |
|    total timesteps | 248124   |
| train/             |          |
|    actor_loss      | 0.405    |
|    critic_loss     | 0.502    |
|    learning_rate   | 0.0005   |
|    n_updates       | 243123   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-8.25 +/- 18.65
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -8.25    |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | 0.245    |
|    critic_loss     | 1.16     |
|    learning_rate   | 0.0005   |
|    n_updates       | 244999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.63    |
| time/              |          |
|    episodes        | 128      |
|    fps             | 15       |
|    time_elapsed    | 16431    |
|    total timesteps | 256128   |
| train/             |          |
|    actor_loss      | 0.173    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 251127   |
---------------------------------
Eval num_timesteps=260000, episode_reward=11.06 +/- 37.20
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 0.0595   |
|    critic_loss     | 0.0595   |
|    learning_rate   | 0.0005   |
|    n_updates       | 254999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.29    |
| time/              |          |
|    episodes        | 132      |
|    fps             | 15       |
|    time_elapsed    | 16977    |
|    total timesteps | 264132   |
| train/             |          |
|    actor_loss      | 0.355    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 259131   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-5.53 +/- 23.55
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -5.53    |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | 0.158    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 264999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.33    |
| time/              |          |
|    episodes        | 136      |
|    fps             | 15       |
|    time_elapsed    | 17517    |
|    total timesteps | 272136   |
| train/             |          |
|    actor_loss      | 0.273    |
|    critic_loss     | 2.74     |
|    learning_rate   | 0.0005   |
|    n_updates       | 267135   |
---------------------------------
Eval num_timesteps=280000, episode_reward=24.80 +/- 28.97
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 24.8     |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 0.213    |
|    critic_loss     | 0.422    |
|    learning_rate   | 0.0005   |
|    n_updates       | 274999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.58    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 15       |
|    time_elapsed    | 18054    |
|    total timesteps | 280140   |
| train/             |          |
|    actor_loss      | 0.0186   |
|    critic_loss     | 0.059    |
|    learning_rate   | 0.0005   |
|    n_updates       | 275139   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.25    |
| time/              |          |
|    episodes        | 144      |
|    fps             | 15       |
|    time_elapsed    | 18481    |
|    total timesteps | 288144   |
| train/             |          |
|    actor_loss      | 0.216    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 283143   |
---------------------------------
Eval num_timesteps=290000, episode_reward=15.64 +/- 19.14
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 15.6     |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | 0.288    |
|    critic_loss     | 1.19     |
|    learning_rate   | 0.0005   |
|    n_updates       | 284999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.52    |
| time/              |          |
|    episodes        | 148      |
|    fps             | 15       |
|    time_elapsed    | 19019    |
|    total timesteps | 296148   |
| train/             |          |
|    actor_loss      | 0.125    |
|    critic_loss     | 0.434    |
|    learning_rate   | 0.0005   |
|    n_updates       | 291147   |
---------------------------------
Eval num_timesteps=300000, episode_reward=2.52 +/- 13.41
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.52     |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 0.176    |
|    critic_loss     | 0.809    |
|    learning_rate   | 0.0005   |
|    n_updates       | 294999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.74    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 15       |
|    time_elapsed    | 19564    |
|    total timesteps | 304152   |
| train/             |          |
|    actor_loss      | 0.338    |
|    critic_loss     | 1.62     |
|    learning_rate   | 0.0005   |
|    n_updates       | 299151   |
---------------------------------
Eval num_timesteps=310000, episode_reward=11.91 +/- 18.83
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 11.9     |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | 0.254    |
|    critic_loss     | 0.807    |
|    learning_rate   | 0.0005   |
|    n_updates       | 304999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.18    |
| time/              |          |
|    episodes        | 156      |
|    fps             | 15       |
|    time_elapsed    | 20110    |
|    total timesteps | 312156   |
| train/             |          |
|    actor_loss      | 0.386    |
|    critic_loss     | 0.505    |
|    learning_rate   | 0.0005   |
|    n_updates       | 307155   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-2.69 +/- 21.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.69    |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 0.251    |
|    critic_loss     | 1.18     |
|    learning_rate   | 0.0005   |
|    n_updates       | 314999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.12    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 15       |
|    time_elapsed    | 20655    |
|    total timesteps | 320160   |
| train/             |          |
|    actor_loss      | 0.226    |
|    critic_loss     | 0.416    |
|    learning_rate   | 0.0005   |
|    n_updates       | 315159   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.6     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 15       |
|    time_elapsed    | 21085    |
|    total timesteps | 328164   |
| train/             |          |
|    actor_loss      | 0.165    |
|    critic_loss     | 0.775    |
|    learning_rate   | 0.0005   |
|    n_updates       | 323163   |
---------------------------------
