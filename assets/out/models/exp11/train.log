2021-12-08 21:16:01.257138: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-08 21:16:01.257218: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp11/TD3_26
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 222      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 22       |
|    time_elapsed    | 360      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 0.0525   |
|    learning_rate   | 0.0005   |
|    n_updates       | 7003     |
---------------------------------
Eval num_timesteps=10000, episode_reward=163.99 +/- 3.12
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 164      |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -1.74    |
|    critic_loss     | 0.117    |
|    learning_rate   | 0.0005   |
|    n_updates       | 8999     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 211      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 18       |
|    time_elapsed    | 871      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | -2.34    |
|    critic_loss     | 0.073    |
|    learning_rate   | 0.0005   |
|    n_updates       | 15007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=164.81 +/- 3.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -2.64    |
|    critic_loss     | 0.0755   |
|    learning_rate   | 0.0005   |
|    n_updates       | 18999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 207      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 17       |
|    time_elapsed    | 1378     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | -3.1     |
|    critic_loss     | 0.0978   |
|    learning_rate   | 0.0005   |
|    n_updates       | 23011    |
---------------------------------
Eval num_timesteps=30000, episode_reward=165.20 +/- 2.08
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -3.36    |
|    critic_loss     | 0.0541   |
|    learning_rate   | 0.0005   |
|    n_updates       | 28999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 209      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 16       |
|    time_elapsed    | 1887     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | -3.49    |
|    critic_loss     | 0.0518   |
|    learning_rate   | 0.0005   |
|    n_updates       | 31015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=160.61 +/- 1.50
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -3.65    |
|    critic_loss     | 0.0526   |
|    learning_rate   | 0.0005   |
|    n_updates       | 38999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 203      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 16       |
|    time_elapsed    | 2398     |
|    total timesteps | 40020    |
| train/             |          |
|    actor_loss      | -3.7     |
|    critic_loss     | 0.0453   |
|    learning_rate   | 0.0005   |
|    n_updates       | 39019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 204      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 17       |
|    time_elapsed    | 2795     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | -3.99    |
|    critic_loss     | 0.0406   |
|    learning_rate   | 0.0005   |
|    n_updates       | 47023    |
---------------------------------
Eval num_timesteps=50000, episode_reward=166.33 +/- 3.78
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -3.94    |
|    critic_loss     | 0.0833   |
|    learning_rate   | 0.0005   |
|    n_updates       | 48999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 205      |
| time/              |          |
|    episodes        | 28       |
|    fps             | 16       |
|    time_elapsed    | 3304     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | -4.08    |
|    critic_loss     | 0.0404   |
|    learning_rate   | 0.0005   |
|    n_updates       | 55027    |
---------------------------------
Eval num_timesteps=60000, episode_reward=161.79 +/- 3.17
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -4.21    |
|    critic_loss     | 0.0365   |
|    learning_rate   | 0.0005   |
|    n_updates       | 58999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 202      |
| time/              |          |
|    episodes        | 32       |
|    fps             | 16       |
|    time_elapsed    | 3812     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | -4.16    |
|    critic_loss     | 0.0322   |
|    learning_rate   | 0.0005   |
|    n_updates       | 63031    |
---------------------------------
Eval num_timesteps=70000, episode_reward=164.73 +/- 2.60
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -4.12    |
|    critic_loss     | 0.0686   |
|    learning_rate   | 0.0005   |
|    n_updates       | 68999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 202      |
| time/              |          |
|    episodes        | 36       |
|    fps             | 16       |
|    time_elapsed    | 4321     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | -4.19    |
|    critic_loss     | 0.0497   |
|    learning_rate   | 0.0005   |
|    n_updates       | 71035    |
---------------------------------
Eval num_timesteps=80000, episode_reward=167.07 +/- 1.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -4.37    |
|    critic_loss     | 0.0232   |
|    learning_rate   | 0.0005   |
|    n_updates       | 78999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 202      |
| time/              |          |
|    episodes        | 40       |
|    fps             | 16       |
|    time_elapsed    | 4833     |
|    total timesteps | 80040    |
| train/             |          |
|    actor_loss      | -4.37    |
|    critic_loss     | 0.0254   |
|    learning_rate   | 0.0005   |
|    n_updates       | 79039    |
---------------------------------
Terminated
2021-12-08 22:43:58.834677: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-08 22:43:58.834742: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp11/TD3_27
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.5     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 39       |
|    time_elapsed    | 202      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 1.65     |
|    critic_loss     | 0.0509   |
|    learning_rate   | 0.0005   |
|    n_updates       | 3003     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-2.08 +/- 0.53
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.57     |
|    critic_loss     | 0.054    |
|    learning_rate   | 0.0005   |
|    n_updates       | 4999     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.6     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 22       |
|    time_elapsed    | 715      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 1.34     |
|    critic_loss     | 0.039    |
|    learning_rate   | 0.0005   |
|    n_updates       | 11007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-2.64 +/- 1.33
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.64    |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 1.11     |
|    critic_loss     | 0.0254   |
|    learning_rate   | 0.0005   |
|    n_updates       | 14999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.57    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 19       |
|    time_elapsed    | 1227     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 1.01     |
|    critic_loss     | 0.00763  |
|    learning_rate   | 0.0005   |
|    n_updates       | 19011    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-2.48 +/- 0.59
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.48    |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 0.708    |
|    critic_loss     | 0.0295   |
|    learning_rate   | 0.0005   |
|    n_updates       | 24999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.08    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 18       |
|    time_elapsed    | 1733     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 0.661    |
|    critic_loss     | 0.0224   |
|    learning_rate   | 0.0005   |
|    n_updates       | 27015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-3.20 +/- 1.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.695    |
|    critic_loss     | 0.00946  |
|    learning_rate   | 0.0005   |
|    n_updates       | 34999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.9     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 17       |
|    time_elapsed    | 2241     |
|    total timesteps | 40020    |
| train/             |          |
|    actor_loss      | 0.682    |
|    critic_loss     | 0.0074   |
|    learning_rate   | 0.0005   |
|    n_updates       | 35019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.57    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 18       |
|    time_elapsed    | 2634     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.395    |
|    critic_loss     | 0.022    |
|    learning_rate   | 0.0005   |
|    n_updates       | 43023    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-2.08 +/- 0.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.388    |
|    critic_loss     | 0.0199   |
|    learning_rate   | 0.0005   |
|    n_updates       | 44999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.69    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 17       |
|    time_elapsed    | 3141     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 0.394    |
|    critic_loss     | 0.0105   |
|    learning_rate   | 0.0005   |
|    n_updates       | 51027    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-2.24 +/- 0.93
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.24    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.43     |
|    critic_loss     | 0.00178  |
|    learning_rate   | 0.0005   |
|    n_updates       | 54999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.56    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 17       |
|    time_elapsed    | 3648     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 0.501    |
|    critic_loss     | 0.00788  |
|    learning_rate   | 0.0005   |
|    n_updates       | 59031    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-2.08 +/- 0.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 0.335    |
|    critic_loss     | 0.00992  |
|    learning_rate   | 0.0005   |
|    n_updates       | 64999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.46    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 17       |
|    time_elapsed    | 4154     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 0.43     |
|    critic_loss     | 0.024    |
|    learning_rate   | 0.0005   |
|    n_updates       | 67035    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-2.48 +/- 0.69
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.48    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.511    |
|    critic_loss     | 0.0203   |
|    learning_rate   | 0.0005   |
|    n_updates       | 74999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.4     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 17       |
|    time_elapsed    | 4663     |
|    total timesteps | 80040    |
| train/             |          |
|    actor_loss      | 0.44     |
|    critic_loss     | 0.00606  |
|    learning_rate   | 0.0005   |
|    n_updates       | 75039    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.41    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 17       |
|    time_elapsed    | 5055     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 0.268    |
|    critic_loss     | 0.00457  |
|    learning_rate   | 0.0005   |
|    n_updates       | 83043    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-2.24 +/- 0.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.24    |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 0.324    |
|    critic_loss     | 0.00994  |
|    learning_rate   | 0.0005   |
|    n_updates       | 84999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.34    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 17       |
|    time_elapsed    | 5560     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 0.407    |
|    critic_loss     | 0.0233   |
|    learning_rate   | 0.0005   |
|    n_updates       | 91047    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-2.88 +/- 0.85
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.88    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.403    |
|    critic_loss     | 0.0121   |
|    learning_rate   | 0.0005   |
|    n_updates       | 94999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.39    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 17       |
|    time_elapsed    | 6061     |
|    total timesteps | 104052   |
| train/             |          |
|    actor_loss      | 0.349    |
|    critic_loss     | 0.00983  |
|    learning_rate   | 0.0005   |
|    n_updates       | 99051    |
---------------------------------
Eval num_timesteps=110000, episode_reward=-2.64 +/- 0.60
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.64    |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 0.279    |
|    critic_loss     | 0.00481  |
|    learning_rate   | 0.0005   |
|    n_updates       | 104999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.39    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 17       |
|    time_elapsed    | 6568     |
|    total timesteps | 112056   |
| train/             |          |
|    actor_loss      | 0.299    |
|    critic_loss     | 0.00118  |
|    learning_rate   | 0.0005   |
|    n_updates       | 107055   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-2.72 +/- 1.02
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.72    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 0.35     |
|    critic_loss     | 0.00801  |
|    learning_rate   | 0.0005   |
|    n_updates       | 114999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.37    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 16       |
|    time_elapsed    | 7067     |
|    total timesteps | 120060   |
| train/             |          |
|    actor_loss      | 0.326    |
|    critic_loss     | 0.00275  |
|    learning_rate   | 0.0005   |
|    n_updates       | 115059   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.38    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 17       |
|    time_elapsed    | 7455     |
|    total timesteps | 128064   |
| train/             |          |
|    actor_loss      | 0.303    |
|    critic_loss     | 0.00124  |
|    learning_rate   | 0.0005   |
|    n_updates       | 123063   |
---------------------------------
Terminated
2021-12-09 00:50:35.782690: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-09 00:50:35.782758: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp11/TD3_30
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.19    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 38       |
|    time_elapsed    | 209      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 1.16     |
|    critic_loss     | 0.0673   |
|    learning_rate   | 0.0005   |
|    n_updates       | 3003     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-1.88 +/- 0.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.88    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.36     |
|    critic_loss     | 0.0721   |
|    learning_rate   | 0.0005   |
|    n_updates       | 4999     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.13    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 21       |
|    time_elapsed    | 733      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 0.863    |
|    critic_loss     | 0.0434   |
|    learning_rate   | 0.0005   |
|    n_updates       | 11007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-1.63 +/- 0.11
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.63    |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 0.884    |
|    critic_loss     | 0.0111   |
|    learning_rate   | 0.0005   |
|    n_updates       | 14999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 12       |
|    fps             | 19       |
|    time_elapsed    | 1256     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 0.783    |
|    critic_loss     | 0.00437  |
|    learning_rate   | 0.0005   |
|    n_updates       | 19011    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-1.78 +/- 0.24
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.78    |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 0.678    |
|    critic_loss     | 0.0177   |
|    learning_rate   | 0.0005   |
|    n_updates       | 24999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.93    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 17       |
|    time_elapsed    | 1787     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 0.706    |
|    critic_loss     | 0.0161   |
|    learning_rate   | 0.0005   |
|    n_updates       | 27015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-2.08 +/- 0.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.602    |
|    critic_loss     | 0.00965  |
|    learning_rate   | 0.0005   |
|    n_updates       | 34999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.97    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 17       |
|    time_elapsed    | 2318     |
|    total timesteps | 40020    |
| train/             |          |
|    actor_loss      | 0.882    |
|    critic_loss     | 0.0713   |
|    learning_rate   | 0.0005   |
|    n_updates       | 35019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 17       |
|    time_elapsed    | 2720     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.517    |
|    critic_loss     | 0.00181  |
|    learning_rate   | 0.0005   |
|    n_updates       | 43023    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-1.82 +/- 0.13
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.82    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.719    |
|    critic_loss     | 0.0433   |
|    learning_rate   | 0.0005   |
|    n_updates       | 44999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.97    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 17       |
|    time_elapsed    | 3253     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 0.508    |
|    critic_loss     | 0.00281  |
|    learning_rate   | 0.0005   |
|    n_updates       | 51027    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-2.11 +/- 0.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.11    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.778    |
|    critic_loss     | 0.0926   |
|    learning_rate   | 0.0005   |
|    n_updates       | 54999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.97    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 16       |
|    time_elapsed    | 3780     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 0.476    |
|    critic_loss     | 0.000796 |
|    learning_rate   | 0.0005   |
|    n_updates       | 59031    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-1.96 +/- 0.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.96    |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 0.439    |
|    critic_loss     | 0.0237   |
|    learning_rate   | 0.0005   |
|    n_updates       | 64999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 16       |
|    time_elapsed    | 4302     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 0.437    |
|    critic_loss     | 0.00914  |
|    learning_rate   | 0.0005   |
|    n_updates       | 67035    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-2.04 +/- 0.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.04    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.387    |
|    critic_loss     | 0.00182  |
|    learning_rate   | 0.0005   |
|    n_updates       | 74999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 40       |
|    fps             | 16       |
|    time_elapsed    | 4824     |
|    total timesteps | 80040    |
| train/             |          |
|    actor_loss      | 0.392    |
|    critic_loss     | 0.0239   |
|    learning_rate   | 0.0005   |
|    n_updates       | 75039    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.99    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 16       |
|    time_elapsed    | 5227     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 0.362    |
|    critic_loss     | 0.00317  |
|    learning_rate   | 0.0005   |
|    n_updates       | 83043    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-1.90 +/- 0.38
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.9     |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 0.339    |
|    critic_loss     | 0.00718  |
|    learning_rate   | 0.0005   |
|    n_updates       | 84999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.95    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 16       |
|    time_elapsed    | 5749     |
|    total timesteps | 96048    |
| train/             |          |
|    actor_loss      | 0.401    |
|    critic_loss     | 0.00708  |
|    learning_rate   | 0.0005   |
|    n_updates       | 91047    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-1.86 +/- 0.40
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.86    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.39     |
|    critic_loss     | 0.00739  |
|    learning_rate   | 0.0005   |
|    n_updates       | 94999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 16       |
|    time_elapsed    | 6267     |
|    total timesteps | 103835   |
| train/             |          |
|    actor_loss      | 0.219    |
|    critic_loss     | 0.0189   |
|    learning_rate   | 0.0005   |
|    n_updates       | 98834    |
---------------------------------
Eval num_timesteps=110000, episode_reward=-1.98 +/- 0.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.98    |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 0.401    |
|    critic_loss     | 0.0606   |
|    learning_rate   | 0.0005   |
|    n_updates       | 104999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.95    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 16       |
|    time_elapsed    | 6795     |
|    total timesteps | 111839   |
| train/             |          |
|    actor_loss      | 0.319    |
|    critic_loss     | 0.000675 |
|    learning_rate   | 0.0005   |
|    n_updates       | 106838   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 16       |
|    time_elapsed    | 7199     |
|    total timesteps | 119843   |
| train/             |          |
|    actor_loss      | 0.333    |
|    critic_loss     | 0.00333  |
|    learning_rate   | 0.0005   |
|    n_updates       | 114842   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-1.82 +/- 0.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.82    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 0.281    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.0005   |
|    n_updates       | 114999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.89    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 16       |
|    time_elapsed    | 7723     |
|    total timesteps | 127847   |
| train/             |          |
|    actor_loss      | 0.313    |
|    critic_loss     | 0.0167   |
|    learning_rate   | 0.0005   |
|    n_updates       | 122846   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-1.98 +/- 0.18
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.98    |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 0.31     |
|    critic_loss     | 0.00568  |
|    learning_rate   | 0.0005   |
|    n_updates       | 124999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 16       |
|    time_elapsed    | 8245     |
|    total timesteps | 135851   |
| train/             |          |
|    actor_loss      | 0.198    |
|    critic_loss     | 0.0106   |
|    learning_rate   | 0.0005   |
|    n_updates       | 130850   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-1.90 +/- 0.24
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.9     |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 0.505    |
|    critic_loss     | 0.0415   |
|    learning_rate   | 0.0005   |
|    n_updates       | 134999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.83    |
| time/              |          |
|    episodes        | 72       |
|    fps             | 16       |
|    time_elapsed    | 8737     |
|    total timesteps | 143232   |
| train/             |          |
|    actor_loss      | 0.306    |
|    critic_loss     | 0.0017   |
|    learning_rate   | 0.0005   |
|    n_updates       | 138231   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-1.84 +/- 0.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.84    |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 0.281    |
|    critic_loss     | 0.00121  |
|    learning_rate   | 0.0005   |
|    n_updates       | 144999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.84    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 16       |
|    time_elapsed    | 9260     |
|    total timesteps | 151236   |
| train/             |          |
|    actor_loss      | 0.21     |
|    critic_loss     | 0.022    |
|    learning_rate   | 0.0005   |
|    n_updates       | 146235   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 16       |
|    time_elapsed    | 9659     |
|    total timesteps | 159240   |
| train/             |          |
|    actor_loss      | 0.246    |
|    critic_loss     | 0.00538  |
|    learning_rate   | 0.0005   |
|    n_updates       | 154239   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-1.84 +/- 0.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.84    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 0.353    |
|    critic_loss     | 0.00129  |
|    learning_rate   | 0.0005   |
|    n_updates       | 154999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 84       |
|    fps             | 16       |
|    time_elapsed    | 10178    |
|    total timesteps | 167244   |
| train/             |          |
|    actor_loss      | 0.272    |
|    critic_loss     | 0.0359   |
|    learning_rate   | 0.0005   |
|    n_updates       | 162243   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-1.95 +/- 0.28
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.95    |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 0.307    |
|    critic_loss     | 0.000302 |
|    learning_rate   | 0.0005   |
|    n_updates       | 164999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.88    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 16       |
|    time_elapsed    | 10707    |
|    total timesteps | 175248   |
| train/             |          |
|    actor_loss      | 0.309    |
|    critic_loss     | 0.000588 |
|    learning_rate   | 0.0005   |
|    n_updates       | 170247   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-2.02 +/- 0.33
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.02    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 0.524    |
|    critic_loss     | 0.0541   |
|    learning_rate   | 0.0005   |
|    n_updates       | 174999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.89    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 16       |
|    time_elapsed    | 11236    |
|    total timesteps | 183252   |
| train/             |          |
|    actor_loss      | 0.372    |
|    critic_loss     | 0.00541  |
|    learning_rate   | 0.0005   |
|    n_updates       | 178251   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-2.22 +/- 0.13
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.22    |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 0.248    |
|    critic_loss     | 0.00915  |
|    learning_rate   | 0.0005   |
|    n_updates       | 184999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 16       |
|    time_elapsed    | 11766    |
|    total timesteps | 191256   |
| train/             |          |
|    actor_loss      | 0.504    |
|    critic_loss     | 0.0589   |
|    learning_rate   | 0.0005   |
|    n_updates       | 186255   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.93    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 16       |
|    time_elapsed    | 12171    |
|    total timesteps | 199260   |
| train/             |          |
|    actor_loss      | 0.327    |
|    critic_loss     | 0.027    |
|    learning_rate   | 0.0005   |
|    n_updates       | 194259   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-2.00 +/- 0.16
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2       |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 0.428    |
|    critic_loss     | 0.0148   |
|    learning_rate   | 0.0005   |
|    n_updates       | 194999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 104      |
|    fps             | 16       |
|    time_elapsed    | 12700    |
|    total timesteps | 207264   |
| train/             |          |
|    actor_loss      | 0.33     |
|    critic_loss     | 0.0192   |
|    learning_rate   | 0.0005   |
|    n_updates       | 202263   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-1.94 +/- 0.44
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.94    |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | 0.349    |
|    critic_loss     | 0.00361  |
|    learning_rate   | 0.0005   |
|    n_updates       | 204999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 108      |
|    fps             | 16       |
|    time_elapsed    | 13230    |
|    total timesteps | 215268   |
| train/             |          |
|    actor_loss      | 0.317    |
|    critic_loss     | 0.00538  |
|    learning_rate   | 0.0005   |
|    n_updates       | 210267   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-1.90 +/- 0.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.9     |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 0.232    |
|    critic_loss     | 0.00287  |
|    learning_rate   | 0.0005   |
|    n_updates       | 214999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 16       |
|    time_elapsed    | 13760    |
|    total timesteps | 223272   |
| train/             |          |
|    actor_loss      | 0.258    |
|    critic_loss     | 0.000945 |
|    learning_rate   | 0.0005   |
|    n_updates       | 218271   |
---------------------------------
Eval num_timesteps=230000, episode_reward=-2.16 +/- 0.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.16    |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 0.335    |
|    critic_loss     | 0.00948  |
|    learning_rate   | 0.0005   |
|    n_updates       | 224999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 16       |
|    time_elapsed    | 14288    |
|    total timesteps | 231276   |
| train/             |          |
|    actor_loss      | 0.225    |
|    critic_loss     | 0.00299  |
|    learning_rate   | 0.0005   |
|    n_updates       | 226275   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.89    |
| time/              |          |
|    episodes        | 120      |
|    fps             | 16       |
|    time_elapsed    | 14692    |
|    total timesteps | 239280   |
| train/             |          |
|    actor_loss      | 0.375    |
|    critic_loss     | 0.0123   |
|    learning_rate   | 0.0005   |
|    n_updates       | 234279   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-1.82 +/- 0.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.82    |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.277    |
|    critic_loss     | 0.00235  |
|    learning_rate   | 0.0005   |
|    n_updates       | 234999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.89    |
| time/              |          |
|    episodes        | 124      |
|    fps             | 16       |
|    time_elapsed    | 15214    |
|    total timesteps | 247284   |
| train/             |          |
|    actor_loss      | 0.281    |
|    critic_loss     | 0.000474 |
|    learning_rate   | 0.0005   |
|    n_updates       | 242283   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-1.78 +/- 0.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.78    |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | 0.287    |
|    critic_loss     | 0.00426  |
|    learning_rate   | 0.0005   |
|    n_updates       | 244999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 128      |
|    fps             | 16       |
|    time_elapsed    | 15736    |
|    total timesteps | 255288   |
| train/             |          |
|    actor_loss      | 0.333    |
|    critic_loss     | 0.00718  |
|    learning_rate   | 0.0005   |
|    n_updates       | 250287   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-2.02 +/- 0.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.02    |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 0.335    |
|    critic_loss     | 0.00611  |
|    learning_rate   | 0.0005   |
|    n_updates       | 254999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.88    |
| time/              |          |
|    episodes        | 132      |
|    fps             | 16       |
|    time_elapsed    | 16257    |
|    total timesteps | 263292   |
| train/             |          |
|    actor_loss      | 0.226    |
|    critic_loss     | 0.00296  |
|    learning_rate   | 0.0005   |
|    n_updates       | 258291   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-1.90 +/- 0.20
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.9     |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | 0.274    |
|    critic_loss     | 0.000517 |
|    learning_rate   | 0.0005   |
|    n_updates       | 264999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 136      |
|    fps             | 16       |
|    time_elapsed    | 16780    |
|    total timesteps | 271296   |
| train/             |          |
|    actor_loss      | 0.293    |
|    critic_loss     | 0.00184  |
|    learning_rate   | 0.0005   |
|    n_updates       | 266295   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.86    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 16       |
|    time_elapsed    | 17178    |
|    total timesteps | 279254   |
| train/             |          |
|    actor_loss      | 0.253    |
|    critic_loss     | 0.000284 |
|    learning_rate   | 0.0005   |
|    n_updates       | 274253   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-1.94 +/- 0.24
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.94    |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 0.226    |
|    critic_loss     | 0.0045   |
|    learning_rate   | 0.0005   |
|    n_updates       | 274999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 144      |
|    fps             | 16       |
|    time_elapsed    | 17700    |
|    total timesteps | 287258   |
| train/             |          |
|    actor_loss      | 0.27     |
|    critic_loss     | 0.00446  |
|    learning_rate   | 0.0005   |
|    n_updates       | 282257   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-1.88 +/- 0.18
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.88    |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | 0.26     |
|    critic_loss     | 0.00672  |
|    learning_rate   | 0.0005   |
|    n_updates       | 284999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 148      |
|    fps             | 16       |
|    time_elapsed    | 18231    |
|    total timesteps | 295262   |
| train/             |          |
|    actor_loss      | 0.271    |
|    critic_loss     | 0.000234 |
|    learning_rate   | 0.0005   |
|    n_updates       | 290261   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-1.94 +/- 0.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.94    |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 0.262    |
|    critic_loss     | 0.000359 |
|    learning_rate   | 0.0005   |
|    n_updates       | 294999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.93    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 16       |
|    time_elapsed    | 18764    |
|    total timesteps | 303266   |
| train/             |          |
|    actor_loss      | 0.307    |
|    critic_loss     | 0.00133  |
|    learning_rate   | 0.0005   |
|    n_updates       | 298265   |
---------------------------------
Eval num_timesteps=310000, episode_reward=-2.04 +/- 0.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.04    |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | 0.412    |
|    critic_loss     | 0.013    |
|    learning_rate   | 0.0005   |
|    n_updates       | 304999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 156      |
|    fps             | 16       |
|    time_elapsed    | 19294    |
|    total timesteps | 311270   |
| train/             |          |
|    actor_loss      | 0.387    |
|    critic_loss     | 0.00678  |
|    learning_rate   | 0.0005   |
|    n_updates       | 306269   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 16       |
|    time_elapsed    | 19698    |
|    total timesteps | 319274   |
| train/             |          |
|    actor_loss      | 0.294    |
|    critic_loss     | 0.018    |
|    learning_rate   | 0.0005   |
|    n_updates       | 314273   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-1.98 +/- 0.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.98    |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 0.286    |
|    critic_loss     | 0.00436  |
|    learning_rate   | 0.0005   |
|    n_updates       | 314999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 164      |
|    fps             | 16       |
|    time_elapsed    | 20226    |
|    total timesteps | 327278   |
| train/             |          |
|    actor_loss      | 0.244    |
|    critic_loss     | 0.00262  |
|    learning_rate   | 0.0005   |
|    n_updates       | 322277   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-1.66 +/- 0.28
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.66    |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | 0.309    |
|    critic_loss     | 0.00301  |
|    learning_rate   | 0.0005   |
|    n_updates       | 324999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 16       |
|    time_elapsed    | 20750    |
|    total timesteps | 335282   |
| train/             |          |
|    actor_loss      | 0.436    |
|    critic_loss     | 0.0254   |
|    learning_rate   | 0.0005   |
|    n_updates       | 330281   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-1.80 +/- 0.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.8     |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | 0.545    |
|    critic_loss     | 0.0567   |
|    learning_rate   | 0.0005   |
|    n_updates       | 334999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 172      |
|    fps             | 16       |
|    time_elapsed    | 21279    |
|    total timesteps | 343286   |
| train/             |          |
|    actor_loss      | 0.312    |
|    critic_loss     | 0.00271  |
|    learning_rate   | 0.0005   |
|    n_updates       | 338285   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-1.88 +/- 0.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.88    |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | 0.422    |
|    critic_loss     | 0.00591  |
|    learning_rate   | 0.0005   |
|    n_updates       | 344999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 176      |
|    fps             | 16       |
|    time_elapsed    | 21810    |
|    total timesteps | 351290   |
| train/             |          |
|    actor_loss      | 0.262    |
|    critic_loss     | 0.0284   |
|    learning_rate   | 0.0005   |
|    n_updates       | 346289   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 16       |
|    time_elapsed    | 22216    |
|    total timesteps | 359294   |
| train/             |          |
|    actor_loss      | 0.331    |
|    critic_loss     | 0.00208  |
|    learning_rate   | 0.0005   |
|    n_updates       | 354293   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-1.84 +/- 0.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.84    |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | 0.356    |
|    critic_loss     | 0.00441  |
|    learning_rate   | 0.0005   |
|    n_updates       | 354999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 16       |
|    time_elapsed    | 22742    |
|    total timesteps | 367298   |
| train/             |          |
|    actor_loss      | 0.399    |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.0005   |
|    n_updates       | 362297   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-1.92 +/- 0.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.92    |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | 0.378    |
|    critic_loss     | 0.00683  |
|    learning_rate   | 0.0005   |
|    n_updates       | 364999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 188      |
|    fps             | 16       |
|    time_elapsed    | 23266    |
|    total timesteps | 375302   |
| train/             |          |
|    actor_loss      | 0.262    |
|    critic_loss     | 0.00163  |
|    learning_rate   | 0.0005   |
|    n_updates       | 370301   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-1.79 +/- 0.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.79    |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | 0.465    |
|    critic_loss     | 0.0458   |
|    learning_rate   | 0.0005   |
|    n_updates       | 374999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 192      |
|    fps             | 16       |
|    time_elapsed    | 23795    |
|    total timesteps | 383306   |
| train/             |          |
|    actor_loss      | 0.225    |
|    critic_loss     | 0.00112  |
|    learning_rate   | 0.0005   |
|    n_updates       | 378305   |
---------------------------------
Eval num_timesteps=390000, episode_reward=-1.92 +/- 0.36
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.92    |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | 0.257    |
|    critic_loss     | 0.000348 |
|    learning_rate   | 0.0005   |
|    n_updates       | 384999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.88    |
| time/              |          |
|    episodes        | 196      |
|    fps             | 16       |
|    time_elapsed    | 24326    |
|    total timesteps | 391310   |
| train/             |          |
|    actor_loss      | 0.291    |
|    critic_loss     | 0.0125   |
|    learning_rate   | 0.0005   |
|    n_updates       | 386309   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.87    |
| time/              |          |
|    episodes        | 200      |
|    fps             | 16       |
|    time_elapsed    | 24733    |
|    total timesteps | 399314   |
| train/             |          |
|    actor_loss      | 0.274    |
|    critic_loss     | 0.000506 |
|    learning_rate   | 0.0005   |
|    n_updates       | 394313   |
---------------------------------
Eval num_timesteps=400000, episode_reward=-1.88 +/- 0.40
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.88    |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | 0.359    |
|    critic_loss     | 0.00879  |
|    learning_rate   | 0.0005   |
|    n_updates       | 394999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 204      |
|    fps             | 16       |
|    time_elapsed    | 25265    |
|    total timesteps | 407318   |
| train/             |          |
|    actor_loss      | 0.0953   |
|    critic_loss     | 0.0222   |
|    learning_rate   | 0.0005   |
|    n_updates       | 402317   |
---------------------------------
Eval num_timesteps=410000, episode_reward=-2.04 +/- 0.20
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.04    |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | 0.25     |
|    critic_loss     | 0.000961 |
|    learning_rate   | 0.0005   |
|    n_updates       | 404999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.9     |
| time/              |          |
|    episodes        | 208      |
|    fps             | 16       |
|    time_elapsed    | 25788    |
|    total timesteps | 415322   |
| train/             |          |
|    actor_loss      | 0.26     |
|    critic_loss     | 0.00647  |
|    learning_rate   | 0.0005   |
|    n_updates       | 410321   |
---------------------------------
Eval num_timesteps=420000, episode_reward=-1.99 +/- 0.26
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.99    |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | 0.204    |
|    critic_loss     | 0.0025   |
|    learning_rate   | 0.0005   |
|    n_updates       | 414999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 16       |
|    time_elapsed    | 26314    |
|    total timesteps | 423326   |
| train/             |          |
|    actor_loss      | 0.26     |
|    critic_loss     | 0.00659  |
|    learning_rate   | 0.0005   |
|    n_updates       | 418325   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-1.82 +/- 0.15
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.82    |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | 0.205    |
|    critic_loss     | 0.00549  |
|    learning_rate   | 0.0005   |
|    n_updates       | 424999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 216      |
|    fps             | 16       |
|    time_elapsed    | 26839    |
|    total timesteps | 431330   |
| train/             |          |
|    actor_loss      | 0.285    |
|    critic_loss     | 0.00928  |
|    learning_rate   | 0.0005   |
|    n_updates       | 426329   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.93    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 16       |
|    time_elapsed    | 27244    |
|    total timesteps | 439334   |
| train/             |          |
|    actor_loss      | 0.241    |
|    critic_loss     | 0.000848 |
|    learning_rate   | 0.0005   |
|    n_updates       | 434333   |
---------------------------------
Eval num_timesteps=440000, episode_reward=-1.84 +/- 0.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.84    |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | 0.208    |
|    critic_loss     | 0.00365  |
|    learning_rate   | 0.0005   |
|    n_updates       | 434999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.96    |
| time/              |          |
|    episodes        | 224      |
|    fps             | 16       |
|    time_elapsed    | 27765    |
|    total timesteps | 447338   |
| train/             |          |
|    actor_loss      | 0.244    |
|    critic_loss     | 0.0147   |
|    learning_rate   | 0.0005   |
|    n_updates       | 442337   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-2.04 +/- 0.42
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.04    |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | 0.204    |
|    critic_loss     | 0.00257  |
|    learning_rate   | 0.0005   |
|    n_updates       | 444999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.96    |
| time/              |          |
|    episodes        | 228      |
|    fps             | 16       |
|    time_elapsed    | 28292    |
|    total timesteps | 455342   |
| train/             |          |
|    actor_loss      | 0.284    |
|    critic_loss     | 0.00699  |
|    learning_rate   | 0.0005   |
|    n_updates       | 450341   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-1.72 +/- 0.18
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.72    |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | 0.229    |
|    critic_loss     | 0.00435  |
|    learning_rate   | 0.0005   |
|    n_updates       | 454999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 232      |
|    fps             | 16       |
|    time_elapsed    | 28823    |
|    total timesteps | 463346   |
| train/             |          |
|    actor_loss      | 0.211    |
|    critic_loss     | 0.0119   |
|    learning_rate   | 0.0005   |
|    n_updates       | 458345   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-1.76 +/- 0.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.76    |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | 0.247    |
|    critic_loss     | 0.000343 |
|    learning_rate   | 0.0005   |
|    n_updates       | 464999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 236      |
|    fps             | 16       |
|    time_elapsed    | 29351    |
|    total timesteps | 471350   |
| train/             |          |
|    actor_loss      | 0.283    |
|    critic_loss     | 0.00271  |
|    learning_rate   | 0.0005   |
|    n_updates       | 466349   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.96    |
| time/              |          |
|    episodes        | 240      |
|    fps             | 16       |
|    time_elapsed    | 29754    |
|    total timesteps | 479354   |
| train/             |          |
|    actor_loss      | 0.234    |
|    critic_loss     | 0.00575  |
|    learning_rate   | 0.0005   |
|    n_updates       | 474353   |
---------------------------------
Eval num_timesteps=480000, episode_reward=-1.82 +/- 0.26
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.82    |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | 0.279    |
|    critic_loss     | 0.0063   |
|    learning_rate   | 0.0005   |
|    n_updates       | 474999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 244      |
|    fps             | 16       |
|    time_elapsed    | 30283    |
|    total timesteps | 487358   |
| train/             |          |
|    actor_loss      | 0.33     |
|    critic_loss     | 0.0582   |
|    learning_rate   | 0.0005   |
|    n_updates       | 482357   |
---------------------------------
Eval num_timesteps=490000, episode_reward=-2.04 +/- 0.24
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.04    |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | 0.3      |
|    critic_loss     | 0.00337  |
|    learning_rate   | 0.0005   |
|    n_updates       | 484999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.91    |
| time/              |          |
|    episodes        | 248      |
|    fps             | 16       |
|    time_elapsed    | 30814    |
|    total timesteps | 495362   |
| train/             |          |
|    actor_loss      | 0.314    |
|    critic_loss     | 0.00122  |
|    learning_rate   | 0.0005   |
|    n_updates       | 490361   |
---------------------------------
Eval num_timesteps=500000, episode_reward=-1.72 +/- 0.24
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.72    |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | 0.254    |
|    critic_loss     | 0.000302 |
|    learning_rate   | 0.0005   |
|    n_updates       | 494999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.92    |
| time/              |          |
|    episodes        | 252      |
|    fps             | 16       |
|    time_elapsed    | 31343    |
|    total timesteps | 503366   |
| train/             |          |
|    actor_loss      | 0.434    |
|    critic_loss     | 0.157    |
|    learning_rate   | 0.0005   |
|    n_updates       | 498365   |
---------------------------------
Eval num_timesteps=510000, episode_reward=-1.90 +/- 0.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.9     |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | 0.252    |
|    critic_loss     | 0.00532  |
|    learning_rate   | 0.0005   |
|    n_updates       | 504999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.93    |
| time/              |          |
|    episodes        | 256      |
|    fps             | 16       |
|    time_elapsed    | 31870    |
|    total timesteps | 511370   |
| train/             |          |
|    actor_loss      | 0.227    |
|    critic_loss     | 0.00443  |
|    learning_rate   | 0.0005   |
|    n_updates       | 506369   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.94    |
| time/              |          |
|    episodes        | 260      |
|    fps             | 16       |
|    time_elapsed    | 32274    |
|    total timesteps | 519374   |
| train/             |          |
|    actor_loss      | 0.273    |
|    critic_loss     | 0.0038   |
|    learning_rate   | 0.0005   |
|    n_updates       | 514373   |
---------------------------------
Eval num_timesteps=520000, episode_reward=-1.86 +/- 0.48
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.86    |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | 0.284    |
|    critic_loss     | 0.00794  |
|    learning_rate   | 0.0005   |
|    n_updates       | 514999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.99    |
| time/              |          |
|    episodes        | 264      |
|    fps             | 16       |
|    time_elapsed    | 32805    |
|    total timesteps | 527378   |
| train/             |          |
|    actor_loss      | 0.0032   |
|    critic_loss     | 0.0636   |
|    learning_rate   | 0.0005   |
|    n_updates       | 522377   |
---------------------------------
Eval num_timesteps=530000, episode_reward=-1.62 +/- 0.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.62    |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | 0.178    |
|    critic_loss     | 0.0025   |
|    learning_rate   | 0.0005   |
|    n_updates       | 524999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 268      |
|    fps             | 16       |
|    time_elapsed    | 33328    |
|    total timesteps | 535382   |
| train/             |          |
|    actor_loss      | 0.251    |
|    critic_loss     | 0.0051   |
|    learning_rate   | 0.0005   |
|    n_updates       | 530381   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-2.00 +/- 0.11
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2       |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | 0.126    |
|    critic_loss     | 0.0118   |
|    learning_rate   | 0.0005   |
|    n_updates       | 534999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 272      |
|    fps             | 16       |
|    time_elapsed    | 33850    |
|    total timesteps | 543386   |
| train/             |          |
|    actor_loss      | 0.17     |
|    critic_loss     | 0.0109   |
|    learning_rate   | 0.0005   |
|    n_updates       | 538385   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-1.86 +/- 0.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.86    |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | 0.224    |
|    critic_loss     | 0.000175 |
|    learning_rate   | 0.0005   |
|    n_updates       | 544999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 276      |
|    fps             | 16       |
|    time_elapsed    | 34379    |
|    total timesteps | 551390   |
| train/             |          |
|    actor_loss      | 0.259    |
|    critic_loss     | 0.000335 |
|    learning_rate   | 0.0005   |
|    n_updates       | 546389   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 280      |
|    fps             | 16       |
|    time_elapsed    | 34784    |
|    total timesteps | 559394   |
| train/             |          |
|    actor_loss      | 0.129    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.0005   |
|    n_updates       | 554393   |
---------------------------------
Eval num_timesteps=560000, episode_reward=-2.14 +/- 0.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.14    |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | 0.257    |
|    critic_loss     | 0.00342  |
|    learning_rate   | 0.0005   |
|    n_updates       | 554999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.03    |
| time/              |          |
|    episodes        | 284      |
|    fps             | 16       |
|    time_elapsed    | 35311    |
|    total timesteps | 567398   |
| train/             |          |
|    actor_loss      | 0.226    |
|    critic_loss     | 0.00278  |
|    learning_rate   | 0.0005   |
|    n_updates       | 562397   |
---------------------------------
Eval num_timesteps=570000, episode_reward=-2.16 +/- 0.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.16    |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | 0.209    |
|    critic_loss     | 0.000214 |
|    learning_rate   | 0.0005   |
|    n_updates       | 564999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.04    |
| time/              |          |
|    episodes        | 288      |
|    fps             | 16       |
|    time_elapsed    | 35843    |
|    total timesteps | 575402   |
| train/             |          |
|    actor_loss      | 0.359    |
|    critic_loss     | 0.0206   |
|    learning_rate   | 0.0005   |
|    n_updates       | 570401   |
---------------------------------
Eval num_timesteps=580000, episode_reward=-2.08 +/- 0.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | 0.31     |
|    critic_loss     | 0.00689  |
|    learning_rate   | 0.0005   |
|    n_updates       | 574999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.03    |
| time/              |          |
|    episodes        | 292      |
|    fps             | 16       |
|    time_elapsed    | 36370    |
|    total timesteps | 583406   |
| train/             |          |
|    actor_loss      | 0.327    |
|    critic_loss     | 0.014    |
|    learning_rate   | 0.0005   |
|    n_updates       | 578405   |
---------------------------------
Eval num_timesteps=590000, episode_reward=-1.98 +/- 0.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.98    |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | 0.182    |
|    critic_loss     | 0.000825 |
|    learning_rate   | 0.0005   |
|    n_updates       | 584999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.03    |
| time/              |          |
|    episodes        | 296      |
|    fps             | 16       |
|    time_elapsed    | 36902    |
|    total timesteps | 591410   |
| train/             |          |
|    actor_loss      | 0.183    |
|    critic_loss     | 0.000994 |
|    learning_rate   | 0.0005   |
|    n_updates       | 586409   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 300      |
|    fps             | 16       |
|    time_elapsed    | 37307    |
|    total timesteps | 599414   |
| train/             |          |
|    actor_loss      | 0.174    |
|    critic_loss     | 0.002    |
|    learning_rate   | 0.0005   |
|    n_updates       | 594413   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-2.25 +/- 0.25
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.25    |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | 0.205    |
|    critic_loss     | 0.00418  |
|    learning_rate   | 0.0005   |
|    n_updates       | 594999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 304      |
|    fps             | 16       |
|    time_elapsed    | 37838    |
|    total timesteps | 607418   |
| train/             |          |
|    actor_loss      | 0.162    |
|    critic_loss     | 0.000986 |
|    learning_rate   | 0.0005   |
|    n_updates       | 602417   |
---------------------------------
Eval num_timesteps=610000, episode_reward=-1.93 +/- 0.30
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.93    |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | 0.21     |
|    critic_loss     | 0.000258 |
|    learning_rate   | 0.0005   |
|    n_updates       | 604999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.03    |
| time/              |          |
|    episodes        | 308      |
|    fps             | 16       |
|    time_elapsed    | 38372    |
|    total timesteps | 615422   |
| train/             |          |
|    actor_loss      | 0.211    |
|    critic_loss     | 0.0206   |
|    learning_rate   | 0.0005   |
|    n_updates       | 610421   |
---------------------------------
Eval num_timesteps=620000, episode_reward=-1.84 +/- 0.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.84    |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | 0.225    |
|    critic_loss     | 0.00454  |
|    learning_rate   | 0.0005   |
|    n_updates       | 614999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.01    |
| time/              |          |
|    episodes        | 312      |
|    fps             | 16       |
|    time_elapsed    | 38903    |
|    total timesteps | 623426   |
| train/             |          |
|    actor_loss      | 0.183    |
|    critic_loss     | 0.00014  |
|    learning_rate   | 0.0005   |
|    n_updates       | 618425   |
---------------------------------
Eval num_timesteps=630000, episode_reward=-1.82 +/- 0.31
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.82    |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | 0.181    |
|    critic_loss     | 0.00559  |
|    learning_rate   | 0.0005   |
|    n_updates       | 624999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 316      |
|    fps             | 16       |
|    time_elapsed    | 39431    |
|    total timesteps | 631430   |
| train/             |          |
|    actor_loss      | 0.23     |
|    critic_loss     | 0.00418  |
|    learning_rate   | 0.0005   |
|    n_updates       | 626429   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.05    |
| time/              |          |
|    episodes        | 320      |
|    fps             | 16       |
|    time_elapsed    | 39836    |
|    total timesteps | 639434   |
| train/             |          |
|    actor_loss      | 0.0725   |
|    critic_loss     | 0.0483   |
|    learning_rate   | 0.0005   |
|    n_updates       | 634433   |
---------------------------------
Eval num_timesteps=640000, episode_reward=-2.14 +/- 0.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.14    |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | 0.356    |
|    critic_loss     | 0.0153   |
|    learning_rate   | 0.0005   |
|    n_updates       | 634999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.01    |
| time/              |          |
|    episodes        | 324      |
|    fps             | 16       |
|    time_elapsed    | 40366    |
|    total timesteps | 647438   |
| train/             |          |
|    actor_loss      | 0.253    |
|    critic_loss     | 0.0173   |
|    learning_rate   | 0.0005   |
|    n_updates       | 642437   |
---------------------------------
Eval num_timesteps=650000, episode_reward=-1.92 +/- 0.38
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.92    |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | 0.268    |
|    critic_loss     | 0.00708  |
|    learning_rate   | 0.0005   |
|    n_updates       | 644999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 328      |
|    fps             | 16       |
|    time_elapsed    | 40898    |
|    total timesteps | 655442   |
| train/             |          |
|    actor_loss      | 0.168    |
|    critic_loss     | 0.00321  |
|    learning_rate   | 0.0005   |
|    n_updates       | 650441   |
---------------------------------
Eval num_timesteps=660000, episode_reward=-1.94 +/- 0.14
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.94    |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | 0.208    |
|    critic_loss     | 0.00277  |
|    learning_rate   | 0.0005   |
|    n_updates       | 654999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.01    |
| time/              |          |
|    episodes        | 332      |
|    fps             | 16       |
|    time_elapsed    | 41427    |
|    total timesteps | 663446   |
| train/             |          |
|    actor_loss      | 0.254    |
|    critic_loss     | 0.000329 |
|    learning_rate   | 0.0005   |
|    n_updates       | 658445   |
---------------------------------
Eval num_timesteps=670000, episode_reward=-2.06 +/- 0.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.06    |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | 0.202    |
|    critic_loss     | 0.0124   |
|    learning_rate   | 0.0005   |
|    n_updates       | 664999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.01    |
| time/              |          |
|    episodes        | 336      |
|    fps             | 16       |
|    time_elapsed    | 41945    |
|    total timesteps | 671450   |
| train/             |          |
|    actor_loss      | 0.284    |
|    critic_loss     | 0.00375  |
|    learning_rate   | 0.0005   |
|    n_updates       | 666449   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 340      |
|    fps             | 16       |
|    time_elapsed    | 42347    |
|    total timesteps | 679454   |
| train/             |          |
|    actor_loss      | 0.219    |
|    critic_loss     | 0.00586  |
|    learning_rate   | 0.0005   |
|    n_updates       | 674453   |
---------------------------------
Eval num_timesteps=680000, episode_reward=-2.04 +/- 0.15
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.04    |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | 0.404    |
|    critic_loss     | 0.0273   |
|    learning_rate   | 0.0005   |
|    n_updates       | 674999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2       |
| time/              |          |
|    episodes        | 344      |
|    fps             | 16       |
|    time_elapsed    | 42867    |
|    total timesteps | 687458   |
| train/             |          |
|    actor_loss      | 0.399    |
|    critic_loss     | 0.0189   |
|    learning_rate   | 0.0005   |
|    n_updates       | 682457   |
---------------------------------
Eval num_timesteps=690000, episode_reward=-2.00 +/- 0.43
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2       |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | 0.319    |
|    critic_loss     | 0.00327  |
|    learning_rate   | 0.0005   |
|    n_updates       | 684999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.02    |
| time/              |          |
|    episodes        | 348      |
|    fps             | 16       |
|    time_elapsed    | 43390    |
|    total timesteps | 695462   |
| train/             |          |
|    actor_loss      | 0.0973   |
|    critic_loss     | 0.0243   |
|    learning_rate   | 0.0005   |
|    n_updates       | 690461   |
---------------------------------
Eval num_timesteps=700000, episode_reward=-2.00 +/- 0.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2       |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | 0.242    |
|    critic_loss     | 0.00183  |
|    learning_rate   | 0.0005   |
|    n_updates       | 694999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.99    |
| time/              |          |
|    episodes        | 352      |
|    fps             | 16       |
|    time_elapsed    | 43922    |
|    total timesteps | 703466   |
| train/             |          |
|    actor_loss      | 0.21     |
|    critic_loss     | 0.000773 |
|    learning_rate   | 0.0005   |
|    n_updates       | 698465   |
---------------------------------
Eval num_timesteps=710000, episode_reward=-1.94 +/- 0.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.94    |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | 0.229    |
|    critic_loss     | 0.00134  |
|    learning_rate   | 0.0005   |
|    n_updates       | 704999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.01    |
| time/              |          |
|    episodes        | 356      |
|    fps             | 16       |
|    time_elapsed    | 44450    |
|    total timesteps | 711470   |
| train/             |          |
|    actor_loss      | 0.111    |
|    critic_loss     | 0.0151   |
|    learning_rate   | 0.0005   |
|    n_updates       | 706469   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.98    |
| time/              |          |
|    episodes        | 360      |
|    fps             | 16       |
|    time_elapsed    | 44838    |
|    total timesteps | 719264   |
| train/             |          |
|    actor_loss      | 0.301    |
|    critic_loss     | 0.00306  |
|    learning_rate   | 0.0005   |
|    n_updates       | 714263   |
---------------------------------
Eval num_timesteps=720000, episode_reward=-1.88 +/- 0.33
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.88    |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 0.0126   |
|    learning_rate   | 0.0005   |
|    n_updates       | 714999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.95    |
| time/              |          |
|    episodes        | 364      |
|    fps             | 16       |
|    time_elapsed    | 45361    |
|    total timesteps | 727268   |
| train/             |          |
|    actor_loss      | 0.282    |
|    critic_loss     | 0.00128  |
|    learning_rate   | 0.0005   |
|    n_updates       | 722267   |
---------------------------------
Eval num_timesteps=730000, episode_reward=-1.85 +/- 0.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -1.85    |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | 0.227    |
|    critic_loss     | 0.00192  |
|    learning_rate   | 0.0005   |
|    n_updates       | 724999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.97    |
| time/              |          |
|    episodes        | 368      |
|    fps             | 16       |
|    time_elapsed    | 45882    |
|    total timesteps | 735272   |
| train/             |          |
|    actor_loss      | 0.273    |
|    critic_loss     | 0.00247  |
|    learning_rate   | 0.0005   |
|    n_updates       | 730271   |
---------------------------------
Eval num_timesteps=740000, episode_reward=-2.11 +/- 0.17
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.11    |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | 0.287    |
|    critic_loss     | 0.000839 |
|    learning_rate   | 0.0005   |
|    n_updates       | 734999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.98    |
| time/              |          |
|    episodes        | 372      |
|    fps             | 16       |
|    time_elapsed    | 46412    |
|    total timesteps | 743276   |
| train/             |          |
|    actor_loss      | 0.238    |
|    critic_loss     | 0.00202  |
|    learning_rate   | 0.0005   |
|    n_updates       | 738275   |
---------------------------------
Eval num_timesteps=750000, episode_reward=-2.08 +/- 0.15
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | 0.176    |
|    critic_loss     | 0.00893  |
|    learning_rate   | 0.0005   |
|    n_updates       | 744999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.95    |
| time/              |          |
|    episodes        | 376      |
|    fps             | 16       |
|    time_elapsed    | 46942    |
|    total timesteps | 751280   |
| train/             |          |
|    actor_loss      | 0.294    |
|    critic_loss     | 0.0034   |
|    learning_rate   | 0.0005   |
|    n_updates       | 746279   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.96    |
| time/              |          |
|    episodes        | 380      |
|    fps             | 16       |
|    time_elapsed    | 47345    |
|    total timesteps | 759284   |
| train/             |          |
|    actor_loss      | 0.16     |
|    critic_loss     | 0.0251   |
|    learning_rate   | 0.0005   |
|    n_updates       | 754283   |
---------------------------------
Eval num_timesteps=760000, episode_reward=-2.08 +/- 0.15
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | 0.318    |
|    critic_loss     | 0.055    |
|    learning_rate   | 0.0005   |
|    n_updates       | 754999   |
---------------------------------
Terminated
2021-12-09 14:08:10.830194: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-09 14:08:10.830260: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp11/TD3_33
Found 1 GPUs for rendering. Using device 0.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 38       |
|    time_elapsed    | 206      |
|    total timesteps | 8004     |
| train/             |          |
|    actor_loss      | 1.03     |
|    critic_loss     | 3.9      |
|    learning_rate   | 0.0005   |
|    n_updates       | 3003     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-27.84 +/- 18.08
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27.8    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.34     |
|    critic_loss     | 2.02     |
|    learning_rate   | 0.0005   |
|    n_updates       | 4999     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -55.7    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 22       |
|    time_elapsed    | 727      |
|    total timesteps | 16008    |
| train/             |          |
|    actor_loss      | 1.17     |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 11007    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-19.05 +/- 37.50
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19      |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 0.962    |
|    critic_loss     | 1.98     |
|    learning_rate   | 0.0005   |
|    n_updates       | 14999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -27.9    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 19       |
|    time_elapsed    | 1251     |
|    total timesteps | 24012    |
| train/             |          |
|    actor_loss      | 1.16     |
|    critic_loss     | 1.34     |
|    learning_rate   | 0.0005   |
|    n_updates       | 19011    |
---------------------------------
Eval num_timesteps=30000, episode_reward=3.39 +/- 12.07
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 3.39     |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 1.01     |
|    critic_loss     | 0.501    |
|    learning_rate   | 0.0005   |
|    n_updates       | 24999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -23      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 18       |
|    time_elapsed    | 1777     |
|    total timesteps | 32016    |
| train/             |          |
|    actor_loss      | 0.768    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 27015    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-36.35 +/- 23.21
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -36.4    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 0.778    |
|    critic_loss     | 1.27     |
|    learning_rate   | 0.0005   |
|    n_updates       | 34999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -16.7    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 17       |
|    time_elapsed    | 2302     |
|    total timesteps | 40020    |
| train/             |          |
|    actor_loss      | 0.746    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 35019    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -23.6    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 17       |
|    time_elapsed    | 2703     |
|    total timesteps | 48024    |
| train/             |          |
|    actor_loss      | 0.877    |
|    critic_loss     | 2.02     |
|    learning_rate   | 0.0005   |
|    n_updates       | 43023    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-27.65 +/- 44.87
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27.7    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.645    |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.0005   |
|    n_updates       | 44999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -23      |
| time/              |          |
|    episodes        | 28       |
|    fps             | 17       |
|    time_elapsed    | 3228     |
|    total timesteps | 56028    |
| train/             |          |
|    actor_loss      | 0.679    |
|    critic_loss     | 2.32     |
|    learning_rate   | 0.0005   |
|    n_updates       | 51027    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-11.57 +/- 38.02
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -11.6    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 0.638    |
|    critic_loss     | 1.18     |
|    learning_rate   | 0.0005   |
|    n_updates       | 54999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -14.8    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 17       |
|    time_elapsed    | 3752     |
|    total timesteps | 64032    |
| train/             |          |
|    actor_loss      | 0.492    |
|    critic_loss     | 1.99     |
|    learning_rate   | 0.0005   |
|    n_updates       | 59031    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-28.16 +/- 14.89
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -28.2    |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 0.585    |
|    critic_loss     | 1.32     |
|    learning_rate   | 0.0005   |
|    n_updates       | 64999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.64    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 16       |
|    time_elapsed    | 4273     |
|    total timesteps | 72036    |
| train/             |          |
|    actor_loss      | 0.385    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 67035    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-22.07 +/- 30.75
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -22.1    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 0.378    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 74999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.2     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 16       |
|    time_elapsed    | 4795     |
|    total timesteps | 80040    |
| train/             |          |
|    actor_loss      | 0.428    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 75039    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.14    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 16       |
|    time_elapsed    | 5195     |
|    total timesteps | 88044    |
| train/             |          |
|    actor_loss      | 0.377    |
|    critic_loss     | 0.427    |
|    learning_rate   | 0.0005   |
|    n_updates       | 83043    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-13.28 +/- 29.73
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -13.3    |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 0.473    |
|    critic_loss     | 1.21     |
|    learning_rate   | 0.0005   |
|    n_updates       | 84999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.71    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 16       |
|    time_elapsed    | 5715     |
|    total timesteps | 96024    |
| train/             |          |
|    actor_loss      | 0.512    |
|    critic_loss     | 1.58     |
|    learning_rate   | 0.0005   |
|    n_updates       | 91023    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-20.04 +/- 26.36
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -20      |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 0.315    |
|    critic_loss     | 0.765    |
|    learning_rate   | 0.0005   |
|    n_updates       | 94999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.97    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 16       |
|    time_elapsed    | 6240     |
|    total timesteps | 104028   |
| train/             |          |
|    actor_loss      | 0.211    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 99027    |
---------------------------------
Eval num_timesteps=110000, episode_reward=-26.99 +/- 26.77
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27      |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 0.268    |
|    critic_loss     | 1.96     |
|    learning_rate   | 0.0005   |
|    n_updates       | 104999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.33    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 16       |
|    time_elapsed    | 6764     |
|    total timesteps | 112032   |
| train/             |          |
|    actor_loss      | 0.295    |
|    critic_loss     | 1.25     |
|    learning_rate   | 0.0005   |
|    n_updates       | 107031   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-29.31 +/- 30.55
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29.3    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 0.492    |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.0005   |
|    n_updates       | 114999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.47    |
| time/              |          |
|    episodes        | 60       |
|    fps             | 16       |
|    time_elapsed    | 7286     |
|    total timesteps | 120036   |
| train/             |          |
|    actor_loss      | 0.28     |
|    critic_loss     | 1.77     |
|    learning_rate   | 0.0005   |
|    n_updates       | 115035   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.05    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 16       |
|    time_elapsed    | 7686     |
|    total timesteps | 128040   |
| train/             |          |
|    actor_loss      | 0.586    |
|    critic_loss     | 0.965    |
|    learning_rate   | 0.0005   |
|    n_updates       | 123039   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-29.70 +/- 40.51
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29.7    |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 0.393    |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.0005   |
|    n_updates       | 124999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.7     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 16       |
|    time_elapsed    | 8206     |
|    total timesteps | 136044   |
| train/             |          |
|    actor_loss      | 0.575    |
|    critic_loss     | 0.945    |
|    learning_rate   | 0.0005   |
|    n_updates       | 131043   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-19.17 +/- 34.11
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19.2    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 0.459    |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.0005   |
|    n_updates       | 134999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.3     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 16       |
|    time_elapsed    | 8729     |
|    total timesteps | 144048   |
| train/             |          |
|    actor_loss      | 0.302    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 139047   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-27.50 +/- 24.48
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27.5    |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 0.17     |
|    critic_loss     | 0.434    |
|    learning_rate   | 0.0005   |
|    n_updates       | 144999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.99    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 16       |
|    time_elapsed    | 9250     |
|    total timesteps | 152052   |
| train/             |          |
|    actor_loss      | -0.0529  |
|    critic_loss     | 1.67     |
|    learning_rate   | 0.0005   |
|    n_updates       | 147051   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-10.99 +/- 13.15
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -11      |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 0.425    |
|    critic_loss     | 1.73     |
|    learning_rate   | 0.0005   |
|    n_updates       | 154999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.19    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 16       |
|    time_elapsed    | 9773     |
|    total timesteps | 160056   |
| train/             |          |
|    actor_loss      | 0.33     |
|    critic_loss     | 0.463    |
|    learning_rate   | 0.0005   |
|    n_updates       | 155055   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.32    |
| time/              |          |
|    episodes        | 84       |
|    fps             | 16       |
|    time_elapsed    | 10174    |
|    total timesteps | 168060   |
| train/             |          |
|    actor_loss      | 0.147    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 163059   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-38.02 +/- 17.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -38      |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 0.428    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.0005   |
|    n_updates       | 164999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.06    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 16       |
|    time_elapsed    | 10698    |
|    total timesteps | 176064   |
| train/             |          |
|    actor_loss      | 0.237    |
|    critic_loss     | 0.48     |
|    learning_rate   | 0.0005   |
|    n_updates       | 171063   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-24.08 +/- 34.86
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -24.1    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 0.309    |
|    critic_loss     | 0.843    |
|    learning_rate   | 0.0005   |
|    n_updates       | 174999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -8.45    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 16       |
|    time_elapsed    | 11225    |
|    total timesteps | 184068   |
| train/             |          |
|    actor_loss      | 0.328    |
|    critic_loss     | 1.97     |
|    learning_rate   | 0.0005   |
|    n_updates       | 179067   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-42.13 +/- 20.77
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -42.1    |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 0.13     |
|    critic_loss     | 0.505    |
|    learning_rate   | 0.0005   |
|    n_updates       | 184999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.54    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 16       |
|    time_elapsed    | 11751    |
|    total timesteps | 192072   |
| train/             |          |
|    actor_loss      | 0.297    |
|    critic_loss     | 0.797    |
|    learning_rate   | 0.0005   |
|    n_updates       | 187071   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-29.38 +/- 18.05
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29.4    |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 0.356    |
|    critic_loss     | 1.19     |
|    learning_rate   | 0.0005   |
|    n_updates       | 194999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.08    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 16       |
|    time_elapsed    | 12274    |
|    total timesteps | 200076   |
| train/             |          |
|    actor_loss      | 0.534    |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.0005   |
|    n_updates       | 195075   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -1.48    |
| time/              |          |
|    episodes        | 104      |
|    fps             | 16       |
|    time_elapsed    | 12637    |
|    total timesteps | 207333   |
| train/             |          |
|    actor_loss      | 0.158    |
|    critic_loss     | 0.0709   |
|    learning_rate   | 0.0005   |
|    n_updates       | 202332   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-15.64 +/- 25.35
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -15.6    |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | 0.282    |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.0005   |
|    n_updates       | 204999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 0.0208   |
| time/              |          |
|    episodes        | 108      |
|    fps             | 16       |
|    time_elapsed    | 13155    |
|    total timesteps | 215337   |
| train/             |          |
|    actor_loss      | 0.245    |
|    critic_loss     | 1.39     |
|    learning_rate   | 0.0005   |
|    n_updates       | 210336   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-31.57 +/- 27.10
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -31.6    |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 0.074    |
|    critic_loss     | 1.28     |
|    learning_rate   | 0.0005   |
|    n_updates       | 214999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -0.351   |
| time/              |          |
|    episodes        | 112      |
|    fps             | 16       |
|    time_elapsed    | 13674    |
|    total timesteps | 223341   |
| train/             |          |
|    actor_loss      | 0.391    |
|    critic_loss     | 2.18     |
|    learning_rate   | 0.0005   |
|    n_updates       | 218340   |
---------------------------------
Eval num_timesteps=230000, episode_reward=-30.22 +/- 26.64
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -30.2    |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 0.41     |
|    critic_loss     | 1.28     |
|    learning_rate   | 0.0005   |
|    n_updates       | 224999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -0.775   |
| time/              |          |
|    episodes        | 116      |
|    fps             | 16       |
|    time_elapsed    | 14197    |
|    total timesteps | 231345   |
| train/             |          |
|    actor_loss      | 0.311    |
|    critic_loss     | 0.866    |
|    learning_rate   | 0.0005   |
|    n_updates       | 226344   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | -0.952   |
| time/              |          |
|    episodes        | 120      |
|    fps             | 16       |
|    time_elapsed    | 14600    |
|    total timesteps | 239349   |
| train/             |          |
|    actor_loss      | 0.241    |
|    critic_loss     | 2.16     |
|    learning_rate   | 0.0005   |
|    n_updates       | 234348   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-11.28 +/- 11.44
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -11.3    |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.469    |
|    critic_loss     | 1.32     |
|    learning_rate   | 0.0005   |
|    n_updates       | 234999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.66     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 16       |
|    time_elapsed    | 15121    |
|    total timesteps | 247353   |
| train/             |          |
|    actor_loss      | 0.29     |
|    critic_loss     | 0.83     |
|    learning_rate   | 0.0005   |
|    n_updates       | 242352   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-13.31 +/- 15.57
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -13.3    |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | 0.249    |
|    critic_loss     | 1.62     |
|    learning_rate   | 0.0005   |
|    n_updates       | 244999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.36     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 16       |
|    time_elapsed    | 15639    |
|    total timesteps | 255357   |
| train/             |          |
|    actor_loss      | 0.185    |
|    critic_loss     | 2.07     |
|    learning_rate   | 0.0005   |
|    n_updates       | 250356   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-25.73 +/- 40.41
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -25.7    |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 0.206    |
|    critic_loss     | 1.6      |
|    learning_rate   | 0.0005   |
|    n_updates       | 254999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.47     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 16       |
|    time_elapsed    | 16159    |
|    total timesteps | 263361   |
| train/             |          |
|    actor_loss      | 0.124    |
|    critic_loss     | 1.24     |
|    learning_rate   | 0.0005   |
|    n_updates       | 258360   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-36.07 +/- 47.16
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -36.1    |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | 0.389    |
|    critic_loss     | 2        |
|    learning_rate   | 0.0005   |
|    n_updates       | 264999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.88     |
| time/              |          |
|    episodes        | 136      |
|    fps             | 16       |
|    time_elapsed    | 16684    |
|    total timesteps | 271365   |
| train/             |          |
|    actor_loss      | 0.223    |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.0005   |
|    n_updates       | 266364   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.45     |
| time/              |          |
|    episodes        | 140      |
|    fps             | 16       |
|    time_elapsed    | 17087    |
|    total timesteps | 279369   |
| train/             |          |
|    actor_loss      | 0.0865   |
|    critic_loss     | 0.538    |
|    learning_rate   | 0.0005   |
|    n_updates       | 274368   |
---------------------------------
Eval num_timesteps=280000, episode_reward=2.33 +/- 13.74
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.33     |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 0.212    |
|    critic_loss     | 0.0749   |
|    learning_rate   | 0.0005   |
|    n_updates       | 274999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 4.64     |
| time/              |          |
|    episodes        | 144      |
|    fps             | 16       |
|    time_elapsed    | 17612    |
|    total timesteps | 287373   |
| train/             |          |
|    actor_loss      | -0.106   |
|    critic_loss     | 0.799    |
|    learning_rate   | 0.0005   |
|    n_updates       | 282372   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-33.51 +/- 17.53
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -33.5    |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | 0.186    |
|    critic_loss     | 0.0848   |
|    learning_rate   | 0.0005   |
|    n_updates       | 284999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.65     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 16       |
|    time_elapsed    | 18133    |
|    total timesteps | 295377   |
| train/             |          |
|    actor_loss      | 0.187    |
|    critic_loss     | 0.44     |
|    learning_rate   | 0.0005   |
|    n_updates       | 290376   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-38.51 +/- 29.26
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -38.5    |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 0.253    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 294999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 3.54     |
| time/              |          |
|    episodes        | 152      |
|    fps             | 16       |
|    time_elapsed    | 18655    |
|    total timesteps | 303381   |
| train/             |          |
|    actor_loss      | 0.107    |
|    critic_loss     | 1.24     |
|    learning_rate   | 0.0005   |
|    n_updates       | 298380   |
---------------------------------
Eval num_timesteps=310000, episode_reward=-35.35 +/- 20.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -35.3    |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -0.0845  |
|    critic_loss     | 1.34     |
|    learning_rate   | 0.0005   |
|    n_updates       | 304999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 1.68     |
| time/              |          |
|    episodes        | 156      |
|    fps             | 16       |
|    time_elapsed    | 19174    |
|    total timesteps | 311385   |
| train/             |          |
|    actor_loss      | 0.0825   |
|    critic_loss     | 1.44     |
|    learning_rate   | 0.0005   |
|    n_updates       | 306384   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.46     |
| time/              |          |
|    episodes        | 160      |
|    fps             | 16       |
|    time_elapsed    | 19577    |
|    total timesteps | 319389   |
| train/             |          |
|    actor_loss      | 0.142    |
|    critic_loss     | 0.78     |
|    learning_rate   | 0.0005   |
|    n_updates       | 314388   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-20.34 +/- 34.98
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -20.3    |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 0.318    |
|    critic_loss     | 1.51     |
|    learning_rate   | 0.0005   |
|    n_updates       | 314999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.39     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 16       |
|    time_elapsed    | 20103    |
|    total timesteps | 327393   |
| train/             |          |
|    actor_loss      | 0.0113   |
|    critic_loss     | 0.107    |
|    learning_rate   | 0.0005   |
|    n_updates       | 322392   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-29.30 +/- 36.38
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29.3    |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -0.0311  |
|    critic_loss     | 0.522    |
|    learning_rate   | 0.0005   |
|    n_updates       | 324999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 5.67     |
| time/              |          |
|    episodes        | 168      |
|    fps             | 16       |
|    time_elapsed    | 20628    |
|    total timesteps | 335397   |
| train/             |          |
|    actor_loss      | 0.0296   |
|    critic_loss     | 0.873    |
|    learning_rate   | 0.0005   |
|    n_updates       | 330396   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-16.81 +/- 20.42
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -16.8    |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -0.0875  |
|    critic_loss     | 0.907    |
|    learning_rate   | 0.0005   |
|    n_updates       | 334999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 6.7      |
| time/              |          |
|    episodes        | 172      |
|    fps             | 16       |
|    time_elapsed    | 21153    |
|    total timesteps | 343401   |
| train/             |          |
|    actor_loss      | 0.00348  |
|    critic_loss     | 0.469    |
|    learning_rate   | 0.0005   |
|    n_updates       | 338400   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-29.24 +/- 25.77
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -29.2    |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | 0.114    |
|    critic_loss     | 1.65     |
|    learning_rate   | 0.0005   |
|    n_updates       | 344999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 8.44     |
| time/              |          |
|    episodes        | 176      |
|    fps             | 16       |
|    time_elapsed    | 21676    |
|    total timesteps | 351405   |
| train/             |          |
|    actor_loss      | 0.26     |
|    critic_loss     | 1.62     |
|    learning_rate   | 0.0005   |
|    n_updates       | 346404   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    episodes        | 180      |
|    fps             | 16       |
|    time_elapsed    | 22080    |
|    total timesteps | 359409   |
| train/             |          |
|    actor_loss      | 0.0136   |
|    critic_loss     | 0.0854   |
|    learning_rate   | 0.0005   |
|    n_updates       | 354408   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-15.01 +/- 24.55
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -15      |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -0.09    |
|    critic_loss     | 0.525    |
|    learning_rate   | 0.0005   |
|    n_updates       | 354999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 8.92     |
| time/              |          |
|    episodes        | 184      |
|    fps             | 16       |
|    time_elapsed    | 22602    |
|    total timesteps | 367413   |
| train/             |          |
|    actor_loss      | -0.166   |
|    critic_loss     | 0.0464   |
|    learning_rate   | 0.0005   |
|    n_updates       | 362412   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-7.41 +/- 11.98
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -7.41    |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | 0.243    |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.0005   |
|    n_updates       | 364999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    episodes        | 188      |
|    fps             | 16       |
|    time_elapsed    | 23124    |
|    total timesteps | 375417   |
| train/             |          |
|    actor_loss      | 0.148    |
|    critic_loss     | 0.444    |
|    learning_rate   | 0.0005   |
|    n_updates       | 370416   |
---------------------------------
Eval num_timesteps=380000, episode_reward=2.41 +/- 25.00
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 2.41     |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | 0.0521   |
|    critic_loss     | 1.69     |
|    learning_rate   | 0.0005   |
|    n_updates       | 374999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    episodes        | 192      |
|    fps             | 16       |
|    time_elapsed    | 23645    |
|    total timesteps | 383421   |
| train/             |          |
|    actor_loss      | -0.172   |
|    critic_loss     | 0.817    |
|    learning_rate   | 0.0005   |
|    n_updates       | 378420   |
---------------------------------
Eval num_timesteps=390000, episode_reward=-33.65 +/- 16.83
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -33.6    |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | 0.0678   |
|    critic_loss     | 1.16     |
|    learning_rate   | 0.0005   |
|    n_updates       | 384999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 8.83     |
| time/              |          |
|    episodes        | 196      |
|    fps             | 16       |
|    time_elapsed    | 24168    |
|    total timesteps | 391425   |
| train/             |          |
|    actor_loss      | 0.0911   |
|    critic_loss     | 0.0983   |
|    learning_rate   | 0.0005   |
|    n_updates       | 386424   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 10       |
| time/              |          |
|    episodes        | 200      |
|    fps             | 16       |
|    time_elapsed    | 24568    |
|    total timesteps | 399429   |
| train/             |          |
|    actor_loss      | 0.22     |
|    critic_loss     | 1.42     |
|    learning_rate   | 0.0005   |
|    n_updates       | 394428   |
---------------------------------
Eval num_timesteps=400000, episode_reward=15.85 +/- 24.52
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | 0.366    |
|    critic_loss     | 0.568    |
|    learning_rate   | 0.0005   |
|    n_updates       | 394999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    episodes        | 204      |
|    fps             | 16       |
|    time_elapsed    | 25095    |
|    total timesteps | 407433   |
| train/             |          |
|    actor_loss      | 0.314    |
|    critic_loss     | 1.66     |
|    learning_rate   | 0.0005   |
|    n_updates       | 402432   |
---------------------------------
Eval num_timesteps=410000, episode_reward=-11.76 +/- 26.93
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -11.8    |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | 0.0829   |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 404999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    episodes        | 208      |
|    fps             | 16       |
|    time_elapsed    | 25618    |
|    total timesteps | 415437   |
| train/             |          |
|    actor_loss      | 0.0278   |
|    critic_loss     | 0.483    |
|    learning_rate   | 0.0005   |
|    n_updates       | 410436   |
---------------------------------
Eval num_timesteps=420000, episode_reward=-16.88 +/- 14.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -16.9    |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | 0.342    |
|    critic_loss     | 1.28     |
|    learning_rate   | 0.0005   |
|    n_updates       | 414999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 11.2     |
| time/              |          |
|    episodes        | 212      |
|    fps             | 16       |
|    time_elapsed    | 26144    |
|    total timesteps | 423441   |
| train/             |          |
|    actor_loss      | -0.176   |
|    critic_loss     | 0.851    |
|    learning_rate   | 0.0005   |
|    n_updates       | 418440   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-6.93 +/- 17.05
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -6.93    |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | 0.0374   |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.0005   |
|    n_updates       | 424999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 9.86     |
| time/              |          |
|    episodes        | 216      |
|    fps             | 16       |
|    time_elapsed    | 26668    |
|    total timesteps | 431445   |
| train/             |          |
|    actor_loss      | -0.177   |
|    critic_loss     | 0.299    |
|    learning_rate   | 0.0005   |
|    n_updates       | 426444   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2e+03     |
|    ep_rew_mean     | 10.2      |
| time/              |           |
|    episodes        | 220       |
|    fps             | 16        |
|    time_elapsed    | 27071     |
|    total timesteps | 439449    |
| train/             |           |
|    actor_loss      | -0.000216 |
|    critic_loss     | 0.0706    |
|    learning_rate   | 0.0005    |
|    n_updates       | 434448    |
----------------------------------
Eval num_timesteps=440000, episode_reward=-33.73 +/- 31.06
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -33.7    |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | 0.102    |
|    critic_loss     | 1.3      |
|    learning_rate   | 0.0005   |
|    n_updates       | 434999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 9.75     |
| time/              |          |
|    episodes        | 224      |
|    fps             | 16       |
|    time_elapsed    | 27590    |
|    total timesteps | 447453   |
| train/             |          |
|    actor_loss      | 0.203    |
|    critic_loss     | 2        |
|    learning_rate   | 0.0005   |
|    n_updates       | 442452   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-14.90 +/- 20.53
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.9    |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | 0.333    |
|    critic_loss     | 0.201    |
|    learning_rate   | 0.0005   |
|    n_updates       | 444999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 7.99     |
| time/              |          |
|    episodes        | 228      |
|    fps             | 16       |
|    time_elapsed    | 28117    |
|    total timesteps | 455457   |
| train/             |          |
|    actor_loss      | 0.239    |
|    critic_loss     | 1.33     |
|    learning_rate   | 0.0005   |
|    n_updates       | 450456   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-11.01 +/- 18.20
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -11      |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | 0.113    |
|    critic_loss     | 0.819    |
|    learning_rate   | 0.0005   |
|    n_updates       | 454999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 8        |
| time/              |          |
|    episodes        | 232      |
|    fps             | 16       |
|    time_elapsed    | 28640    |
|    total timesteps | 463461   |
| train/             |          |
|    actor_loss      | -0.109   |
|    critic_loss     | 0.536    |
|    learning_rate   | 0.0005   |
|    n_updates       | 458460   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-25.77 +/- 19.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -25.8    |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | 0.0878   |
|    critic_loss     | 0.0732   |
|    learning_rate   | 0.0005   |
|    n_updates       | 464999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 7.59     |
| time/              |          |
|    episodes        | 236      |
|    fps             | 16       |
|    time_elapsed    | 29162    |
|    total timesteps | 471465   |
| train/             |          |
|    actor_loss      | 0.258    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 466464   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 7.83     |
| time/              |          |
|    episodes        | 240      |
|    fps             | 16       |
|    time_elapsed    | 29567    |
|    total timesteps | 479469   |
| train/             |          |
|    actor_loss      | 0.00838  |
|    critic_loss     | 0.934    |
|    learning_rate   | 0.0005   |
|    n_updates       | 474468   |
---------------------------------
Eval num_timesteps=480000, episode_reward=-51.40 +/- 14.94
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -51.4    |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | 0.147    |
|    critic_loss     | 1.25     |
|    learning_rate   | 0.0005   |
|    n_updates       | 474999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.04     |
| time/              |          |
|    episodes        | 244      |
|    fps             | 16       |
|    time_elapsed    | 30094    |
|    total timesteps | 487473   |
| train/             |          |
|    actor_loss      | 0.112    |
|    critic_loss     | 0.845    |
|    learning_rate   | 0.0005   |
|    n_updates       | 482472   |
---------------------------------
Eval num_timesteps=490000, episode_reward=-13.88 +/- 26.19
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -13.9    |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | 0.137    |
|    critic_loss     | 0.0862   |
|    learning_rate   | 0.0005   |
|    n_updates       | 484999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.95     |
| time/              |          |
|    episodes        | 248      |
|    fps             | 16       |
|    time_elapsed    | 30618    |
|    total timesteps | 495477   |
| train/             |          |
|    actor_loss      | -0.027   |
|    critic_loss     | 0.919    |
|    learning_rate   | 0.0005   |
|    n_updates       | 490476   |
---------------------------------
Eval num_timesteps=500000, episode_reward=-24.80 +/- 22.95
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -24.8    |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | 0.0521   |
|    critic_loss     | 0.451    |
|    learning_rate   | 0.0005   |
|    n_updates       | 494999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.77     |
| time/              |          |
|    episodes        | 252      |
|    fps             | 16       |
|    time_elapsed    | 31138    |
|    total timesteps | 503481   |
| train/             |          |
|    actor_loss      | 0.229    |
|    critic_loss     | 0.129    |
|    learning_rate   | 0.0005   |
|    n_updates       | 498480   |
---------------------------------
Eval num_timesteps=510000, episode_reward=-20.52 +/- 26.63
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -20.5    |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -0.217   |
|    critic_loss     | 0.653    |
|    learning_rate   | 0.0005   |
|    n_updates       | 504999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 7.82     |
| time/              |          |
|    episodes        | 256      |
|    fps             | 16       |
|    time_elapsed    | 31659    |
|    total timesteps | 511485   |
| train/             |          |
|    actor_loss      | 0.125    |
|    critic_loss     | 0.458    |
|    learning_rate   | 0.0005   |
|    n_updates       | 506484   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.88     |
| time/              |          |
|    episodes        | 260      |
|    fps             | 16       |
|    time_elapsed    | 32058    |
|    total timesteps | 519489   |
| train/             |          |
|    actor_loss      | 0.267    |
|    critic_loss     | 0.943    |
|    learning_rate   | 0.0005   |
|    n_updates       | 514488   |
---------------------------------
Eval num_timesteps=520000, episode_reward=-42.10 +/- 18.37
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -42.1    |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | 0.0605   |
|    critic_loss     | 0.0472   |
|    learning_rate   | 0.0005   |
|    n_updates       | 514999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.26     |
| time/              |          |
|    episodes        | 264      |
|    fps             | 16       |
|    time_elapsed    | 32581    |
|    total timesteps | 527493   |
| train/             |          |
|    actor_loss      | -0.0297  |
|    critic_loss     | 1.28     |
|    learning_rate   | 0.0005   |
|    n_updates       | 522492   |
---------------------------------
Eval num_timesteps=530000, episode_reward=-19.69 +/- 25.91
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19.7    |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -0.113   |
|    critic_loss     | 1.38     |
|    learning_rate   | 0.0005   |
|    n_updates       | 524999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.94     |
| time/              |          |
|    episodes        | 268      |
|    fps             | 16       |
|    time_elapsed    | 33103    |
|    total timesteps | 535497   |
| train/             |          |
|    actor_loss      | 0.318    |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.0005   |
|    n_updates       | 530496   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-6.49 +/- 10.79
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -6.49    |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | 0.243    |
|    critic_loss     | 0.882    |
|    learning_rate   | 0.0005   |
|    n_updates       | 534999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.34     |
| time/              |          |
|    episodes        | 272      |
|    fps             | 16       |
|    time_elapsed    | 33621    |
|    total timesteps | 543501   |
| train/             |          |
|    actor_loss      | 0.0909   |
|    critic_loss     | 0.993    |
|    learning_rate   | 0.0005   |
|    n_updates       | 538500   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-26.97 +/- 31.89
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27      |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | 0.00721  |
|    critic_loss     | 0.0843   |
|    learning_rate   | 0.0005   |
|    n_updates       | 544999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.28     |
| time/              |          |
|    episodes        | 276      |
|    fps             | 16       |
|    time_elapsed    | 34114    |
|    total timesteps | 550971   |
| train/             |          |
|    actor_loss      | -0.121   |
|    critic_loss     | 0.465    |
|    learning_rate   | 0.0005   |
|    n_updates       | 545970   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    episodes        | 280      |
|    fps             | 16       |
|    time_elapsed    | 34518    |
|    total timesteps | 558975   |
| train/             |          |
|    actor_loss      | 0.24     |
|    critic_loss     | 0.834    |
|    learning_rate   | 0.0005   |
|    n_updates       | 553974   |
---------------------------------
Eval num_timesteps=560000, episode_reward=-14.30 +/- 5.35
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.3    |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | 0.15     |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.0005   |
|    n_updates       | 554999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    episodes        | 284      |
|    fps             | 16       |
|    time_elapsed    | 35034    |
|    total timesteps | 566979   |
| train/             |          |
|    actor_loss      | 0.113    |
|    critic_loss     | 0.126    |
|    learning_rate   | 0.0005   |
|    n_updates       | 561978   |
---------------------------------
Eval num_timesteps=570000, episode_reward=-38.09 +/- 22.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -38.1    |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | 0.198    |
|    critic_loss     | 0.178    |
|    learning_rate   | 0.0005   |
|    n_updates       | 564999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.07     |
| time/              |          |
|    episodes        | 288      |
|    fps             | 16       |
|    time_elapsed    | 35558    |
|    total timesteps | 574983   |
| train/             |          |
|    actor_loss      | 0.184    |
|    critic_loss     | 1.66     |
|    learning_rate   | 0.0005   |
|    n_updates       | 569982   |
---------------------------------
Eval num_timesteps=580000, episode_reward=-37.50 +/- 20.27
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -37.5    |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -0.0187  |
|    critic_loss     | 1.25     |
|    learning_rate   | 0.0005   |
|    n_updates       | 574999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.839    |
| time/              |          |
|    episodes        | 292      |
|    fps             | 16       |
|    time_elapsed    | 36083    |
|    total timesteps | 582987   |
| train/             |          |
|    actor_loss      | 0.238    |
|    critic_loss     | 1.98     |
|    learning_rate   | 0.0005   |
|    n_updates       | 577986   |
---------------------------------
Eval num_timesteps=590000, episode_reward=-14.47 +/- 36.72
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.5    |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -0.112   |
|    critic_loss     | 2.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 584999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    episodes        | 296      |
|    fps             | 16       |
|    time_elapsed    | 36601    |
|    total timesteps | 590991   |
| train/             |          |
|    actor_loss      | 0.102    |
|    critic_loss     | 2        |
|    learning_rate   | 0.0005   |
|    n_updates       | 585990   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.15     |
| time/              |          |
|    episodes        | 300      |
|    fps             | 16       |
|    time_elapsed    | 37005    |
|    total timesteps | 598995   |
| train/             |          |
|    actor_loss      | 0.336    |
|    critic_loss     | 1.75     |
|    learning_rate   | 0.0005   |
|    n_updates       | 593994   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-41.43 +/- 17.60
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -41.4    |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | 0.177    |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 594999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.91     |
| time/              |          |
|    episodes        | 304      |
|    fps             | 16       |
|    time_elapsed    | 37527    |
|    total timesteps | 606999   |
| train/             |          |
|    actor_loss      | 0.105    |
|    critic_loss     | 0.178    |
|    learning_rate   | 0.0005   |
|    n_updates       | 601998   |
---------------------------------
Eval num_timesteps=610000, episode_reward=-23.11 +/- 16.62
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -23.1    |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | 0.125    |
|    critic_loss     | 0.85     |
|    learning_rate   | 0.0005   |
|    n_updates       | 604999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.16     |
| time/              |          |
|    episodes        | 308      |
|    fps             | 16       |
|    time_elapsed    | 38038    |
|    total timesteps | 615003   |
| train/             |          |
|    actor_loss      | -0.0877  |
|    critic_loss     | 0.0736   |
|    learning_rate   | 0.0005   |
|    n_updates       | 610002   |
---------------------------------
Eval num_timesteps=620000, episode_reward=-38.57 +/- 29.37
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -38.6    |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | 0.255    |
|    critic_loss     | 1.67     |
|    learning_rate   | 0.0005   |
|    n_updates       | 614999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.212    |
| time/              |          |
|    episodes        | 312      |
|    fps             | 16       |
|    time_elapsed    | 38549    |
|    total timesteps | 623007   |
| train/             |          |
|    actor_loss      | 0.241    |
|    critic_loss     | 0.501    |
|    learning_rate   | 0.0005   |
|    n_updates       | 618006   |
---------------------------------
Eval num_timesteps=630000, episode_reward=-48.41 +/- 51.23
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -48.4    |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | 0.0228   |
|    critic_loss     | 1.73     |
|    learning_rate   | 0.0005   |
|    n_updates       | 624999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 3.57     |
| time/              |          |
|    episodes        | 316      |
|    fps             | 16       |
|    time_elapsed    | 39064    |
|    total timesteps | 631011   |
| train/             |          |
|    actor_loss      | 0.199    |
|    critic_loss     | 3.54     |
|    learning_rate   | 0.0005   |
|    n_updates       | 626010   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.936    |
| time/              |          |
|    episodes        | 320      |
|    fps             | 16       |
|    time_elapsed    | 39465    |
|    total timesteps | 639015   |
| train/             |          |
|    actor_loss      | 0.133    |
|    critic_loss     | 1.32     |
|    learning_rate   | 0.0005   |
|    n_updates       | 634014   |
---------------------------------
Eval num_timesteps=640000, episode_reward=-20.24 +/- 23.26
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -20.2    |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | 0.227    |
|    critic_loss     | 1.57     |
|    learning_rate   | 0.0005   |
|    n_updates       | 634999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.59    |
| time/              |          |
|    episodes        | 324      |
|    fps             | 16       |
|    time_elapsed    | 39981    |
|    total timesteps | 647019   |
| train/             |          |
|    actor_loss      | 0.063    |
|    critic_loss     | 1.49     |
|    learning_rate   | 0.0005   |
|    n_updates       | 642018   |
---------------------------------
Eval num_timesteps=650000, episode_reward=-30.66 +/- 30.06
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -30.7    |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | 0.283    |
|    critic_loss     | 1.03     |
|    learning_rate   | 0.0005   |
|    n_updates       | 644999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.75    |
| time/              |          |
|    episodes        | 328      |
|    fps             | 16       |
|    time_elapsed    | 40498    |
|    total timesteps | 655023   |
| train/             |          |
|    actor_loss      | 0.0616   |
|    critic_loss     | 0.842    |
|    learning_rate   | 0.0005   |
|    n_updates       | 650022   |
---------------------------------
Eval num_timesteps=660000, episode_reward=-15.01 +/- 35.70
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -15      |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | 0.0648   |
|    critic_loss     | 0.497    |
|    learning_rate   | 0.0005   |
|    n_updates       | 654999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.75    |
| time/              |          |
|    episodes        | 332      |
|    fps             | 16       |
|    time_elapsed    | 41018    |
|    total timesteps | 663027   |
| train/             |          |
|    actor_loss      | 0.124    |
|    critic_loss     | 1.36     |
|    learning_rate   | 0.0005   |
|    n_updates       | 658026   |
---------------------------------
Eval num_timesteps=670000, episode_reward=-19.10 +/- 21.72
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -19.1    |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -0.0128  |
|    critic_loss     | 0.0755   |
|    learning_rate   | 0.0005   |
|    n_updates       | 664999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.615   |
| time/              |          |
|    episodes        | 336      |
|    fps             | 16       |
|    time_elapsed    | 41540    |
|    total timesteps | 671031   |
| train/             |          |
|    actor_loss      | 0.0533   |
|    critic_loss     | 1.63     |
|    learning_rate   | 0.0005   |
|    n_updates       | 666030   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.451    |
| time/              |          |
|    episodes        | 340      |
|    fps             | 16       |
|    time_elapsed    | 41942    |
|    total timesteps | 679035   |
| train/             |          |
|    actor_loss      | 0.103    |
|    critic_loss     | 0.0509   |
|    learning_rate   | 0.0005   |
|    n_updates       | 674034   |
---------------------------------
Eval num_timesteps=680000, episode_reward=-39.69 +/- 38.78
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -39.7    |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -0.0738  |
|    critic_loss     | 0.495    |
|    learning_rate   | 0.0005   |
|    n_updates       | 674999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.62     |
| time/              |          |
|    episodes        | 344      |
|    fps             | 16       |
|    time_elapsed    | 42460    |
|    total timesteps | 687039   |
| train/             |          |
|    actor_loss      | -0.0304  |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.0005   |
|    n_updates       | 682038   |
---------------------------------
Eval num_timesteps=690000, episode_reward=-22.65 +/- 34.30
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -22.7    |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | 0.143    |
|    critic_loss     | 0.893    |
|    learning_rate   | 0.0005   |
|    n_updates       | 684999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 5.89     |
| time/              |          |
|    episodes        | 348      |
|    fps             | 16       |
|    time_elapsed    | 42980    |
|    total timesteps | 695043   |
| train/             |          |
|    actor_loss      | 0.232    |
|    critic_loss     | 1.27     |
|    learning_rate   | 0.0005   |
|    n_updates       | 690042   |
---------------------------------
Eval num_timesteps=700000, episode_reward=-26.60 +/- 10.11
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -26.6    |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | 0.156    |
|    critic_loss     | 1.24     |
|    learning_rate   | 0.0005   |
|    n_updates       | 694999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 4.64     |
| time/              |          |
|    episodes        | 352      |
|    fps             | 16       |
|    time_elapsed    | 43495    |
|    total timesteps | 703047   |
| train/             |          |
|    actor_loss      | -0.0705  |
|    critic_loss     | 0.941    |
|    learning_rate   | 0.0005   |
|    n_updates       | 698046   |
---------------------------------
Eval num_timesteps=710000, episode_reward=-16.00 +/- 20.32
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -16      |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | 0.186    |
|    critic_loss     | 0.0982   |
|    learning_rate   | 0.0005   |
|    n_updates       | 704999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.643   |
| time/              |          |
|    episodes        | 356      |
|    fps             | 16       |
|    time_elapsed    | 44009    |
|    total timesteps | 711051   |
| train/             |          |
|    actor_loss      | 0.227    |
|    critic_loss     | 0.846    |
|    learning_rate   | 0.0005   |
|    n_updates       | 706050   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.269    |
| time/              |          |
|    episodes        | 360      |
|    fps             | 16       |
|    time_elapsed    | 44407    |
|    total timesteps | 719055   |
| train/             |          |
|    actor_loss      | 0.147    |
|    critic_loss     | 1.55     |
|    learning_rate   | 0.0005   |
|    n_updates       | 714054   |
---------------------------------
Eval num_timesteps=720000, episode_reward=-21.83 +/- 35.49
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -21.8    |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | 0.101    |
|    critic_loss     | 0.812    |
|    learning_rate   | 0.0005   |
|    n_updates       | 714999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.75     |
| time/              |          |
|    episodes        | 364      |
|    fps             | 16       |
|    time_elapsed    | 44918    |
|    total timesteps | 726937   |
| train/             |          |
|    actor_loss      | 0.0879   |
|    critic_loss     | 0.49     |
|    learning_rate   | 0.0005   |
|    n_updates       | 721936   |
---------------------------------
Eval num_timesteps=730000, episode_reward=-4.82 +/- 19.40
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -4.82    |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -0.12    |
|    critic_loss     | 1.5      |
|    learning_rate   | 0.0005   |
|    n_updates       | 724999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    episodes        | 368      |
|    fps             | 16       |
|    time_elapsed    | 45439    |
|    total timesteps | 734941   |
| train/             |          |
|    actor_loss      | 0.173    |
|    critic_loss     | 1.12     |
|    learning_rate   | 0.0005   |
|    n_updates       | 729940   |
---------------------------------
Eval num_timesteps=740000, episode_reward=-28.82 +/- 11.45
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -28.8    |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | 0.136    |
|    critic_loss     | 0.469    |
|    learning_rate   | 0.0005   |
|    n_updates       | 734999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.99e+03 |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    episodes        | 372      |
|    fps             | 16       |
|    time_elapsed    | 45959    |
|    total timesteps | 742945   |
| train/             |          |
|    actor_loss      | 0.129    |
|    critic_loss     | 0.447    |
|    learning_rate   | 0.0005   |
|    n_updates       | 737944   |
---------------------------------
Eval num_timesteps=750000, episode_reward=0.03 +/- 24.90
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | 0.0331   |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | 0.039    |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 744999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.08     |
| time/              |          |
|    episodes        | 376      |
|    fps             | 16       |
|    time_elapsed    | 46479    |
|    total timesteps | 750949   |
| train/             |          |
|    actor_loss      | 0.286    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.0005   |
|    n_updates       | 745948   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.574   |
| time/              |          |
|    episodes        | 380      |
|    fps             | 16       |
|    time_elapsed    | 46879    |
|    total timesteps | 758953   |
| train/             |          |
|    actor_loss      | 0.408    |
|    critic_loss     | 1.31     |
|    learning_rate   | 0.0005   |
|    n_updates       | 753952   |
---------------------------------
Eval num_timesteps=760000, episode_reward=-47.43 +/- 31.70
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -47.4    |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | 0.0305   |
|    critic_loss     | 2.06     |
|    learning_rate   | 0.0005   |
|    n_updates       | 754999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.21     |
| time/              |          |
|    episodes        | 384      |
|    fps             | 16       |
|    time_elapsed    | 47398    |
|    total timesteps | 766957   |
| train/             |          |
|    actor_loss      | 0.2      |
|    critic_loss     | 0.466    |
|    learning_rate   | 0.0005   |
|    n_updates       | 761956   |
---------------------------------
Eval num_timesteps=770000, episode_reward=-34.48 +/- 35.22
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -34.5    |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -0.0142  |
|    critic_loss     | 0.871    |
|    learning_rate   | 0.0005   |
|    n_updates       | 764999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.0782  |
| time/              |          |
|    episodes        | 388      |
|    fps             | 16       |
|    time_elapsed    | 47920    |
|    total timesteps | 774961   |
| train/             |          |
|    actor_loss      | -0.067   |
|    critic_loss     | 0.66     |
|    learning_rate   | 0.0005   |
|    n_updates       | 769960   |
---------------------------------
Eval num_timesteps=780000, episode_reward=-5.65 +/- 32.74
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -5.65    |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | 0.0732   |
|    critic_loss     | 1.59     |
|    learning_rate   | 0.0005   |
|    n_updates       | 774999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.825    |
| time/              |          |
|    episodes        | 392      |
|    fps             | 16       |
|    time_elapsed    | 48436    |
|    total timesteps | 782965   |
| train/             |          |
|    actor_loss      | 0.165    |
|    critic_loss     | 0.07     |
|    learning_rate   | 0.0005   |
|    n_updates       | 777964   |
---------------------------------
Eval num_timesteps=790000, episode_reward=-14.06 +/- 17.62
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -14.1    |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -0.0644  |
|    critic_loss     | 2.26     |
|    learning_rate   | 0.0005   |
|    n_updates       | 784999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.449   |
| time/              |          |
|    episodes        | 396      |
|    fps             | 16       |
|    time_elapsed    | 48954    |
|    total timesteps | 790969   |
| train/             |          |
|    actor_loss      | 0.35     |
|    critic_loss     | 1.2      |
|    learning_rate   | 0.0005   |
|    n_updates       | 785968   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.01    |
| time/              |          |
|    episodes        | 400      |
|    fps             | 16       |
|    time_elapsed    | 49355    |
|    total timesteps | 798973   |
| train/             |          |
|    actor_loss      | 0.28     |
|    critic_loss     | 0.905    |
|    learning_rate   | 0.0005   |
|    n_updates       | 793972   |
---------------------------------
Eval num_timesteps=800000, episode_reward=-34.57 +/- 17.35
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -34.6    |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | 0.185    |
|    critic_loss     | 0.461    |
|    learning_rate   | 0.0005   |
|    n_updates       | 794999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.851   |
| time/              |          |
|    episodes        | 404      |
|    fps             | 16       |
|    time_elapsed    | 49875    |
|    total timesteps | 806977   |
| train/             |          |
|    actor_loss      | 0.000612 |
|    critic_loss     | 1.27     |
|    learning_rate   | 0.0005   |
|    n_updates       | 801976   |
---------------------------------
Eval num_timesteps=810000, episode_reward=-20.71 +/- 26.44
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -20.7    |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -0.0397  |
|    critic_loss     | 0.842    |
|    learning_rate   | 0.0005   |
|    n_updates       | 804999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.07    |
| time/              |          |
|    episodes        | 408      |
|    fps             | 16       |
|    time_elapsed    | 50395    |
|    total timesteps | 814981   |
| train/             |          |
|    actor_loss      | 0.112    |
|    critic_loss     | 0.905    |
|    learning_rate   | 0.0005   |
|    n_updates       | 809980   |
---------------------------------
Eval num_timesteps=820000, episode_reward=-40.25 +/- 28.36
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -40.2    |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | 0.276    |
|    critic_loss     | 0.508    |
|    learning_rate   | 0.0005   |
|    n_updates       | 814999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.255    |
| time/              |          |
|    episodes        | 412      |
|    fps             | 16       |
|    time_elapsed    | 50915    |
|    total timesteps | 822985   |
| train/             |          |
|    actor_loss      | 0.0696   |
|    critic_loss     | 0.472    |
|    learning_rate   | 0.0005   |
|    n_updates       | 817984   |
---------------------------------
Eval num_timesteps=830000, episode_reward=-27.18 +/- 30.34
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -27.2    |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | 0.019    |
|    critic_loss     | 0.106    |
|    learning_rate   | 0.0005   |
|    n_updates       | 824999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.07    |
| time/              |          |
|    episodes        | 416      |
|    fps             | 16       |
|    time_elapsed    | 51436    |
|    total timesteps | 830989   |
| train/             |          |
|    actor_loss      | 0.11     |
|    critic_loss     | 1.22     |
|    learning_rate   | 0.0005   |
|    n_updates       | 825988   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.46     |
| time/              |          |
|    episodes        | 420      |
|    fps             | 16       |
|    time_elapsed    | 51836    |
|    total timesteps | 838993   |
| train/             |          |
|    actor_loss      | 0.183    |
|    critic_loss     | 0.501    |
|    learning_rate   | 0.0005   |
|    n_updates       | 833992   |
---------------------------------
Eval num_timesteps=840000, episode_reward=-25.18 +/- 15.01
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -25.2    |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | 0.19     |
|    critic_loss     | 0.84     |
|    learning_rate   | 0.0005   |
|    n_updates       | 834999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 2.01     |
| time/              |          |
|    episodes        | 424      |
|    fps             | 16       |
|    time_elapsed    | 52360    |
|    total timesteps | 846997   |
| train/             |          |
|    actor_loss      | 0.186    |
|    critic_loss     | 0.545    |
|    learning_rate   | 0.0005   |
|    n_updates       | 841996   |
---------------------------------
Eval num_timesteps=850000, episode_reward=-9.39 +/- 33.29
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -9.39    |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | 0.179    |
|    critic_loss     | 0.842    |
|    learning_rate   | 0.0005   |
|    n_updates       | 844999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 1.16     |
| time/              |          |
|    episodes        | 428      |
|    fps             | 16       |
|    time_elapsed    | 52882    |
|    total timesteps | 855001   |
| train/             |          |
|    actor_loss      | 0.136    |
|    critic_loss     | 0.797    |
|    learning_rate   | 0.0005   |
|    n_updates       | 850000   |
---------------------------------
Eval num_timesteps=860000, episode_reward=-26.87 +/- 38.13
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -26.9    |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | 0.23     |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 854999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | 0.407    |
| time/              |          |
|    episodes        | 432      |
|    fps             | 16       |
|    time_elapsed    | 53402    |
|    total timesteps | 863005   |
| train/             |          |
|    actor_loss      | 0.11     |
|    critic_loss     | 0.94     |
|    learning_rate   | 0.0005   |
|    n_updates       | 858004   |
---------------------------------
Eval num_timesteps=870000, episode_reward=-11.95 +/- 14.91
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -11.9    |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | 0.326    |
|    critic_loss     | 1.31     |
|    learning_rate   | 0.0005   |
|    n_updates       | 864999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -0.0999  |
| time/              |          |
|    episodes        | 436      |
|    fps             | 16       |
|    time_elapsed    | 53928    |
|    total timesteps | 871009   |
| train/             |          |
|    actor_loss      | 0.121    |
|    critic_loss     | 1.75     |
|    learning_rate   | 0.0005   |
|    n_updates       | 866008   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.8     |
| time/              |          |
|    episodes        | 440      |
|    fps             | 16       |
|    time_elapsed    | 54331    |
|    total timesteps | 879013   |
| train/             |          |
|    actor_loss      | -0.0557  |
|    critic_loss     | 0.434    |
|    learning_rate   | 0.0005   |
|    n_updates       | 874012   |
---------------------------------
Eval num_timesteps=880000, episode_reward=-30.11 +/- 16.01
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -30.1    |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | 0.04     |
|    critic_loss     | 1.68     |
|    learning_rate   | 0.0005   |
|    n_updates       | 874999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.03    |
| time/              |          |
|    episodes        | 444      |
|    fps             | 16       |
|    time_elapsed    | 54854    |
|    total timesteps | 887017   |
| train/             |          |
|    actor_loss      | 0.112    |
|    critic_loss     | 1.6      |
|    learning_rate   | 0.0005   |
|    n_updates       | 882016   |
---------------------------------
Eval num_timesteps=890000, episode_reward=-32.49 +/- 19.20
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -32.5    |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | 0.0471   |
|    critic_loss     | 0.463    |
|    learning_rate   | 0.0005   |
|    n_updates       | 884999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.69    |
| time/              |          |
|    episodes        | 448      |
|    fps             | 16       |
|    time_elapsed    | 55374    |
|    total timesteps | 895021   |
| train/             |          |
|    actor_loss      | 0.182    |
|    critic_loss     | 1.23     |
|    learning_rate   | 0.0005   |
|    n_updates       | 890020   |
---------------------------------
Eval num_timesteps=900000, episode_reward=-10.79 +/- 41.14
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -10.8    |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | 0.411    |
|    critic_loss     | 0.679    |
|    learning_rate   | 0.0005   |
|    n_updates       | 894999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.68    |
| time/              |          |
|    episodes        | 452      |
|    fps             | 16       |
|    time_elapsed    | 55895    |
|    total timesteps | 903025   |
| train/             |          |
|    actor_loss      | 0.167    |
|    critic_loss     | 0.511    |
|    learning_rate   | 0.0005   |
|    n_updates       | 898024   |
---------------------------------
Eval num_timesteps=910000, episode_reward=-3.05 +/- 33.09
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -3.05    |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | 0.337    |
|    critic_loss     | 0.574    |
|    learning_rate   | 0.0005   |
|    n_updates       | 904999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -2.71    |
| time/              |          |
|    episodes        | 456      |
|    fps             | 16       |
|    time_elapsed    | 56415    |
|    total timesteps | 911029   |
| train/             |          |
|    actor_loss      | -0.201   |
|    critic_loss     | 0.381    |
|    learning_rate   | 0.0005   |
|    n_updates       | 906028   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.54    |
| time/              |          |
|    episodes        | 460      |
|    fps             | 16       |
|    time_elapsed    | 56817    |
|    total timesteps | 919033   |
| train/             |          |
|    actor_loss      | -0.113   |
|    critic_loss     | 0.291    |
|    learning_rate   | 0.0005   |
|    n_updates       | 914032   |
---------------------------------
Eval num_timesteps=920000, episode_reward=-32.76 +/- 26.89
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -32.8    |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | 0.264    |
|    critic_loss     | 1.29     |
|    learning_rate   | 0.0005   |
|    n_updates       | 914999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -7.41    |
| time/              |          |
|    episodes        | 464      |
|    fps             | 16       |
|    time_elapsed    | 57340    |
|    total timesteps | 927037   |
| train/             |          |
|    actor_loss      | 0.12     |
|    critic_loss     | 1.31     |
|    learning_rate   | 0.0005   |
|    n_updates       | 922036   |
---------------------------------
Eval num_timesteps=930000, episode_reward=-16.59 +/- 9.16
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -16.6    |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | 0.0327   |
|    critic_loss     | 0.853    |
|    learning_rate   | 0.0005   |
|    n_updates       | 924999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -5.25    |
| time/              |          |
|    episodes        | 468      |
|    fps             | 16       |
|    time_elapsed    | 57855    |
|    total timesteps | 935041   |
| train/             |          |
|    actor_loss      | 0.221    |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.0005   |
|    n_updates       | 930040   |
---------------------------------
Eval num_timesteps=940000, episode_reward=-38.61 +/- 26.13
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -38.6    |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | 0.147    |
|    critic_loss     | 0.503    |
|    learning_rate   | 0.0005   |
|    n_updates       | 934999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.14    |
| time/              |          |
|    episodes        | 472      |
|    fps             | 16       |
|    time_elapsed    | 58376    |
|    total timesteps | 943045   |
| train/             |          |
|    actor_loss      | 0.172    |
|    critic_loss     | 1.31     |
|    learning_rate   | 0.0005   |
|    n_updates       | 938044   |
---------------------------------
Eval num_timesteps=950000, episode_reward=-16.69 +/- 29.36
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -16.7    |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | 0.0204   |
|    critic_loss     | 1.28     |
|    learning_rate   | 0.0005   |
|    n_updates       | 944999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.17    |
| time/              |          |
|    episodes        | 476      |
|    fps             | 16       |
|    time_elapsed    | 58899    |
|    total timesteps | 951049   |
| train/             |          |
|    actor_loss      | 0.247    |
|    critic_loss     | 0.438    |
|    learning_rate   | 0.0005   |
|    n_updates       | 946048   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -9.46    |
| time/              |          |
|    episodes        | 480      |
|    fps             | 16       |
|    time_elapsed    | 59303    |
|    total timesteps | 959053   |
| train/             |          |
|    actor_loss      | 0.0422   |
|    critic_loss     | 0.469    |
|    learning_rate   | 0.0005   |
|    n_updates       | 954052   |
---------------------------------
Eval num_timesteps=960000, episode_reward=-13.86 +/- 24.94
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -13.9    |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | 0.0811   |
|    critic_loss     | 0.872    |
|    learning_rate   | 0.0005   |
|    n_updates       | 954999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.95    |
| time/              |          |
|    episodes        | 484      |
|    fps             | 16       |
|    time_elapsed    | 59824    |
|    total timesteps | 967057   |
| train/             |          |
|    actor_loss      | -0.00322 |
|    critic_loss     | 0.113    |
|    learning_rate   | 0.0005   |
|    n_updates       | 962056   |
---------------------------------
Eval num_timesteps=970000, episode_reward=-12.50 +/- 11.60
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -12.5    |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | 0.12     |
|    critic_loss     | 1.26     |
|    learning_rate   | 0.0005   |
|    n_updates       | 964999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -6.09    |
| time/              |          |
|    episodes        | 488      |
|    fps             | 16       |
|    time_elapsed    | 60339    |
|    total timesteps | 975061   |
| train/             |          |
|    actor_loss      | 0.158    |
|    critic_loss     | 0.803    |
|    learning_rate   | 0.0005   |
|    n_updates       | 970060   |
---------------------------------
Eval num_timesteps=980000, episode_reward=-20.71 +/- 30.13
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -20.7    |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | 0.134    |
|    critic_loss     | 2.33     |
|    learning_rate   | 0.0005   |
|    n_updates       | 974999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -4.28    |
| time/              |          |
|    episodes        | 492      |
|    fps             | 16       |
|    time_elapsed    | 60861    |
|    total timesteps | 983065   |
| train/             |          |
|    actor_loss      | -0.0462  |
|    critic_loss     | 0.0592   |
|    learning_rate   | 0.0005   |
|    n_updates       | 978064   |
---------------------------------
Eval num_timesteps=990000, episode_reward=-16.27 +/- 20.55
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -16.3    |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | 0.289    |
|    critic_loss     | 0.524    |
|    learning_rate   | 0.0005   |
|    n_updates       | 984999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -1.55    |
| time/              |          |
|    episodes        | 496      |
|    fps             | 16       |
|    time_elapsed    | 61384    |
|    total timesteps | 991069   |
| train/             |          |
|    actor_loss      | 0.307    |
|    critic_loss     | 1.77     |
|    learning_rate   | 0.0005   |
|    n_updates       | 986068   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2e+03    |
|    ep_rew_mean     | -3.18    |
| time/              |          |
|    episodes        | 500      |
|    fps             | 16       |
|    time_elapsed    | 61787    |
|    total timesteps | 999073   |
| train/             |          |
|    actor_loss      | 0.168    |
|    critic_loss     | 1.69     |
|    learning_rate   | 0.0005   |
|    n_updates       | 994072   |
---------------------------------
Eval num_timesteps=1000000, episode_reward=-25.42 +/- 19.02
Episode length: 2001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 2e+03    |
|    mean_reward     | -25.4    |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -0.111   |
|    critic_loss     | 0.9      |
|    learning_rate   | 0.0005   |
|    n_updates       | 994999   |
---------------------------------
