/home/shandilya/Desktop/Projects/NavigationController
Env Type: maze
Task: GoalRewardSimple
Traceback (most recent call last):
  File "train.py", line 80, in <module>
    lmbda = args.lmbda
  File "/home/shandilya/Desktop/Projects/NavigationController/learning/explore.py", line 73, in __init__
    lambda : sb3.common.monitor.Monitor(env_class(
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/shandilya/Desktop/Projects/NavigationController/learning/explore.py", line 78, in <lambda>
    history_steps
  File "/home/shandilya/Desktop/Projects/NavigationController/simulations/maze_env.py", line 117, in __init__
    self.set_env()
  File "/home/shandilya/Desktop/Projects/NavigationController/simulations/maze_env.py", line 244, in set_env
    self.sampled_path = self.__sample_path()
  File "/home/shandilya/Desktop/Projects/NavigationController/simulations/maze_env.py", line 310, in __sample_path
    target
  File "/home/shandilya/py36/lib/python3.6/site-packages/networkx/algorithms/shortest_paths/generic.py", line 526, in _build_paths_from_predecessors
    f"Target {target} cannot be reached" f"from given sources"
networkx.exception.NetworkXNoPath: Target 9 cannot be reachedfrom given sources
2021-12-17 12:50:39.042658: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-17 12:50:39.042694: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Env Type: maze
Task: GoalRewardSimple
Model: <class 'stable_baselines3.td3.td3.TD3'>
Using cpu device
Logging to assets/out/models/exp17/TD3_7
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | 0.742    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 65       |
|    time_elapsed    | 13       |
|    total timesteps | 900      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 11.7     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 74       |
|    time_elapsed    | 25       |
|    total timesteps | 1874     |
---------------------------------
Traceback (most recent call last):
  File "train.py", line 82, in <module>
    model.learn(args.timesteps)
  File "/home/shandilya/Desktop/Projects/NavigationController/learning/explore.py", line 229, in learn
    callback = self.rl_callback
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 211, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 359, in learn
    log_interval=log_interval,
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 563, in collect_rollouts
    action, buffer_action = self._sample_action(learning_starts, action_noise)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 407, in _sample_action
    unscaled_action, _ = self.predict(self._last_obs, deterministic=False)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/base_class.py", line 539, in predict
    return self.policy.predict(observation, state, mask, deterministic)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/policies.py", line 302, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/td3/policies.py", line 226, in _predict
    return self.actor(observation)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/td3/policies.py", line 77, in forward
    features = self.extract_features(obs)
  File "/home/shandilya/py36/lib/python3.6/site-packages/stable_baselines3/common/policies.py", line 128, in extract_features
    return self.features_extractor(preprocessed_obs)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/shandilya/Desktop/Projects/NavigationController/utils/td3_utils.py", line 98, in forward
    out = self.fc(x)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 96, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/shandilya/py36/lib/python3.6/site-packages/torch/nn/functional.py", line 1847, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x900 and 1200x300)
2021-12-17 12:52:33.380954: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-17 12:52:33.380998: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Env Type: maze
Task: GoalRewardSimple
Model: <class 'stable_baselines3.td3.td3.TD3'>
Using cpu device
Logging to assets/out/models/exp17/TD3_9
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | 1.84     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 76       |
|    time_elapsed    | 13       |
|    total timesteps | 995      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 242      |
|    ep_rew_mean     | 8.84     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 79       |
|    time_elapsed    | 24       |
|    total timesteps | 1932     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 245      |
|    ep_rew_mean     | -5.35    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 56       |
|    time_elapsed    | 51       |
|    total timesteps | 2936     |
| train/             |          |
|    actor_loss      | -0.106   |
|    critic_loss     | 2.79     |
|    learning_rate   | 0.000997 |
|    n_updates       | 251      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 245      |
|    ep_rew_mean     | -1.52    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 31       |
|    time_elapsed    | 125      |
|    total timesteps | 3914     |
| train/             |          |
|    actor_loss      | -0.189   |
|    critic_loss     | 2.03     |
|    learning_rate   | 0.000996 |
|    n_updates       | 1229     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | 1.71     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 23       |
|    time_elapsed    | 205      |
|    total timesteps | 4918     |
| train/             |          |
|    actor_loss      | -0.156   |
|    critic_loss     | 2.36     |
|    learning_rate   | 0.000995 |
|    n_updates       | 2233     |
---------------------------------
Eval num_timesteps=5000, episode_reward=54.92 +/- 12.81
Episode length: 180.40 +/- 7.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 54.9     |
| time/              |          |
|    total_timesteps | 5000     |
| train/             |          |
|    actor_loss      | -0.15    |
|    critic_loss     | 2.04     |
|    learning_rate   | 0.000995 |
|    n_updates       | 2484     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | 0.601    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 19       |
|    time_elapsed    | 298      |
|    total timesteps | 5914     |
| train/             |          |
|    actor_loss      | -0.176   |
|    critic_loss     | 2.07     |
|    learning_rate   | 0.000994 |
|    n_updates       | 3229     |
---------------------------------
Terminated
2021-12-17 12:58:28.688505: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-17 12:58:28.688548: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Env Type: maze
Task: GoalRewardSimple
Model: <class 'stable_baselines3.td3.td3.TD3'>
Using cpu device
Logging to assets/out/models/exp17/TD3_10
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -42.4    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 73       |
|    time_elapsed    | 13       |
|    total timesteps | 1004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 242      |
|    ep_rew_mean     | -31.8    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 78       |
|    time_elapsed    | 24       |
|    total timesteps | 1937     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 245      |
|    ep_rew_mean     | -25.8    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 55       |
|    time_elapsed    | 52       |
|    total timesteps | 2941     |
| train/             |          |
|    actor_loss      | 0.207    |
|    critic_loss     | 3.74     |
|    learning_rate   | 0.000997 |
|    n_updates       | 251      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | -19.4    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 30       |
|    time_elapsed    | 126      |
|    total timesteps | 3934     |
| train/             |          |
|    actor_loss      | 0.394    |
|    critic_loss     | 2.3      |
|    learning_rate   | 0.000996 |
|    n_updates       | 1244     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | -16.5    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 23       |
|    time_elapsed    | 207      |
|    total timesteps | 4938     |
| train/             |          |
|    actor_loss      | 0.556    |
|    critic_loss     | 1.79     |
|    learning_rate   | 0.000995 |
|    n_updates       | 2248     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-32.29 +/- 18.11
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -32.3    |
| time/              |          |
|    total_timesteps | 5000     |
| train/             |          |
|    actor_loss      | 0.584    |
|    critic_loss     | 1.87     |
|    learning_rate   | 0.000995 |
|    n_updates       | 2499     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -17.2    |
| time/              |          |
|    episodes        | 24       |
|    fps             | 19       |
|    time_elapsed    | 305      |
|    total timesteps | 5942     |
| train/             |          |
|    actor_loss      | 0.699    |
|    critic_loss     | 2.17     |
|    learning_rate   | 0.000994 |
|    n_updates       | 3252     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -18.2    |
| time/              |          |
|    episodes        | 28       |
|    fps             | 18       |
|    time_elapsed    | 385      |
|    total timesteps | 6946     |
| train/             |          |
|    actor_loss      | 0.823    |
|    critic_loss     | 2.31     |
|    learning_rate   | 0.000993 |
|    n_updates       | 4256     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | -19.5    |
| time/              |          |
|    episodes        | 32       |
|    fps             | 17       |
|    time_elapsed    | 464      |
|    total timesteps | 7950     |
| train/             |          |
|    actor_loss      | 1        |
|    critic_loss     | 2.12     |
|    learning_rate   | 0.000992 |
|    n_updates       | 5260     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -17.3    |
| time/              |          |
|    episodes        | 36       |
|    fps             | 16       |
|    time_elapsed    | 545      |
|    total timesteps | 8954     |
| train/             |          |
|    actor_loss      | 1.14     |
|    critic_loss     | 2.03     |
|    learning_rate   | 0.000991 |
|    n_updates       | 6264     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -20.4    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 15       |
|    time_elapsed    | 624      |
|    total timesteps | 9958     |
| train/             |          |
|    actor_loss      | 1.24     |
|    critic_loss     | 1.64     |
|    learning_rate   | 0.00099  |
|    n_updates       | 7268     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-30.62 +/- 18.57
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -30.6    |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 1.29     |
|    critic_loss     | 2.32     |
|    learning_rate   | 0.00099  |
|    n_updates       | 7519     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -21.8    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 15       |
|    time_elapsed    | 726      |
|    total timesteps | 10962    |
| train/             |          |
|    actor_loss      | 1.36     |
|    critic_loss     | 1.92     |
|    learning_rate   | 0.000989 |
|    n_updates       | 8272     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -22.6    |
| time/              |          |
|    episodes        | 48       |
|    fps             | 14       |
|    time_elapsed    | 807      |
|    total timesteps | 11966    |
| train/             |          |
|    actor_loss      | 1.55     |
|    critic_loss     | 2.66     |
|    learning_rate   | 0.000988 |
|    n_updates       | 9276     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -22.9    |
| time/              |          |
|    episodes        | 52       |
|    fps             | 14       |
|    time_elapsed    | 886      |
|    total timesteps | 12970    |
| train/             |          |
|    actor_loss      | 1.7      |
|    critic_loss     | 2.26     |
|    learning_rate   | 0.000987 |
|    n_updates       | 10280    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -22.2    |
| time/              |          |
|    episodes        | 56       |
|    fps             | 14       |
|    time_elapsed    | 969      |
|    total timesteps | 13974    |
| train/             |          |
|    actor_loss      | 1.87     |
|    critic_loss     | 2.78     |
|    learning_rate   | 0.000986 |
|    n_updates       | 11284    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -23      |
| time/              |          |
|    episodes        | 60       |
|    fps             | 14       |
|    time_elapsed    | 1050     |
|    total timesteps | 14978    |
| train/             |          |
|    actor_loss      | 2.07     |
|    critic_loss     | 2.81     |
|    learning_rate   | 0.000985 |
|    n_updates       | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-32.23 +/- 7.68
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -32.2    |
| time/              |          |
|    total_timesteps | 15000    |
| train/             |          |
|    actor_loss      | 2.09     |
|    critic_loss     | 2.68     |
|    learning_rate   | 0.000985 |
|    n_updates       | 12539    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -22.6    |
| time/              |          |
|    episodes        | 64       |
|    fps             | 13       |
|    time_elapsed    | 1150     |
|    total timesteps | 15982    |
| train/             |          |
|    actor_loss      | 2.19     |
|    critic_loss     | 2.67     |
|    learning_rate   | 0.000984 |
|    n_updates       | 13292    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -21.3    |
| time/              |          |
|    episodes        | 68       |
|    fps             | 13       |
|    time_elapsed    | 1229     |
|    total timesteps | 16962    |
| train/             |          |
|    actor_loss      | 2.31     |
|    critic_loss     | 2.68     |
|    learning_rate   | 0.000983 |
|    n_updates       | 14272    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -21      |
| time/              |          |
|    episodes        | 72       |
|    fps             | 13       |
|    time_elapsed    | 1309     |
|    total timesteps | 17966    |
| train/             |          |
|    actor_loss      | 2.43     |
|    critic_loss     | 2.62     |
|    learning_rate   | 0.000982 |
|    n_updates       | 15276    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -20.9    |
| time/              |          |
|    episodes        | 76       |
|    fps             | 13       |
|    time_elapsed    | 1390     |
|    total timesteps | 18970    |
| train/             |          |
|    actor_loss      | 2.55     |
|    critic_loss     | 2.33     |
|    learning_rate   | 0.000981 |
|    n_updates       | 16280    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -19.9    |
| time/              |          |
|    episodes        | 80       |
|    fps             | 13       |
|    time_elapsed    | 1469     |
|    total timesteps | 19974    |
| train/             |          |
|    actor_loss      | 2.66     |
|    critic_loss     | 2.21     |
|    learning_rate   | 0.00098  |
|    n_updates       | 17284    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-36.23 +/- 14.11
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -36.2    |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 2.67     |
|    critic_loss     | 2.11     |
|    learning_rate   | 0.00098  |
|    n_updates       | 17535    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -19.8    |
| time/              |          |
|    episodes        | 84       |
|    fps             | 13       |
|    time_elapsed    | 1571     |
|    total timesteps | 20978    |
| train/             |          |
|    actor_loss      | 2.75     |
|    critic_loss     | 2.49     |
|    learning_rate   | 0.000979 |
|    n_updates       | 18288    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -20.3    |
| time/              |          |
|    episodes        | 88       |
|    fps             | 13       |
|    time_elapsed    | 1654     |
|    total timesteps | 21982    |
| train/             |          |
|    actor_loss      | 2.84     |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.000978 |
|    n_updates       | 19292    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -19.2    |
| time/              |          |
|    episodes        | 92       |
|    fps             | 13       |
|    time_elapsed    | 1735     |
|    total timesteps | 22986    |
| train/             |          |
|    actor_loss      | 2.97     |
|    critic_loss     | 3.2      |
|    learning_rate   | 0.000977 |
|    n_updates       | 20296    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -18.6    |
| time/              |          |
|    episodes        | 96       |
|    fps             | 13       |
|    time_elapsed    | 1816     |
|    total timesteps | 23990    |
| train/             |          |
|    actor_loss      | 3.03     |
|    critic_loss     | 2.67     |
|    learning_rate   | 0.000976 |
|    n_updates       | 21300    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -17.9    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 13       |
|    time_elapsed    | 1891     |
|    total timesteps | 24927    |
| train/             |          |
|    actor_loss      | 3.12     |
|    critic_loss     | 2.42     |
|    learning_rate   | 0.000976 |
|    n_updates       | 22237    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-42.58 +/- 19.09
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -42.6    |
| time/              |          |
|    total_timesteps | 25000    |
| train/             |          |
|    actor_loss      | 3.15     |
|    critic_loss     | 2.8      |
|    learning_rate   | 0.000975 |
|    n_updates       | 22488    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | -17      |
| time/              |          |
|    episodes        | 104      |
|    fps             | 13       |
|    time_elapsed    | 1990     |
|    total timesteps | 25931    |
| train/             |          |
|    actor_loss      | 3.21     |
|    critic_loss     | 2.13     |
|    learning_rate   | 0.000975 |
|    n_updates       | 23241    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    episodes        | 108      |
|    fps             | 13       |
|    time_elapsed    | 2070     |
|    total timesteps | 26935    |
| train/             |          |
|    actor_loss      | 3.27     |
|    critic_loss     | 2.52     |
|    learning_rate   | 0.000974 |
|    n_updates       | 24245    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16      |
| time/              |          |
|    episodes        | 112      |
|    fps             | 12       |
|    time_elapsed    | 2150     |
|    total timesteps | 27939    |
| train/             |          |
|    actor_loss      | 3.33     |
|    critic_loss     | 2.18     |
|    learning_rate   | 0.000973 |
|    n_updates       | 25249    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.9    |
| time/              |          |
|    episodes        | 116      |
|    fps             | 12       |
|    time_elapsed    | 2231     |
|    total timesteps | 28943    |
| train/             |          |
|    actor_loss      | 3.4      |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.000972 |
|    n_updates       | 26253    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -17.8    |
| time/              |          |
|    episodes        | 120      |
|    fps             | 12       |
|    time_elapsed    | 2313     |
|    total timesteps | 29947    |
| train/             |          |
|    actor_loss      | 3.45     |
|    critic_loss     | 2.44     |
|    learning_rate   | 0.000971 |
|    n_updates       | 27257    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-39.10 +/- 12.67
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -39.1    |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 3.47     |
|    critic_loss     | 2.37     |
|    learning_rate   | 0.00097  |
|    n_updates       | 27508    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -18      |
| time/              |          |
|    episodes        | 124      |
|    fps             | 12       |
|    time_elapsed    | 2413     |
|    total timesteps | 30951    |
| train/             |          |
|    actor_loss      | 3.48     |
|    critic_loss     | 2.28     |
|    learning_rate   | 0.00097  |
|    n_updates       | 28261    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -18      |
| time/              |          |
|    episodes        | 128      |
|    fps             | 12       |
|    time_elapsed    | 2493     |
|    total timesteps | 31955    |
| train/             |          |
|    actor_loss      | 3.53     |
|    critic_loss     | 2        |
|    learning_rate   | 0.000969 |
|    n_updates       | 29265    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.7    |
| time/              |          |
|    episodes        | 132      |
|    fps             | 12       |
|    time_elapsed    | 2575     |
|    total timesteps | 32959    |
| train/             |          |
|    actor_loss      | 3.61     |
|    critic_loss     | 2.39     |
|    learning_rate   | 0.000968 |
|    n_updates       | 30269    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.7    |
| time/              |          |
|    episodes        | 136      |
|    fps             | 12       |
|    time_elapsed    | 2656     |
|    total timesteps | 33963    |
| train/             |          |
|    actor_loss      | 3.64     |
|    critic_loss     | 1.84     |
|    learning_rate   | 0.000967 |
|    n_updates       | 31273    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -14.8    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 12       |
|    time_elapsed    | 2736     |
|    total timesteps | 34967    |
| train/             |          |
|    actor_loss      | 3.67     |
|    critic_loss     | 1.69     |
|    learning_rate   | 0.000966 |
|    n_updates       | 32277    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-35.89 +/- 11.40
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -35.9    |
| time/              |          |
|    total_timesteps | 35000    |
| train/             |          |
|    actor_loss      | 3.7      |
|    critic_loss     | 2.08     |
|    learning_rate   | 0.000965 |
|    n_updates       | 32528    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -14.9    |
| time/              |          |
|    episodes        | 144      |
|    fps             | 12       |
|    time_elapsed    | 2835     |
|    total timesteps | 35971    |
| train/             |          |
|    actor_loss      | 3.7      |
|    critic_loss     | 1.69     |
|    learning_rate   | 0.000965 |
|    n_updates       | 33281    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -14      |
| time/              |          |
|    episodes        | 148      |
|    fps             | 12       |
|    time_elapsed    | 2916     |
|    total timesteps | 36975    |
| train/             |          |
|    actor_loss      | 3.76     |
|    critic_loss     | 2.39     |
|    learning_rate   | 0.000964 |
|    n_updates       | 34285    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -13.6    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 12       |
|    time_elapsed    | 3001     |
|    total timesteps | 37979    |
| train/             |          |
|    actor_loss      | 3.81     |
|    critic_loss     | 2.3      |
|    learning_rate   | 0.000963 |
|    n_updates       | 35289    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -14      |
| time/              |          |
|    episodes        | 156      |
|    fps             | 12       |
|    time_elapsed    | 3084     |
|    total timesteps | 38983    |
| train/             |          |
|    actor_loss      | 3.88     |
|    critic_loss     | 2.43     |
|    learning_rate   | 0.000962 |
|    n_updates       | 36293    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -14.1    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 12       |
|    time_elapsed    | 3167     |
|    total timesteps | 39987    |
| train/             |          |
|    actor_loss      | 3.91     |
|    critic_loss     | 2.42     |
|    learning_rate   | 0.000961 |
|    n_updates       | 37297    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-31.85 +/- 15.23
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -31.8    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 3.9      |
|    critic_loss     | 2.21     |
|    learning_rate   | 0.00096  |
|    n_updates       | 37548    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    episodes        | 164      |
|    fps             | 12       |
|    time_elapsed    | 3269     |
|    total timesteps | 40991    |
| train/             |          |
|    actor_loss      | 3.97     |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.00096  |
|    n_updates       | 38301    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -15.5    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 12       |
|    time_elapsed    | 3354     |
|    total timesteps | 41995    |
| train/             |          |
|    actor_loss      | 3.99     |
|    critic_loss     | 2.39     |
|    learning_rate   | 0.000959 |
|    n_updates       | 39305    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -15.4    |
| time/              |          |
|    episodes        | 172      |
|    fps             | 12       |
|    time_elapsed    | 3438     |
|    total timesteps | 42999    |
| train/             |          |
|    actor_loss      | 3.96     |
|    critic_loss     | 1.94     |
|    learning_rate   | 0.000958 |
|    n_updates       | 40309    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -15.6    |
| time/              |          |
|    episodes        | 176      |
|    fps             | 12       |
|    time_elapsed    | 3524     |
|    total timesteps | 44003    |
| train/             |          |
|    actor_loss      | 4.04     |
|    critic_loss     | 2.3      |
|    learning_rate   | 0.000957 |
|    n_updates       | 41313    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-32.50 +/- 15.60
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -32.5    |
| time/              |          |
|    total_timesteps | 45000    |
| train/             |          |
|    actor_loss      | 4.09     |
|    critic_loss     | 2.23     |
|    learning_rate   | 0.000956 |
|    n_updates       | 42317    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.5    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 12       |
|    time_elapsed    | 3628     |
|    total timesteps | 45007    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -15.9    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 12       |
|    time_elapsed    | 3715     |
|    total timesteps | 46011    |
| train/             |          |
|    actor_loss      | 4.12     |
|    critic_loss     | 2.46     |
|    learning_rate   | 0.000955 |
|    n_updates       | 43321    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -15.5    |
| time/              |          |
|    episodes        | 188      |
|    fps             | 12       |
|    time_elapsed    | 3802     |
|    total timesteps | 47015    |
| train/             |          |
|    actor_loss      | 4.18     |
|    critic_loss     | 2.28     |
|    learning_rate   | 0.000954 |
|    n_updates       | 44325    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.2    |
| time/              |          |
|    episodes        | 192      |
|    fps             | 12       |
|    time_elapsed    | 3889     |
|    total timesteps | 48019    |
| train/             |          |
|    actor_loss      | 4.22     |
|    critic_loss     | 2.16     |
|    learning_rate   | 0.000953 |
|    n_updates       | 45329    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 250      |
|    ep_rew_mean     | -16.4    |
| time/              |          |
|    episodes        | 196      |
|    fps             | 12       |
|    time_elapsed    | 3981     |
|    total timesteps | 49023    |
| train/             |          |
|    actor_loss      | 4.25     |
|    critic_loss     | 2.08     |
|    learning_rate   | 0.000952 |
|    n_updates       | 46333    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-40.35 +/- 13.72
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -40.4    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 4.23     |
|    critic_loss     | 2.01     |
|    learning_rate   | 0.000951 |
|    n_updates       | 47337    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -17      |
| time/              |          |
|    episodes        | 200      |
|    fps             | 12       |
|    time_elapsed    | 4087     |
|    total timesteps | 50027    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -17.3    |
| time/              |          |
|    episodes        | 204      |
|    fps             | 12       |
|    time_elapsed    | 4177     |
|    total timesteps | 51031    |
| train/             |          |
|    actor_loss      | 4.24     |
|    critic_loss     | 2.33     |
|    learning_rate   | 0.00095  |
|    n_updates       | 48341    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -17.5    |
| time/              |          |
|    episodes        | 208      |
|    fps             | 12       |
|    time_elapsed    | 4265     |
|    total timesteps | 52035    |
| train/             |          |
|    actor_loss      | 4.27     |
|    critic_loss     | 2.14     |
|    learning_rate   | 0.000949 |
|    n_updates       | 49345    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -18.1    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 12       |
|    time_elapsed    | 4355     |
|    total timesteps | 53039    |
| train/             |          |
|    actor_loss      | 4.31     |
|    critic_loss     | 2.12     |
|    learning_rate   | 0.000948 |
|    n_updates       | 50349    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -18.1    |
| time/              |          |
|    episodes        | 216      |
|    fps             | 12       |
|    time_elapsed    | 4444     |
|    total timesteps | 54043    |
| train/             |          |
|    actor_loss      | 4.36     |
|    critic_loss     | 2.49     |
|    learning_rate   | 0.000947 |
|    n_updates       | 51353    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-24.70 +/- 4.63
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -24.7    |
| time/              |          |
|    total_timesteps | 55000    |
| train/             |          |
|    actor_loss      | 4.36     |
|    critic_loss     | 2.11     |
|    learning_rate   | 0.000946 |
|    n_updates       | 52357    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -17.9    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 12       |
|    time_elapsed    | 4551     |
|    total timesteps | 55047    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -17.8    |
| time/              |          |
|    episodes        | 224      |
|    fps             | 12       |
|    time_elapsed    | 4641     |
|    total timesteps | 56051    |
| train/             |          |
|    actor_loss      | 4.35     |
|    critic_loss     | 1.99     |
|    learning_rate   | 0.000945 |
|    n_updates       | 53361    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -17.7    |
| time/              |          |
|    episodes        | 228      |
|    fps             | 12       |
|    time_elapsed    | 4732     |
|    total timesteps | 57055    |
| train/             |          |
|    actor_loss      | 4.35     |
|    critic_loss     | 1.68     |
|    learning_rate   | 0.000944 |
|    n_updates       | 54365    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -18.5    |
| time/              |          |
|    episodes        | 232      |
|    fps             | 12       |
|    time_elapsed    | 4822     |
|    total timesteps | 58059    |
| train/             |          |
|    actor_loss      | 4.4      |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.000943 |
|    n_updates       | 55369    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -19      |
| time/              |          |
|    episodes        | 236      |
|    fps             | 12       |
|    time_elapsed    | 4914     |
|    total timesteps | 59063    |
| train/             |          |
|    actor_loss      | 4.46     |
|    critic_loss     | 2.77     |
|    learning_rate   | 0.000942 |
|    n_updates       | 56373    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-45.81 +/- 23.92
Episode length: 251.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -45.8    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 4.44     |
|    critic_loss     | 1.88     |
|    learning_rate   | 0.000941 |
|    n_updates       | 57377    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -19.6    |
| time/              |          |
|    episodes        | 240      |
|    fps             | 11       |
|    time_elapsed    | 5025     |
|    total timesteps | 60067    |
---------------------------------
Terminated
2021-12-17 14:22:46.480619: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shandilya/.mujoco/mjpro150/bin
2021-12-17 14:22:46.480655: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/home/shandilya/Desktop/Projects/NavigationController
Env Type: maze
Task: GoalRewardSimple
Model: <class 'stable_baselines3.td3.td3.TD3'>
Using cpu device
Logging to assets/out/models/exp17/TD3_11
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -11.8    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 68       |
|    time_elapsed    | 14       |
|    total timesteps | 1004     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -21      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 72       |
|    time_elapsed    | 27       |
|    total timesteps | 2008     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | -22.8    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 32       |
|    time_elapsed    | 93       |
|    total timesteps | 3012     |
| train/             |          |
|    actor_loss      | 0.129    |
|    critic_loss     | 2.39     |
|    learning_rate   | 0.000997 |
|    n_updates       | 502      |
---------------------------------
