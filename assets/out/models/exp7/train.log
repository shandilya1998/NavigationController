2021-12-02 22:57:37.159931: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-02 22:57:37.160005: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp7/TD3_5
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -5.18    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 8        |
|    time_elapsed     | 738      |
|    total timesteps  | 6004     |
| train/              |          |
|    actor_loss       | -0.00636 |
|    critic_loss      | 0.00166  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1101     |
|    total_actor_loss | -0.00589 |
|    value_loss       | 0.000469 |
----------------------------------
Eval num_timesteps=9000, episode_reward=-4.12 +/- 6.96
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -4.12    |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | 0.00504  |
|    critic_loss      | 0.00489  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1698     |
|    total_actor_loss | 0.00702  |
|    value_loss       | 0.00197  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -6.8     |
| time/               |          |
|    episodes         | 8        |
|    fps              | 6        |
|    time_elapsed     | 1917     |
|    total timesteps  | 12008    |
| train/              |          |
|    actor_loss       | -0.00589 |
|    critic_loss      | 0.00249  |
|    learning_rate    | 0.0007   |
|    n_updates        | 2301     |
|    total_actor_loss | -0.00478 |
|    value_loss       | 0.00111  |
----------------------------------
Eval num_timesteps=18000, episode_reward=-1.02 +/- 4.61
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -1.02    |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | 0.009    |
|    critic_loss      | 0.0051   |
|    learning_rate    | 0.0007   |
|    n_updates        | 3498     |
|    total_actor_loss | 0.0123   |
|    value_loss       | 0.00334  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -4.97    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 5        |
|    time_elapsed     | 3109     |
|    total timesteps  | 18012    |
| train/              |          |
|    actor_loss       | 0.0111   |
|    critic_loss      | 0.00224  |
|    learning_rate    | 0.0007   |
|    n_updates        | 3501     |
|    total_actor_loss | 0.0119   |
|    value_loss       | 0.000809 |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -6.01    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 6        |
|    time_elapsed     | 3888     |
|    total timesteps  | 24016    |
| train/              |          |
|    actor_loss       | 0.0173   |
|    critic_loss      | 0.00179  |
|    learning_rate    | 0.0007   |
|    n_updates        | 4704     |
|    total_actor_loss | 0.0183   |
|    value_loss       | 0.00109  |
----------------------------------
Eval num_timesteps=27000, episode_reward=-0.56 +/- 4.64
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -0.56    |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | 0.0263   |
|    critic_loss      | 0.0011   |
|    learning_rate    | 0.0007   |
|    n_updates        | 5298     |
|    total_actor_loss | 0.0269   |
|    value_loss       | 0.000644 |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -6.14    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 5        |
|    time_elapsed     | 5071     |
|    total timesteps  | 30020    |
| train/              |          |
|    actor_loss       | 0.0266   |
|    critic_loss      | 0.00144  |
|    learning_rate    | 0.0007   |
|    n_updates        | 5904     |
|    total_actor_loss | 0.0274   |
|    value_loss       | 0.000825 |
----------------------------------
Eval num_timesteps=36000, episode_reward=-9.12 +/- 6.48
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -9.12    |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | 0.0268   |
|    critic_loss      | 0.00118  |
|    learning_rate    | 0.0007   |
|    n_updates        | 7098     |
|    total_actor_loss | 0.0275   |
|    value_loss       | 0.000645 |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -5.38    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 5        |
|    time_elapsed     | 6257     |
|    total timesteps  | 36024    |
| train/              |          |
|    actor_loss       | 0.0226   |
|    critic_loss      | 0.00367  |
|    learning_rate    | 0.0007   |
|    n_updates        | 7104     |
|    total_actor_loss | 0.0243   |
|    value_loss       | 0.00172  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -4.01    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 5        |
|    time_elapsed     | 7028     |
|    total timesteps  | 42028    |
| train/              |          |
|    actor_loss       | 0.0198   |
|    critic_loss      | 0.00411  |
|    learning_rate    | 0.0007   |
|    n_updates        | 8304     |
|    total_actor_loss | 0.0218   |
|    value_loss       | 0.00194  |
----------------------------------
Eval num_timesteps=45000, episode_reward=-1.38 +/- 13.12
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -1.38    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | 0.0164   |
|    critic_loss      | 0.00146  |
|    learning_rate    | 0.0007   |
|    n_updates        | 8898     |
|    total_actor_loss | 0.0171   |
|    value_loss       | 0.000727 |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -4.55    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 5        |
|    time_elapsed     | 8211     |
|    total timesteps  | 48032    |
| train/              |          |
|    actor_loss       | 0.0199   |
|    critic_loss      | 0.00155  |
|    learning_rate    | 0.0007   |
|    n_updates        | 9507     |
|    total_actor_loss | 0.021    |
|    value_loss       | 0.00109  |
----------------------------------
Eval num_timesteps=54000, episode_reward=-3.86 +/- 8.77
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -3.86    |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | 0.0213   |
|    critic_loss      | 0.00281  |
|    learning_rate    | 0.0007   |
|    n_updates        | 10698    |
|    total_actor_loss | 0.0224   |
|    value_loss       | 0.00108  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -4.01    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 5        |
|    time_elapsed     | 9385     |
|    total timesteps  | 54036    |
| train/              |          |
|    actor_loss       | 0.0231   |
|    critic_loss      | 0.00331  |
|    learning_rate    | 0.0007   |
|    n_updates        | 10707    |
|    total_actor_loss | 0.0244   |
|    value_loss       | 0.00123  |
----------------------------------
Terminated
2021-12-03 01:38:54.875688: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-03 01:38:54.875757: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp7/TD3_6
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.4e+03  |
|    ep_rew_mean      | 23       |
| time/               |          |
|    episodes         | 4        |
|    fps              | 8        |
|    time_elapsed     | 684      |
|    total timesteps  | 5599     |
| train/              |          |
|    actor_loss       | 0.00141  |
|    critic_loss      | 0.00943  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1020     |
|    total_actor_loss | 0.0108   |
|    value_loss       | 0.00936  |
----------------------------------
Eval num_timesteps=9000, episode_reward=11.25 +/- 7.87
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 11.2     |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | -0.0381  |
|    critic_loss      | 0.00533  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1698     |
|    total_actor_loss | -0.0361  |
|    value_loss       | 0.00201  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.45e+03 |
|    ep_rew_mean      | 17       |
| time/               |          |
|    episodes         | 8        |
|    fps              | 6        |
|    time_elapsed     | 1882     |
|    total timesteps  | 11603    |
| train/              |          |
|    actor_loss       | -0.0126  |
|    critic_loss      | 0.018    |
|    learning_rate    | 0.0007   |
|    n_updates        | 2220     |
|    total_actor_loss | 0.000826 |
|    value_loss       | 0.0134   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.47e+03 |
|    ep_rew_mean      | 15.5     |
| time/               |          |
|    episodes         | 12       |
|    fps              | 6        |
|    time_elapsed     | 2654     |
|    total timesteps  | 17607    |
| train/              |          |
|    actor_loss       | -0.0598  |
|    critic_loss      | 0.00524  |
|    learning_rate    | 0.0007   |
|    n_updates        | 3420     |
|    total_actor_loss | -0.0497  |
|    value_loss       | 0.0101   |
----------------------------------
Eval num_timesteps=18000, episode_reward=8.33 +/- 3.54
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 8.33     |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | -0.00421 |
|    critic_loss      | 0.0153   |
|    learning_rate    | 0.0007   |
|    n_updates        | 3498     |
|    total_actor_loss | 0.00188  |
|    value_loss       | 0.00609  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.48e+03 |
|    ep_rew_mean      | 12.6     |
| time/               |          |
|    episodes         | 16       |
|    fps              | 6        |
|    time_elapsed     | 3839     |
|    total timesteps  | 23611    |
| train/              |          |
|    actor_loss       | 0.0136   |
|    critic_loss      | 0.0127   |
|    learning_rate    | 0.0007   |
|    n_updates        | 4623     |
|    total_actor_loss | 0.0245   |
|    value_loss       | 0.0109   |
----------------------------------
Eval num_timesteps=27000, episode_reward=8.52 +/- 10.98
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 8.52     |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | -0.0071  |
|    critic_loss      | 0.00848  |
|    learning_rate    | 0.0007   |
|    n_updates        | 5298     |
|    total_actor_loss | -0.00072 |
|    value_loss       | 0.00638  |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 1.48e+03  |
|    ep_rew_mean      | 12.1      |
| time/               |           |
|    episodes         | 20        |
|    fps              | 5         |
|    time_elapsed     | 5017      |
|    total timesteps  | 29615     |
| train/              |           |
|    actor_loss       | -0.000189 |
|    critic_loss      | 0.00406   |
|    learning_rate    | 0.0007    |
|    n_updates        | 5823      |
|    total_actor_loss | 0.000898  |
|    value_loss       | 0.00109   |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.48e+03 |
|    ep_rew_mean      | 18.6     |
| time/               |          |
|    episodes         | 24       |
|    fps              | 6        |
|    time_elapsed     | 5794     |
|    total timesteps  | 35619    |
| train/              |          |
|    actor_loss       | -0.0979  |
|    critic_loss      | 0.0179   |
|    learning_rate    | 0.0007   |
|    n_updates        | 7023     |
|    total_actor_loss | -0.0833  |
|    value_loss       | 0.0146   |
----------------------------------
Eval num_timesteps=36000, episode_reward=9.09 +/- 7.04
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 9.09     |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | -0.152   |
|    critic_loss      | 0.00822  |
|    learning_rate    | 0.0007   |
|    n_updates        | 7098     |
|    total_actor_loss | -0.135   |
|    value_loss       | 0.0167   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 16.8     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 5        |
|    time_elapsed     | 6958     |
|    total timesteps  | 41623    |
| train/              |          |
|    actor_loss       | -0.122   |
|    critic_loss      | 0.0215   |
|    learning_rate    | 0.0007   |
|    n_updates        | 8223     |
|    total_actor_loss | -0.107   |
|    value_loss       | 0.0146   |
----------------------------------
Eval num_timesteps=45000, episode_reward=16.31 +/- 6.48
Episode length: 1457.80 +/- 86.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.46e+03 |
|    mean_reward      | 16.3     |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | -0.024   |
|    critic_loss      | 0.0168   |
|    learning_rate    | 0.0007   |
|    n_updates        | 8898     |
|    total_actor_loss | -0.0146  |
|    value_loss       | 0.0094   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 19.2     |
| time/               |          |
|    episodes         | 32       |
|    fps              | 5        |
|    time_elapsed     | 8136     |
|    total timesteps  | 47627    |
| train/              |          |
|    actor_loss       | -0.0879  |
|    critic_loss      | 0.0199   |
|    learning_rate    | 0.0007   |
|    n_updates        | 9426     |
|    total_actor_loss | -0.0763  |
|    value_loss       | 0.0116   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 17.9     |
| time/               |          |
|    episodes         | 36       |
|    fps              | 6        |
|    time_elapsed     | 8924     |
|    total timesteps  | 53624    |
| train/              |          |
|    actor_loss       | -0.103   |
|    critic_loss      | 0.0164   |
|    learning_rate    | 0.0007   |
|    n_updates        | 10623    |
|    total_actor_loss | -0.0822  |
|    value_loss       | 0.0211   |
----------------------------------
Eval num_timesteps=54000, episode_reward=28.07 +/- 18.67
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 28.1     |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | -0.157   |
|    critic_loss      | 0.0186   |
|    learning_rate    | 0.0007   |
|    n_updates        | 10698    |
|    total_actor_loss | -0.142   |
|    value_loss       | 0.0146   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 17.5     |
| time/               |          |
|    episodes         | 40       |
|    fps              | 5        |
|    time_elapsed     | 10101    |
|    total timesteps  | 59628    |
| train/              |          |
|    actor_loss       | -0.00979 |
|    critic_loss      | 0.0063   |
|    learning_rate    | 0.0007   |
|    n_updates        | 11826    |
|    total_actor_loss | -0.0055  |
|    value_loss       | 0.00429  |
----------------------------------
Eval num_timesteps=63000, episode_reward=26.58 +/- 13.20
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 26.6     |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    actor_loss       | 0.00546  |
|    critic_loss      | 0.00348  |
|    learning_rate    | 0.0007   |
|    n_updates        | 12498    |
|    total_actor_loss | 0.0132   |
|    value_loss       | 0.00778  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 17.5     |
| time/               |          |
|    episodes         | 44       |
|    fps              | 5        |
|    time_elapsed     | 11259    |
|    total timesteps  | 65632    |
| train/              |          |
|    actor_loss       | -0.00475 |
|    critic_loss      | 0.011    |
|    learning_rate    | 0.0007   |
|    n_updates        | 13026    |
|    total_actor_loss | 0.0172   |
|    value_loss       | 0.022    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 17.6     |
| time/               |          |
|    episodes         | 48       |
|    fps              | 5        |
|    time_elapsed     | 12033    |
|    total timesteps  | 71636    |
| train/              |          |
|    actor_loss       | -0.0527  |
|    critic_loss      | 0.026    |
|    learning_rate    | 0.0007   |
|    n_updates        | 14226    |
|    total_actor_loss | -0.0393  |
|    value_loss       | 0.0134   |
----------------------------------
Eval num_timesteps=72000, episode_reward=28.52 +/- 15.77
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 28.5     |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    actor_loss       | -0.0318  |
|    critic_loss      | 0.0124   |
|    learning_rate    | 0.0007   |
|    n_updates        | 14298    |
|    total_actor_loss | -0.0244  |
|    value_loss       | 0.00742  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 17.5     |
| time/               |          |
|    episodes         | 52       |
|    fps              | 5        |
|    time_elapsed     | 13206    |
|    total timesteps  | 77640    |
| train/              |          |
|    actor_loss       | -0.0162  |
|    critic_loss      | 0.00503  |
|    learning_rate    | 0.0007   |
|    n_updates        | 15426    |
|    total_actor_loss | -0.0104  |
|    value_loss       | 0.00582  |
----------------------------------
Eval num_timesteps=81000, episode_reward=14.07 +/- 15.99
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 14.1     |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    actor_loss       | -0.00953 |
|    critic_loss      | 0.00728  |
|    learning_rate    | 0.0007   |
|    n_updates        | 16098    |
|    total_actor_loss | -0.00278 |
|    value_loss       | 0.00675  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 16.9     |
| time/               |          |
|    episodes         | 56       |
|    fps              | 5        |
|    time_elapsed     | 14393    |
|    total timesteps  | 83644    |
| train/              |          |
|    actor_loss       | -0.06    |
|    critic_loss      | 0.0069   |
|    learning_rate    | 0.0007   |
|    n_updates        | 16629    |
|    total_actor_loss | -0.0565  |
|    value_loss       | 0.0035   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 16.4     |
| time/               |          |
|    episodes         | 60       |
|    fps              | 5        |
|    time_elapsed     | 15168    |
|    total timesteps  | 89648    |
| train/              |          |
|    actor_loss       | -0.00182 |
|    critic_loss      | 0.00442  |
|    learning_rate    | 0.0007   |
|    n_updates        | 17829    |
|    total_actor_loss | 0.00267  |
|    value_loss       | 0.0045   |
----------------------------------
Eval num_timesteps=90000, episode_reward=21.81 +/- 15.56
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 21.8     |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    actor_loss       | -0.0121  |
|    critic_loss      | 0.00972  |
|    learning_rate    | 0.0007   |
|    n_updates        | 17898    |
|    total_actor_loss | -0.00138 |
|    value_loss       | 0.0107   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 16.1     |
| time/               |          |
|    episodes         | 64       |
|    fps              | 5        |
|    time_elapsed     | 16350    |
|    total timesteps  | 95652    |
| train/              |          |
|    actor_loss       | -0.0639  |
|    critic_loss      | 0.00122  |
|    learning_rate    | 0.0007   |
|    n_updates        | 19029    |
|    total_actor_loss | -0.0627  |
|    value_loss       | 0.00123  |
----------------------------------
Eval num_timesteps=99000, episode_reward=36.68 +/- 23.76
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 36.7     |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    actor_loss       | -0.068   |
|    critic_loss      | 0.00622  |
|    learning_rate    | 0.0007   |
|    n_updates        | 19698    |
|    total_actor_loss | -0.0641  |
|    value_loss       | 0.00392  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.49e+03 |
|    ep_rew_mean      | 17.9     |
| time/               |          |
|    episodes         | 68       |
|    fps              | 5        |
|    time_elapsed     | 17538    |
|    total timesteps  | 101656   |
| train/              |          |
|    actor_loss       | -0.127   |
|    critic_loss      | 0.00968  |
|    learning_rate    | 0.0007   |
|    n_updates        | 20232    |
|    total_actor_loss | -0.114   |
|    value_loss       | 0.0126   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 19       |
| time/               |          |
|    episodes         | 72       |
|    fps              | 5        |
|    time_elapsed     | 18311    |
|    total timesteps  | 107660   |
| train/              |          |
|    actor_loss       | -0.233   |
|    critic_loss      | 0.0188   |
|    learning_rate    | 0.0007   |
|    n_updates        | 21432    |
|    total_actor_loss | -0.214   |
|    value_loss       | 0.0196   |
----------------------------------
Eval num_timesteps=108000, episode_reward=11.51 +/- 4.41
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 11.5     |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    actor_loss       | -0.226   |
|    critic_loss      | 0.0374   |
|    learning_rate    | 0.0007   |
|    n_updates        | 21498    |
|    total_actor_loss | -0.197   |
|    value_loss       | 0.0291   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 18.5     |
| time/               |          |
|    episodes         | 76       |
|    fps              | 5        |
|    time_elapsed     | 19497    |
|    total timesteps  | 113664   |
| train/              |          |
|    actor_loss       | -0.196   |
|    critic_loss      | 0.0151   |
|    learning_rate    | 0.0007   |
|    n_updates        | 22632    |
|    total_actor_loss | -0.187   |
|    value_loss       | 0.00933  |
----------------------------------
Eval num_timesteps=117000, episode_reward=8.16 +/- 4.92
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 8.16     |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    actor_loss       | -0.119   |
|    critic_loss      | 0.00225  |
|    learning_rate    | 0.0007   |
|    n_updates        | 23298    |
|    total_actor_loss | -0.116   |
|    value_loss       | 0.0027   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 18       |
| time/               |          |
|    episodes         | 80       |
|    fps              | 5        |
|    time_elapsed     | 20668    |
|    total timesteps  | 119668   |
| train/              |          |
|    actor_loss       | -0.111   |
|    critic_loss      | 0.00301  |
|    learning_rate    | 0.0007   |
|    n_updates        | 23832    |
|    total_actor_loss | -0.109   |
|    value_loss       | 0.00229  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.6     |
| time/               |          |
|    episodes         | 84       |
|    fps              | 5        |
|    time_elapsed     | 21451    |
|    total timesteps  | 125672   |
| train/              |          |
|    actor_loss       | -0.097   |
|    critic_loss      | 0.00198  |
|    learning_rate    | 0.0007   |
|    n_updates        | 25035    |
|    total_actor_loss | -0.0955  |
|    value_loss       | 0.00149  |
----------------------------------
Eval num_timesteps=126000, episode_reward=11.12 +/- 8.04
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 11.1     |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    actor_loss       | -0.0969  |
|    critic_loss      | 0.00413  |
|    learning_rate    | 0.0007   |
|    n_updates        | 25098    |
|    total_actor_loss | -0.0929  |
|    value_loss       | 0.00392  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.4     |
| time/               |          |
|    episodes         | 88       |
|    fps              | 5        |
|    time_elapsed     | 22630    |
|    total timesteps  | 131676   |
| train/              |          |
|    actor_loss       | -0.111   |
|    critic_loss      | 0.00245  |
|    learning_rate    | 0.0007   |
|    n_updates        | 26235    |
|    total_actor_loss | -0.107   |
|    value_loss       | 0.0032   |
----------------------------------
Eval num_timesteps=135000, episode_reward=17.61 +/- 19.29
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 17.6     |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    actor_loss       | -0.129   |
|    critic_loss      | 0.00577  |
|    learning_rate    | 0.0007   |
|    n_updates        | 26898    |
|    total_actor_loss | -0.124   |
|    value_loss       | 0.00428  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 18.1     |
| time/               |          |
|    episodes         | 92       |
|    fps              | 5        |
|    time_elapsed     | 23824    |
|    total timesteps  | 137680   |
| train/              |          |
|    actor_loss       | -0.168   |
|    critic_loss      | 0.0107   |
|    learning_rate    | 0.0007   |
|    n_updates        | 27435    |
|    total_actor_loss | -0.157   |
|    value_loss       | 0.0104   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.8     |
| time/               |          |
|    episodes         | 96       |
|    fps              | 5        |
|    time_elapsed     | 24603    |
|    total timesteps  | 143684   |
| train/              |          |
|    actor_loss       | -0.121   |
|    critic_loss      | 0.00683  |
|    learning_rate    | 0.0007   |
|    n_updates        | 28635    |
|    total_actor_loss | -0.119   |
|    value_loss       | 0.00224  |
----------------------------------
Eval num_timesteps=144000, episode_reward=16.23 +/- 12.71
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 16.2     |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    actor_loss       | -0.146   |
|    critic_loss      | 0.00941  |
|    learning_rate    | 0.0007   |
|    n_updates        | 28698    |
|    total_actor_loss | -0.142   |
|    value_loss       | 0.00416  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.7     |
| time/               |          |
|    episodes         | 100      |
|    fps              | 5        |
|    time_elapsed     | 25799    |
|    total timesteps  | 149688   |
| train/              |          |
|    actor_loss       | -0.134   |
|    critic_loss      | 0.00631  |
|    learning_rate    | 0.0007   |
|    n_updates        | 29838    |
|    total_actor_loss | -0.129   |
|    value_loss       | 0.00547  |
----------------------------------
Eval num_timesteps=153000, episode_reward=6.81 +/- 5.29
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 6.81     |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    actor_loss       | -0.111   |
|    critic_loss      | 0.00757  |
|    learning_rate    | 0.0007   |
|    n_updates        | 30498    |
|    total_actor_loss | -0.107   |
|    value_loss       | 0.00367  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.1     |
| time/               |          |
|    episodes         | 104      |
|    fps              | 5        |
|    time_elapsed     | 26978    |
|    total timesteps  | 155692   |
| train/              |          |
|    actor_loss       | -0.15    |
|    critic_loss      | 0.00991  |
|    learning_rate    | 0.0007   |
|    n_updates        | 31038    |
|    total_actor_loss | -0.144   |
|    value_loss       | 0.0058   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 18       |
| time/               |          |
|    episodes         | 108      |
|    fps              | 5        |
|    time_elapsed     | 27758    |
|    total timesteps  | 161696   |
| train/              |          |
|    actor_loss       | -0.13    |
|    critic_loss      | 0.00804  |
|    learning_rate    | 0.0007   |
|    n_updates        | 32238    |
|    total_actor_loss | -0.127   |
|    value_loss       | 0.00336  |
----------------------------------
Eval num_timesteps=162000, episode_reward=19.61 +/- 14.95
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 19.6     |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    actor_loss       | -0.133   |
|    critic_loss      | 0.0131   |
|    learning_rate    | 0.0007   |
|    n_updates        | 32298    |
|    total_actor_loss | -0.126   |
|    value_loss       | 0.00668  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.8     |
| time/               |          |
|    episodes         | 112      |
|    fps              | 5        |
|    time_elapsed     | 28937    |
|    total timesteps  | 167700   |
| train/              |          |
|    actor_loss       | -0.167   |
|    critic_loss      | 0.01     |
|    learning_rate    | 0.0007   |
|    n_updates        | 33438    |
|    total_actor_loss | -0.162   |
|    value_loss       | 0.00551  |
----------------------------------
Eval num_timesteps=171000, episode_reward=27.48 +/- 15.50
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 27.5     |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    actor_loss       | -0.136   |
|    critic_loss      | 0.00464  |
|    learning_rate    | 0.0007   |
|    n_updates        | 34098    |
|    total_actor_loss | -0.133   |
|    value_loss       | 0.00313  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 18       |
| time/               |          |
|    episodes         | 116      |
|    fps              | 5        |
|    time_elapsed     | 30129    |
|    total timesteps  | 173704   |
| train/              |          |
|    actor_loss       | -0.144   |
|    critic_loss      | 0.0034   |
|    learning_rate    | 0.0007   |
|    n_updates        | 34641    |
|    total_actor_loss | -0.142   |
|    value_loss       | 0.00188  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 18.4     |
| time/               |          |
|    episodes         | 120      |
|    fps              | 5        |
|    time_elapsed     | 30896    |
|    total timesteps  | 179708   |
| train/              |          |
|    actor_loss       | -0.144   |
|    critic_loss      | 0.0108   |
|    learning_rate    | 0.0007   |
|    n_updates        | 35841    |
|    total_actor_loss | -0.14    |
|    value_loss       | 0.00355  |
----------------------------------
Eval num_timesteps=180000, episode_reward=19.27 +/- 13.49
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 19.3     |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    actor_loss       | -0.132   |
|    critic_loss      | 0.00497  |
|    learning_rate    | 0.0007   |
|    n_updates        | 35898    |
|    total_actor_loss | -0.128   |
|    value_loss       | 0.00368  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17       |
| time/               |          |
|    episodes         | 124      |
|    fps              | 5        |
|    time_elapsed     | 32079    |
|    total timesteps  | 185712   |
| train/              |          |
|    actor_loss       | -0.19    |
|    critic_loss      | 0.0044   |
|    learning_rate    | 0.0007   |
|    n_updates        | 37041    |
|    total_actor_loss | -0.186   |
|    value_loss       | 0.00367  |
----------------------------------
Eval num_timesteps=189000, episode_reward=18.69 +/- 12.91
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 18.7     |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    actor_loss       | -0.15    |
|    critic_loss      | 0.00505  |
|    learning_rate    | 0.0007   |
|    n_updates        | 37698    |
|    total_actor_loss | -0.146   |
|    value_loss       | 0.00397  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.4     |
| time/               |          |
|    episodes         | 128      |
|    fps              | 5        |
|    time_elapsed     | 33266    |
|    total timesteps  | 191716   |
| train/              |          |
|    actor_loss       | -0.164   |
|    critic_loss      | 0.00582  |
|    learning_rate    | 0.0007   |
|    n_updates        | 38244    |
|    total_actor_loss | -0.158   |
|    value_loss       | 0.00619  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17       |
| time/               |          |
|    episodes         | 132      |
|    fps              | 5        |
|    time_elapsed     | 34059    |
|    total timesteps  | 197720   |
| train/              |          |
|    actor_loss       | -0.201   |
|    critic_loss      | 0.0131   |
|    learning_rate    | 0.0007   |
|    n_updates        | 39444    |
|    total_actor_loss | -0.192   |
|    value_loss       | 0.00844  |
----------------------------------
Eval num_timesteps=198000, episode_reward=17.19 +/- 12.62
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 17.2     |
| time/               |          |
|    total_timesteps  | 198000   |
| train/              |          |
|    actor_loss       | -0.199   |
|    critic_loss      | 0.00662  |
|    learning_rate    | 0.0007   |
|    n_updates        | 39498    |
|    total_actor_loss | -0.195   |
|    value_loss       | 0.00401  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.4     |
| time/               |          |
|    episodes         | 136      |
|    fps              | 5        |
|    time_elapsed     | 35250    |
|    total timesteps  | 203724   |
| train/              |          |
|    actor_loss       | -0.245   |
|    critic_loss      | 0.00919  |
|    learning_rate    | 0.0007   |
|    n_updates        | 40644    |
|    total_actor_loss | -0.242   |
|    value_loss       | 0.00289  |
----------------------------------
Eval num_timesteps=207000, episode_reward=12.71 +/- 23.54
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 12.7     |
| time/               |          |
|    total_timesteps  | 207000   |
| train/              |          |
|    actor_loss       | -0.243   |
|    critic_loss      | 0.00915  |
|    learning_rate    | 0.0007   |
|    n_updates        | 41298    |
|    total_actor_loss | -0.234   |
|    value_loss       | 0.00849  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 17.5     |
| time/               |          |
|    episodes         | 140      |
|    fps              | 5        |
|    time_elapsed     | 36439    |
|    total timesteps  | 209728   |
| train/              |          |
|    actor_loss       | -0.237   |
|    critic_loss      | 0.00974  |
|    learning_rate    | 0.0007   |
|    n_updates        | 41844    |
|    total_actor_loss | -0.225   |
|    value_loss       | 0.0118   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 19.2     |
| time/               |          |
|    episodes         | 144      |
|    fps              | 5        |
|    time_elapsed     | 37219    |
|    total timesteps  | 215732   |
| train/              |          |
|    actor_loss       | -0.293   |
|    critic_loss      | 0.018    |
|    learning_rate    | 0.0007   |
|    n_updates        | 43047    |
|    total_actor_loss | -0.259   |
|    value_loss       | 0.0335   |
----------------------------------
Eval num_timesteps=216000, episode_reward=59.78 +/- 64.39
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 59.8     |
| time/               |          |
|    total_timesteps  | 216000   |
| train/              |          |
|    actor_loss       | -0.304   |
|    critic_loss      | 0.0207   |
|    learning_rate    | 0.0007   |
|    n_updates        | 43098    |
|    total_actor_loss | -0.296   |
|    value_loss       | 0.00798  |
----------------------------------
Terminated
2021-12-03 12:15:15.865827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-03 12:15:15.865896: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp7/TD3_8
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 49.9     |
| time/               |          |
|    episodes         | 4        |
|    fps              | 8        |
|    time_elapsed     | 729      |
|    total timesteps  | 6004     |
| train/              |          |
|    actor_loss       | -0.117   |
|    critic_loss      | 0.0378   |
|    learning_rate    | 0.0007   |
|    n_updates        | 1101     |
|    total_actor_loss | -0.095   |
|    value_loss       | 0.0224   |
----------------------------------
Eval num_timesteps=9000, episode_reward=15.59 +/- 10.00
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 15.6     |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | -0.0292  |
|    critic_loss      | 0.0152   |
|    learning_rate    | 0.0007   |
|    n_updates        | 1698     |
|    total_actor_loss | -0.0175  |
|    value_loss       | 0.0117   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 34       |
| time/               |          |
|    episodes         | 8        |
|    fps              | 6        |
|    time_elapsed     | 1924     |
|    total timesteps  | 12008    |
| train/              |          |
|    actor_loss       | -0.072   |
|    critic_loss      | 0.0119   |
|    learning_rate    | 0.0007   |
|    n_updates        | 2301     |
|    total_actor_loss | -0.0649  |
|    value_loss       | 0.00701  |
----------------------------------
Eval num_timesteps=18000, episode_reward=33.41 +/- 38.00
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 33.4     |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | -0.0639  |
|    critic_loss      | 0.025    |
|    learning_rate    | 0.0007   |
|    n_updates        | 3498     |
|    total_actor_loss | -0.037   |
|    value_loss       | 0.0269   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 28.2     |
| time/               |          |
|    episodes         | 12       |
|    fps              | 5        |
|    time_elapsed     | 3118     |
|    total timesteps  | 18012    |
| train/              |          |
|    actor_loss       | -0.0717  |
|    critic_loss      | 0.0219   |
|    learning_rate    | 0.0007   |
|    n_updates        | 3501     |
|    total_actor_loss | -0.0654  |
|    value_loss       | 0.00632  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 29.2     |
| time/               |          |
|    episodes         | 16       |
|    fps              | 6        |
|    time_elapsed     | 3903     |
|    total timesteps  | 24016    |
| train/              |          |
|    actor_loss       | -0.147   |
|    critic_loss      | 0.0146   |
|    learning_rate    | 0.0007   |
|    n_updates        | 4704     |
|    total_actor_loss | -0.137   |
|    value_loss       | 0.0101   |
----------------------------------
Eval num_timesteps=27000, episode_reward=24.82 +/- 26.00
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 24.8     |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | -0.146   |
|    critic_loss      | 0.0268   |
|    learning_rate    | 0.0007   |
|    n_updates        | 5298     |
|    total_actor_loss | -0.134   |
|    value_loss       | 0.012    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 32.1     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 5        |
|    time_elapsed     | 5094     |
|    total timesteps  | 30020    |
| train/              |          |
|    actor_loss       | -0.191   |
|    critic_loss      | 0.0132   |
|    learning_rate    | 0.0007   |
|    n_updates        | 5904     |
|    total_actor_loss | -0.179   |
|    value_loss       | 0.0117   |
----------------------------------
Eval num_timesteps=36000, episode_reward=261.79 +/- 60.16
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 262      |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | -0.326   |
|    critic_loss      | 0.0725   |
|    learning_rate    | 0.0007   |
|    n_updates        | 7098     |
|    total_actor_loss | -0.279   |
|    value_loss       | 0.047    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 40.4     |
| time/               |          |
|    episodes         | 24       |
|    fps              | 5        |
|    time_elapsed     | 6287     |
|    total timesteps  | 36024    |
| train/              |          |
|    actor_loss       | -0.329   |
|    critic_loss      | 0.0348   |
|    learning_rate    | 0.0007   |
|    n_updates        | 7104     |
|    total_actor_loss | -0.309   |
|    value_loss       | 0.0191   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 42.2     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 5        |
|    time_elapsed     | 7068     |
|    total timesteps  | 42028    |
| train/              |          |
|    actor_loss       | -0.43    |
|    critic_loss      | 0.0221   |
|    learning_rate    | 0.0007   |
|    n_updates        | 8304     |
|    total_actor_loss | -0.412   |
|    value_loss       | 0.0173   |
----------------------------------
Eval num_timesteps=45000, episode_reward=52.28 +/- 18.60
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 52.3     |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | -0.411   |
|    critic_loss      | 0.0156   |
|    learning_rate    | 0.0007   |
|    n_updates        | 8898     |
|    total_actor_loss | -0.405   |
|    value_loss       | 0.00628  |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 47       |
| time/               |          |
|    episodes         | 32       |
|    fps              | 5        |
|    time_elapsed     | 8267     |
|    total timesteps  | 48032    |
| train/              |          |
|    actor_loss       | -0.548   |
|    critic_loss      | 0.0303   |
|    learning_rate    | 0.0007   |
|    n_updates        | 9507     |
|    total_actor_loss | -0.532   |
|    value_loss       | 0.016    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.47e+03 |
|    ep_rew_mean      | 60.7     |
| time/               |          |
|    episodes         | 36       |
|    fps              | 5        |
|    time_elapsed     | 8884     |
|    total timesteps  | 52750    |
| train/              |          |
|    actor_loss       | -1.08    |
|    critic_loss      | 0.0861   |
|    learning_rate    | 0.0007   |
|    n_updates        | 10449    |
|    total_actor_loss | -1.05    |
|    value_loss       | 0.0254   |
----------------------------------
Eval num_timesteps=54000, episode_reward=49.84 +/- 44.77
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 49.8     |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | -1.05    |
|    critic_loss      | 0.0513   |
|    learning_rate    | 0.0007   |
|    n_updates        | 10698    |
|    total_actor_loss | -1.02    |
|    value_loss       | 0.0241   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.47e+03 |
|    ep_rew_mean      | 64       |
| time/               |          |
|    episodes         | 40       |
|    fps              | 5        |
|    time_elapsed     | 10069    |
|    total timesteps  | 58754    |
| train/              |          |
|    actor_loss       | -0.831   |
|    critic_loss      | 0.046    |
|    learning_rate    | 0.0007   |
|    n_updates        | 11649    |
|    total_actor_loss | -0.789   |
|    value_loss       | 0.0424   |
----------------------------------
Eval num_timesteps=63000, episode_reward=202.23 +/- 105.42
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 202      |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    actor_loss       | -1.27    |
|    critic_loss      | 0.349    |
|    learning_rate    | 0.0007   |
|    n_updates        | 12498    |
|    total_actor_loss | -0.951   |
|    value_loss       | 0.318    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.47e+03 |
|    ep_rew_mean      | 76.9     |
| time/               |          |
|    episodes         | 44       |
|    fps              | 5        |
|    time_elapsed     | 11252    |
|    total timesteps  | 64758    |
| train/              |          |
|    actor_loss       | -1.37    |
|    critic_loss      | 0.104    |
|    learning_rate    | 0.0007   |
|    n_updates        | 12852    |
|    total_actor_loss | -1.3     |
|    value_loss       | 0.0728   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.47e+03 |
|    ep_rew_mean      | 76.6     |
| time/               |          |
|    episodes         | 48       |
|    fps              | 5        |
|    time_elapsed     | 12029    |
|    total timesteps  | 70762    |
| train/              |          |
|    actor_loss       | -1.35    |
|    critic_loss      | 0.18     |
|    learning_rate    | 0.0007   |
|    n_updates        | 14052    |
|    total_actor_loss | -1.24    |
|    value_loss       | 0.107    |
----------------------------------
Terminated
2021-12-03 15:44:50.115943: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-03 15:44:50.116006: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp7/TD3_15
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 15.5     |
| time/               |          |
|    episodes         | 4        |
|    fps              | 7        |
|    time_elapsed     | 819      |
|    total timesteps  | 6004     |
| train/              |          |
|    actor_loss       | -0.0283  |
|    critic_loss      | 0.00659  |
|    learning_rate    | 0.0007   |
|    n_updates        | 1101     |
|    total_actor_loss | -0.0281  |
|    value_loss       | 0.000234 |
----------------------------------
Eval num_timesteps=9000, episode_reward=24.42 +/- 13.02
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 24.4     |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | -0.05    |
|    critic_loss      | 0.0322   |
|    learning_rate    | 0.0007   |
|    n_updates        | 1698     |
|    total_actor_loss | -0.0302  |
|    value_loss       | 0.0198   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 29.4     |
| time/               |          |
|    episodes         | 8        |
|    fps              | 5        |
|    time_elapsed     | 2138     |
|    total timesteps  | 12008    |
| train/              |          |
|    actor_loss       | -0.0812  |
|    critic_loss      | 0.0168   |
|    learning_rate    | 0.0007   |
|    n_updates        | 2301     |
|    total_actor_loss | -0.0736  |
|    value_loss       | 0.00762  |
----------------------------------
Eval num_timesteps=18000, episode_reward=60.15 +/- 29.97
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 60.1     |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | -0.161   |
|    critic_loss      | 0.037    |
|    learning_rate    | 0.0007   |
|    n_updates        | 3498     |
|    total_actor_loss | -0.142   |
|    value_loss       | 0.0186   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 28       |
| time/               |          |
|    episodes         | 12       |
|    fps              | 5        |
|    time_elapsed     | 3438     |
|    total timesteps  | 18012    |
| train/              |          |
|    actor_loss       | -0.177   |
|    critic_loss      | 0.0178   |
|    learning_rate    | 0.0007   |
|    n_updates        | 3501     |
|    total_actor_loss | -0.162   |
|    value_loss       | 0.0158   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 39       |
| time/               |          |
|    episodes         | 16       |
|    fps              | 5        |
|    time_elapsed     | 4295     |
|    total timesteps  | 24016    |
| train/              |          |
|    actor_loss       | -0.292   |
|    critic_loss      | 0.0791   |
|    learning_rate    | 0.0007   |
|    n_updates        | 4704     |
|    total_actor_loss | -0.227   |
|    value_loss       | 0.0644   |
----------------------------------
Eval num_timesteps=27000, episode_reward=55.85 +/- 37.68
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 55.8     |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | -0.337   |
|    critic_loss      | 0.0448   |
|    learning_rate    | 0.0007   |
|    n_updates        | 5298     |
|    total_actor_loss | -0.293   |
|    value_loss       | 0.0448   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 37.4     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 5        |
|    time_elapsed     | 5601     |
|    total timesteps  | 30020    |
| train/              |          |
|    actor_loss       | -0.365   |
|    critic_loss      | 0.0495   |
|    learning_rate    | 0.0007   |
|    n_updates        | 5904     |
|    total_actor_loss | -0.334   |
|    value_loss       | 0.0303   |
----------------------------------
Eval num_timesteps=36000, episode_reward=56.40 +/- 42.53
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 56.4     |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | -0.437   |
|    critic_loss      | 0.0341   |
|    learning_rate    | 0.0007   |
|    n_updates        | 7098     |
|    total_actor_loss | -0.409   |
|    value_loss       | 0.0283   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 41.8     |
| time/               |          |
|    episodes         | 24       |
|    fps              | 5        |
|    time_elapsed     | 6884     |
|    total timesteps  | 36024    |
| train/              |          |
|    actor_loss       | -0.403   |
|    critic_loss      | 0.0458   |
|    learning_rate    | 0.0007   |
|    n_updates        | 7104     |
|    total_actor_loss | -0.381   |
|    value_loss       | 0.0214   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 40.8     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 5        |
|    time_elapsed     | 7736     |
|    total timesteps  | 42028    |
| train/              |          |
|    actor_loss       | -0.43    |
|    critic_loss      | 0.0749   |
|    learning_rate    | 0.0007   |
|    n_updates        | 8304     |
|    total_actor_loss | -0.376   |
|    value_loss       | 0.0542   |
----------------------------------
Eval num_timesteps=45000, episode_reward=43.51 +/- 45.50
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 43.5     |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | -0.423   |
|    critic_loss      | 0.049    |
|    learning_rate    | 0.0007   |
|    n_updates        | 8898     |
|    total_actor_loss | -0.396   |
|    value_loss       | 0.0272   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 38.9     |
| time/               |          |
|    episodes         | 32       |
|    fps              | 5        |
|    time_elapsed     | 9019     |
|    total timesteps  | 48032    |
| train/              |          |
|    actor_loss       | -0.42    |
|    critic_loss      | 0.0298   |
|    learning_rate    | 0.0007   |
|    n_updates        | 9507     |
|    total_actor_loss | -0.371   |
|    value_loss       | 0.0495   |
----------------------------------
Eval num_timesteps=54000, episode_reward=28.06 +/- 21.07
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 28.1     |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | -0.441   |
|    critic_loss      | 0.0277   |
|    learning_rate    | 0.0007   |
|    n_updates        | 10698    |
|    total_actor_loss | -0.43    |
|    value_loss       | 0.0114   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 38       |
| time/               |          |
|    episodes         | 36       |
|    fps              | 5        |
|    time_elapsed     | 10305    |
|    total timesteps  | 54036    |
| train/              |          |
|    actor_loss       | -0.467   |
|    critic_loss      | 0.0282   |
|    learning_rate    | 0.0007   |
|    n_updates        | 10707    |
|    total_actor_loss | -0.436   |
|    value_loss       | 0.0306   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 39.2     |
| time/               |          |
|    episodes         | 40       |
|    fps              | 5        |
|    time_elapsed     | 11160    |
|    total timesteps  | 60040    |
| train/              |          |
|    actor_loss       | -0.442   |
|    critic_loss      | 0.0641   |
|    learning_rate    | 0.0007   |
|    n_updates        | 11907    |
|    total_actor_loss | -0.431   |
|    value_loss       | 0.0108   |
----------------------------------
Eval num_timesteps=63000, episode_reward=27.20 +/- 13.45
Episode length: 1283.20 +/- 435.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.28e+03 |
|    mean_reward      | 27.2     |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    actor_loss       | -0.446   |
|    critic_loss      | 0.102    |
|    learning_rate    | 0.0007   |
|    n_updates        | 12498    |
|    total_actor_loss | -0.399   |
|    value_loss       | 0.047    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 38.8     |
| time/               |          |
|    episodes         | 44       |
|    fps              | 5        |
|    time_elapsed     | 12374    |
|    total timesteps  | 66044    |
| train/              |          |
|    actor_loss       | -0.423   |
|    critic_loss      | 0.0275   |
|    learning_rate    | 0.0007   |
|    n_updates        | 13107    |
|    total_actor_loss | -0.382   |
|    value_loss       | 0.0401   |
----------------------------------
Eval num_timesteps=72000, episode_reward=54.10 +/- 48.59
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 54.1     |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    actor_loss       | -0.456   |
|    critic_loss      | 0.0527   |
|    learning_rate    | 0.0007   |
|    n_updates        | 14298    |
|    total_actor_loss | -0.401   |
|    value_loss       | 0.0556   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 38.8     |
| time/               |          |
|    episodes         | 48       |
|    fps              | 5        |
|    time_elapsed     | 13677    |
|    total timesteps  | 72048    |
| train/              |          |
|    actor_loss       | -0.444   |
|    critic_loss      | 0.0622   |
|    learning_rate    | 0.0007   |
|    n_updates        | 14310    |
|    total_actor_loss | -0.392   |
|    value_loss       | 0.0516   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 36.6     |
| time/               |          |
|    episodes         | 52       |
|    fps              | 5        |
|    time_elapsed     | 14531    |
|    total timesteps  | 78052    |
| train/              |          |
|    actor_loss       | -0.417   |
|    critic_loss      | 0.0204   |
|    learning_rate    | 0.0007   |
|    n_updates        | 15510    |
|    total_actor_loss | -0.396   |
|    value_loss       | 0.021    |
----------------------------------
Eval num_timesteps=81000, episode_reward=12.42 +/- 15.30
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 12.4     |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    actor_loss       | -0.361   |
|    critic_loss      | 0.0191   |
|    learning_rate    | 0.0007   |
|    n_updates        | 16098    |
|    total_actor_loss | -0.338   |
|    value_loss       | 0.0231   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 35.8     |
| time/               |          |
|    episodes         | 56       |
|    fps              | 5        |
|    time_elapsed     | 15822    |
|    total timesteps  | 84056    |
| train/              |          |
|    actor_loss       | -0.384   |
|    critic_loss      | 0.0171   |
|    learning_rate    | 0.0007   |
|    n_updates        | 16710    |
|    total_actor_loss | -0.376   |
|    value_loss       | 0.00779  |
----------------------------------
Eval num_timesteps=90000, episode_reward=125.28 +/- 145.91
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 125      |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    actor_loss       | -0.496   |
|    critic_loss      | 0.136    |
|    learning_rate    | 0.0007   |
|    n_updates        | 17898    |
|    total_actor_loss | -0.395   |
|    value_loss       | 0.101    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 39.4     |
| time/               |          |
|    episodes         | 60       |
|    fps              | 5        |
|    time_elapsed     | 17115    |
|    total timesteps  | 90060    |
| train/              |          |
|    actor_loss       | -0.527   |
|    critic_loss      | 0.0925   |
|    learning_rate    | 0.0007   |
|    n_updates        | 17910    |
|    total_actor_loss | -0.467   |
|    value_loss       | 0.0598   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 38.5     |
| time/               |          |
|    episodes         | 64       |
|    fps              | 5        |
|    time_elapsed     | 17960    |
|    total timesteps  | 96064    |
| train/              |          |
|    actor_loss       | -0.695   |
|    critic_loss      | 0.177    |
|    learning_rate    | 0.0007   |
|    n_updates        | 19113    |
|    total_actor_loss | -0.657   |
|    value_loss       | 0.0385   |
----------------------------------
Eval num_timesteps=99000, episode_reward=18.03 +/- 22.78
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 18       |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    actor_loss       | -0.641   |
|    critic_loss      | 0.184    |
|    learning_rate    | 0.0007   |
|    n_updates        | 19698    |
|    total_actor_loss | -0.427   |
|    value_loss       | 0.214    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 38.7     |
| time/               |          |
|    episodes         | 68       |
|    fps              | 5        |
|    time_elapsed     | 19265    |
|    total timesteps  | 102068   |
| train/              |          |
|    actor_loss       | -0.607   |
|    critic_loss      | 0.104    |
|    learning_rate    | 0.0007   |
|    n_updates        | 20313    |
|    total_actor_loss | -0.548   |
|    value_loss       | 0.0587   |
----------------------------------
Eval num_timesteps=108000, episode_reward=26.25 +/- 29.82
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 26.3     |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    actor_loss       | -0.661   |
|    critic_loss      | 0.316    |
|    learning_rate    | 0.0007   |
|    n_updates        | 21498    |
|    total_actor_loss | -0.203   |
|    value_loss       | 0.459    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 40.7     |
| time/               |          |
|    episodes         | 72       |
|    fps              | 5        |
|    time_elapsed     | 20571    |
|    total timesteps  | 108072   |
| train/              |          |
|    actor_loss       | -0.803   |
|    critic_loss      | 0.181    |
|    learning_rate    | 0.0007   |
|    n_updates        | 21513    |
|    total_actor_loss | -0.535   |
|    value_loss       | 0.268    |
----------------------------------
Terminated
2021-12-03 21:39:19.214832: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-03 21:39:19.214905: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp7/TD3_17
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 141      |
| time/               |          |
|    episodes         | 4        |
|    fps              | 7        |
|    time_elapsed     | 787      |
|    total timesteps  | 6004     |
| train/              |          |
|    actor_loss       | -0.466   |
|    critic_loss      | 0.116    |
|    learning_rate    | 0.001    |
|    n_updates        | 1100     |
|    total_actor_loss | -0.42    |
|    value_loss       | 0.0455   |
----------------------------------
Eval num_timesteps=9000, episode_reward=118.90 +/- 101.80
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 119      |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | -0.564   |
|    critic_loss      | 0.213    |
|    learning_rate    | 0.001    |
|    n_updates        | 1698     |
|    total_actor_loss | -0.488   |
|    value_loss       | 0.0763   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 138      |
| time/               |          |
|    episodes         | 8        |
|    fps              | 5        |
|    time_elapsed     | 2065     |
|    total timesteps  | 12008    |
| train/              |          |
|    actor_loss       | -0.674   |
|    critic_loss      | 0.174    |
|    learning_rate    | 0.001    |
|    n_updates        | 2300     |
|    total_actor_loss | -0.628   |
|    value_loss       | 0.0464   |
----------------------------------
Eval num_timesteps=18000, episode_reward=101.50 +/- 36.39
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 101      |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | -0.962   |
|    critic_loss      | 0.252    |
|    learning_rate    | 0.001    |
|    n_updates        | 3498     |
|    total_actor_loss | -0.836   |
|    value_loss       | 0.126    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 115      |
| time/               |          |
|    episodes         | 12       |
|    fps              | 5        |
|    time_elapsed     | 3318     |
|    total timesteps  | 18012    |
| train/              |          |
|    actor_loss       | -0.961   |
|    critic_loss      | 0.176    |
|    learning_rate    | 0.001    |
|    n_updates        | 3502     |
|    total_actor_loss | -0.899   |
|    value_loss       | 0.0612   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 103      |
| time/               |          |
|    episodes         | 16       |
|    fps              | 5        |
|    time_elapsed     | 4158     |
|    total timesteps  | 24016    |
| train/              |          |
|    actor_loss       | -1.03    |
|    critic_loss      | 0.0507   |
|    learning_rate    | 0.001    |
|    n_updates        | 4702     |
|    total_actor_loss | -1.01    |
|    value_loss       | 0.0201   |
----------------------------------
Eval num_timesteps=27000, episode_reward=187.93 +/- 51.61
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 188      |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | -1.1     |
|    critic_loss      | 0.0661   |
|    learning_rate    | 0.001    |
|    n_updates        | 5298     |
|    total_actor_loss | -1.09    |
|    value_loss       | 0.00548  |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 101      |
| time/               |          |
|    episodes         | 20       |
|    fps              | 5        |
|    time_elapsed     | 5462     |
|    total timesteps  | 30020    |
| train/              |          |
|    actor_loss       | -1.15    |
|    critic_loss      | 0.186    |
|    learning_rate    | 0.001    |
|    n_updates        | 5902     |
|    total_actor_loss | -1.03    |
|    value_loss       | 0.119    |
----------------------------------
Eval num_timesteps=36000, episode_reward=102.72 +/- 45.92
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 103      |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | -1.29    |
|    critic_loss      | 0.081    |
|    learning_rate    | 0.001    |
|    n_updates        | 7098     |
|    total_actor_loss | -1.27    |
|    value_loss       | 0.0204   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 103      |
| time/               |          |
|    episodes         | 24       |
|    fps              | 5        |
|    time_elapsed     | 6761     |
|    total timesteps  | 36024    |
| train/              |          |
|    actor_loss       | -1.33    |
|    critic_loss      | 0.23     |
|    learning_rate    | 0.001    |
|    n_updates        | 7104     |
|    total_actor_loss | -1.2     |
|    value_loss       | 0.13     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 107      |
| time/               |          |
|    episodes         | 28       |
|    fps              | 5        |
|    time_elapsed     | 7594     |
|    total timesteps  | 42028    |
| train/              |          |
|    actor_loss       | -1.51    |
|    critic_loss      | 0.252    |
|    learning_rate    | 0.001    |
|    n_updates        | 8304     |
|    total_actor_loss | -1.42    |
|    value_loss       | 0.0964   |
----------------------------------
Eval num_timesteps=45000, episode_reward=107.81 +/- 70.69
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 108      |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | -1.65    |
|    critic_loss      | 0.119    |
|    learning_rate    | 0.001    |
|    n_updates        | 8898     |
|    total_actor_loss | -1.59    |
|    value_loss       | 0.0627   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 104      |
| time/               |          |
|    episodes         | 32       |
|    fps              | 5        |
|    time_elapsed     | 8886     |
|    total timesteps  | 48032    |
| train/              |          |
|    actor_loss       | -1.7     |
|    critic_loss      | 0.135    |
|    learning_rate    | 0.001    |
|    n_updates        | 9506     |
|    total_actor_loss | -1.65    |
|    value_loss       | 0.0481   |
----------------------------------
Eval num_timesteps=54000, episode_reward=36.54 +/- 20.47
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 36.5     |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | -1.88    |
|    critic_loss      | 0.187    |
|    learning_rate    | 0.001    |
|    n_updates        | 10698    |
|    total_actor_loss | -1.8     |
|    value_loss       | 0.0827   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 110      |
| time/               |          |
|    episodes         | 36       |
|    fps              | 5        |
|    time_elapsed     | 10185    |
|    total timesteps  | 54036    |
| train/              |          |
|    actor_loss       | -1.91    |
|    critic_loss      | 0.213    |
|    learning_rate    | 0.001    |
|    n_updates        | 10706    |
|    total_actor_loss | -1.75    |
|    value_loss       | 0.154    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 105      |
| time/               |          |
|    episodes         | 40       |
|    fps              | 5        |
|    time_elapsed     | 11034    |
|    total timesteps  | 60040    |
| train/              |          |
|    actor_loss       | -2.05    |
|    critic_loss      | 0.077    |
|    learning_rate    | 0.001    |
|    n_updates        | 11906    |
|    total_actor_loss | -2.02    |
|    value_loss       | 0.0293   |
----------------------------------
Eval num_timesteps=63000, episode_reward=57.80 +/- 85.14
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 57.8     |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    actor_loss       | -2.15    |
|    critic_loss      | 0.163    |
|    learning_rate    | 0.001    |
|    n_updates        | 12498    |
|    total_actor_loss | -2.07    |
|    value_loss       | 0.0805   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 110      |
| time/               |          |
|    episodes         | 44       |
|    fps              | 5        |
|    time_elapsed     | 12315    |
|    total timesteps  | 66044    |
| train/              |          |
|    actor_loss       | -2.21    |
|    critic_loss      | 0.16     |
|    learning_rate    | 0.001    |
|    n_updates        | 13108    |
|    total_actor_loss | -2.17    |
|    value_loss       | 0.0312   |
----------------------------------
Terminated
2021-12-04 01:17:22.982068: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-04 01:17:22.982139: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp7/TD3_18
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 131      |
| time/               |          |
|    episodes         | 4        |
|    fps              | 7        |
|    time_elapsed     | 794      |
|    total timesteps  | 6004     |
| train/              |          |
|    actor_loss       | -0.297   |
|    critic_loss      | 0.0829   |
|    learning_rate    | 0.001    |
|    n_updates        | 1100     |
|    total_actor_loss | -0.246   |
|    value_loss       | 0.0506   |
----------------------------------
Eval num_timesteps=9000, episode_reward=186.06 +/- 79.43
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 186      |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | -0.367   |
|    critic_loss      | 0.0764   |
|    learning_rate    | 0.001    |
|    n_updates        | 1698     |
|    total_actor_loss | -0.325   |
|    value_loss       | 0.0424   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 144      |
| time/               |          |
|    episodes         | 8        |
|    fps              | 5        |
|    time_elapsed     | 2069     |
|    total timesteps  | 12008    |
| train/              |          |
|    actor_loss       | -0.484   |
|    critic_loss      | 0.172    |
|    learning_rate    | 0.001    |
|    n_updates        | 2300     |
|    total_actor_loss | -0.349   |
|    value_loss       | 0.134    |
----------------------------------
Eval num_timesteps=18000, episode_reward=148.80 +/- 75.55
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 149      |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | -0.492   |
|    critic_loss      | 0.038    |
|    learning_rate    | 0.001    |
|    n_updates        | 3498     |
|    total_actor_loss | -0.446   |
|    value_loss       | 0.0467   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 119      |
| time/               |          |
|    episodes         | 12       |
|    fps              | 5        |
|    time_elapsed     | 3350     |
|    total timesteps  | 18012    |
| train/              |          |
|    actor_loss       | -0.483   |
|    critic_loss      | 0.16     |
|    learning_rate    | 0.001    |
|    n_updates        | 3502     |
|    total_actor_loss | -0.373   |
|    value_loss       | 0.11     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 129      |
| time/               |          |
|    episodes         | 16       |
|    fps              | 5        |
|    time_elapsed     | 4196     |
|    total timesteps  | 24016    |
| train/              |          |
|    actor_loss       | -0.756   |
|    critic_loss      | 0.221    |
|    learning_rate    | 0.001    |
|    n_updates        | 4702     |
|    total_actor_loss | -0.552   |
|    value_loss       | 0.205    |
----------------------------------
Eval num_timesteps=27000, episode_reward=63.19 +/- 12.48
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 63.2     |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | -0.905   |
|    critic_loss      | 0.184    |
|    learning_rate    | 0.001    |
|    n_updates        | 5298     |
|    total_actor_loss | -0.802   |
|    value_loss       | 0.103    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 120      |
| time/               |          |
|    episodes         | 20       |
|    fps              | 5        |
|    time_elapsed     | 5459     |
|    total timesteps  | 30020    |
| train/              |          |
|    actor_loss       | -0.775   |
|    critic_loss      | 0.177    |
|    learning_rate    | 0.001    |
|    n_updates        | 5902     |
|    total_actor_loss | -0.682   |
|    value_loss       | 0.0938   |
----------------------------------
Eval num_timesteps=36000, episode_reward=163.08 +/- 78.00
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 163      |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | -1.01    |
|    critic_loss      | 0.218    |
|    learning_rate    | 0.001    |
|    n_updates        | 7098     |
|    total_actor_loss | -0.799   |
|    value_loss       | 0.212    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 124      |
| time/               |          |
|    episodes         | 24       |
|    fps              | 5        |
|    time_elapsed     | 6724     |
|    total timesteps  | 36024    |
| train/              |          |
|    actor_loss       | -0.962   |
|    critic_loss      | 0.147    |
|    learning_rate    | 0.001    |
|    n_updates        | 7104     |
|    total_actor_loss | -0.924   |
|    value_loss       | 0.0383   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 127      |
| time/               |          |
|    episodes         | 28       |
|    fps              | 5        |
|    time_elapsed     | 7552     |
|    total timesteps  | 42028    |
| train/              |          |
|    actor_loss       | -1.12    |
|    critic_loss      | 0.191    |
|    learning_rate    | 0.001    |
|    n_updates        | 8304     |
|    total_actor_loss | -1.06    |
|    value_loss       | 0.0646   |
----------------------------------
Eval num_timesteps=45000, episode_reward=102.81 +/- 55.98
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 103      |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | -1.19    |
|    critic_loss      | 0.504    |
|    learning_rate    | 0.001    |
|    n_updates        | 8898     |
|    total_actor_loss | -0.798   |
|    value_loss       | 0.393    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 122      |
| time/               |          |
|    episodes         | 32       |
|    fps              | 5        |
|    time_elapsed     | 8819     |
|    total timesteps  | 48032    |
| train/              |          |
|    actor_loss       | -1.38    |
|    critic_loss      | 0.118    |
|    learning_rate    | 0.001    |
|    n_updates        | 9506     |
|    total_actor_loss | -1.25    |
|    value_loss       | 0.132    |
----------------------------------
Eval num_timesteps=54000, episode_reward=92.52 +/- 56.43
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 92.5     |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | -1.32    |
|    critic_loss      | 0.117    |
|    learning_rate    | 0.001    |
|    n_updates        | 10698    |
|    total_actor_loss | -1.2     |
|    value_loss       | 0.121    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 119      |
| time/               |          |
|    episodes         | 36       |
|    fps              | 5        |
|    time_elapsed     | 10092    |
|    total timesteps  | 54036    |
| train/              |          |
|    actor_loss       | -1.21    |
|    critic_loss      | 0.069    |
|    learning_rate    | 0.001    |
|    n_updates        | 10706    |
|    total_actor_loss | -1.17    |
|    value_loss       | 0.0375   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 120      |
| time/               |          |
|    episodes         | 40       |
|    fps              | 5        |
|    time_elapsed     | 10949    |
|    total timesteps  | 60040    |
| train/              |          |
|    actor_loss       | -1.36    |
|    critic_loss      | 0.0897   |
|    learning_rate    | 0.001    |
|    n_updates        | 11906    |
|    total_actor_loss | -1.32    |
|    value_loss       | 0.0366   |
----------------------------------
Eval num_timesteps=63000, episode_reward=129.91 +/- 79.04
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 130      |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    actor_loss       | -1.42    |
|    critic_loss      | 0.0944   |
|    learning_rate    | 0.001    |
|    n_updates        | 12498    |
|    total_actor_loss | -1.3     |
|    value_loss       | 0.122    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 120      |
| time/               |          |
|    episodes         | 44       |
|    fps              | 5        |
|    time_elapsed     | 12223    |
|    total timesteps  | 66044    |
| train/              |          |
|    actor_loss       | -1.46    |
|    critic_loss      | 0.0679   |
|    learning_rate    | 0.001    |
|    n_updates        | 13108    |
|    total_actor_loss | -1.38    |
|    value_loss       | 0.0826   |
----------------------------------
Eval num_timesteps=72000, episode_reward=163.80 +/- 94.29
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 164      |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    actor_loss       | -1.51    |
|    critic_loss      | 0.222    |
|    learning_rate    | 0.001    |
|    n_updates        | 14298    |
|    total_actor_loss | -1.46    |
|    value_loss       | 0.046    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 119      |
| time/               |          |
|    episodes         | 48       |
|    fps              | 5        |
|    time_elapsed     | 13489    |
|    total timesteps  | 72048    |
| train/              |          |
|    actor_loss       | -1.55    |
|    critic_loss      | 0.295    |
|    learning_rate    | 0.001    |
|    n_updates        | 14308    |
|    total_actor_loss | -1.52    |
|    value_loss       | 0.03     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 122      |
| time/               |          |
|    episodes         | 52       |
|    fps              | 5        |
|    time_elapsed     | 14338    |
|    total timesteps  | 78052    |
| train/              |          |
|    actor_loss       | -1.79    |
|    critic_loss      | 0.274    |
|    learning_rate    | 0.001    |
|    n_updates        | 15510    |
|    total_actor_loss | -1.71    |
|    value_loss       | 0.0853   |
----------------------------------
Eval num_timesteps=81000, episode_reward=137.96 +/- 77.37
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 138      |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    actor_loss       | -1.76    |
|    critic_loss      | 0.133    |
|    learning_rate    | 0.001    |
|    n_updates        | 16098    |
|    total_actor_loss | -1.66    |
|    value_loss       | 0.103    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 124      |
| time/               |          |
|    episodes         | 56       |
|    fps              | 5        |
|    time_elapsed     | 15614    |
|    total timesteps  | 84056    |
| train/              |          |
|    actor_loss       | -2       |
|    critic_loss      | 0.23     |
|    learning_rate    | 0.001    |
|    n_updates        | 16710    |
|    total_actor_loss | -1.87    |
|    value_loss       | 0.132    |
----------------------------------
Eval num_timesteps=90000, episode_reward=61.83 +/- 12.06
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 61.8     |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    actor_loss       | -2       |
|    critic_loss      | 0.275    |
|    learning_rate    | 0.001    |
|    n_updates        | 17898    |
|    total_actor_loss | -1.87    |
|    value_loss       | 0.131    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 125      |
| time/               |          |
|    episodes         | 60       |
|    fps              | 5        |
|    time_elapsed     | 16897    |
|    total timesteps  | 90060    |
| train/              |          |
|    actor_loss       | -2.01    |
|    critic_loss      | 0.423    |
|    learning_rate    | 0.001    |
|    n_updates        | 17910    |
|    total_actor_loss | -1.82    |
|    value_loss       | 0.186    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 123      |
| time/               |          |
|    episodes         | 64       |
|    fps              | 5        |
|    time_elapsed     | 17735    |
|    total timesteps  | 96064    |
| train/              |          |
|    actor_loss       | -1.92    |
|    critic_loss      | 0.379    |
|    learning_rate    | 0.001    |
|    n_updates        | 19112    |
|    total_actor_loss | -1.61    |
|    value_loss       | 0.309    |
----------------------------------
Eval num_timesteps=99000, episode_reward=90.49 +/- 53.11
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 90.5     |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    actor_loss       | -2.03    |
|    critic_loss      | 0.422    |
|    learning_rate    | 0.001    |
|    n_updates        | 19698    |
|    total_actor_loss | -1.71    |
|    value_loss       | 0.314    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 126      |
| time/               |          |
|    episodes         | 68       |
|    fps              | 5        |
|    time_elapsed     | 19010    |
|    total timesteps  | 102068   |
| train/              |          |
|    actor_loss       | -2.01    |
|    critic_loss      | 0.332    |
|    learning_rate    | 0.001    |
|    n_updates        | 20312    |
|    total_actor_loss | -1.88    |
|    value_loss       | 0.13     |
----------------------------------
Eval num_timesteps=108000, episode_reward=120.42 +/- 53.86
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 120      |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    actor_loss       | -2.08    |
|    critic_loss      | 0.265    |
|    learning_rate    | 0.001    |
|    n_updates        | 21498    |
|    total_actor_loss | -1.92    |
|    value_loss       | 0.165    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 124      |
| time/               |          |
|    episodes         | 72       |
|    fps              | 5        |
|    time_elapsed     | 20277    |
|    total timesteps  | 108072   |
| train/              |          |
|    actor_loss       | -2.01    |
|    critic_loss      | 0.274    |
|    learning_rate    | 0.001    |
|    n_updates        | 21514    |
|    total_actor_loss | -1.98    |
|    value_loss       | 0.0364   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 123      |
| time/               |          |
|    episodes         | 76       |
|    fps              | 5        |
|    time_elapsed     | 21119    |
|    total timesteps  | 114076   |
| train/              |          |
|    actor_loss       | -1.96    |
|    critic_loss      | 0.0974   |
|    learning_rate    | 0.001    |
|    n_updates        | 22714    |
|    total_actor_loss | -1.85    |
|    value_loss       | 0.102    |
----------------------------------
Eval num_timesteps=117000, episode_reward=102.06 +/- 58.33
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 102      |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    actor_loss       | -1.92    |
|    critic_loss      | 0.0315   |
|    learning_rate    | 0.001    |
|    n_updates        | 23298    |
|    total_actor_loss | -1.89    |
|    value_loss       | 0.0327   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 120      |
| time/               |          |
|    episodes         | 80       |
|    fps              | 5        |
|    time_elapsed     | 22397    |
|    total timesteps  | 120080   |
| train/              |          |
|    actor_loss       | -2.01    |
|    critic_loss      | 0.0864   |
|    learning_rate    | 0.001    |
|    n_updates        | 23914    |
|    total_actor_loss | -1.91    |
|    value_loss       | 0.104    |
----------------------------------
Eval num_timesteps=126000, episode_reward=137.53 +/- 85.70
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 138      |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    actor_loss       | -2.06    |
|    critic_loss      | 0.115    |
|    learning_rate    | 0.001    |
|    n_updates        | 25098    |
|    total_actor_loss | -1.98    |
|    value_loss       | 0.0824   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 121      |
| time/               |          |
|    episodes         | 84       |
|    fps              | 5        |
|    time_elapsed     | 23661    |
|    total timesteps  | 126084   |
| train/              |          |
|    actor_loss       | -1.96    |
|    critic_loss      | 0.0275   |
|    learning_rate    | 0.001    |
|    n_updates        | 25116    |
|    total_actor_loss | -1.94    |
|    value_loss       | 0.0281   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 122      |
| time/               |          |
|    episodes         | 88       |
|    fps              | 5        |
|    time_elapsed     | 24508    |
|    total timesteps  | 132088   |
| train/              |          |
|    actor_loss       | -2.3     |
|    critic_loss      | 0.229    |
|    learning_rate    | 0.001    |
|    n_updates        | 26316    |
|    total_actor_loss | -2.11    |
|    value_loss       | 0.187    |
----------------------------------
Eval num_timesteps=135000, episode_reward=100.50 +/- 76.44
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 101      |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    actor_loss       | -2.57    |
|    critic_loss      | 0.337    |
|    learning_rate    | 0.001    |
|    n_updates        | 26898    |
|    total_actor_loss | -2.18    |
|    value_loss       | 0.392    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 124      |
| time/               |          |
|    episodes         | 92       |
|    fps              | 5        |
|    time_elapsed     | 25791    |
|    total timesteps  | 138092   |
| train/              |          |
|    actor_loss       | -2.2     |
|    critic_loss      | 0.189    |
|    learning_rate    | 0.001    |
|    n_updates        | 27518    |
|    total_actor_loss | -2.1     |
|    value_loss       | 0.104    |
----------------------------------
Eval num_timesteps=144000, episode_reward=94.40 +/- 74.29
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 94.4     |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    actor_loss       | -2.35    |
|    critic_loss      | 0.627    |
|    learning_rate    | 0.001    |
|    n_updates        | 28698    |
|    total_actor_loss | -1.91    |
|    value_loss       | 0.436    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 128      |
| time/               |          |
|    episodes         | 96       |
|    fps              | 5        |
|    time_elapsed     | 27075    |
|    total timesteps  | 144096   |
| train/              |          |
|    actor_loss       | -2.42    |
|    critic_loss      | 0.525    |
|    learning_rate    | 0.001    |
|    n_updates        | 28718    |
|    total_actor_loss | -2.21    |
|    value_loss       | 0.211    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 128      |
| time/               |          |
|    episodes         | 100      |
|    fps              | 5        |
|    time_elapsed     | 27929    |
|    total timesteps  | 150100   |
| train/              |          |
|    actor_loss       | -2.76    |
|    critic_loss      | 0.152    |
|    learning_rate    | 0.001    |
|    n_updates        | 29918    |
|    total_actor_loss | -2.62    |
|    value_loss       | 0.131    |
----------------------------------
Eval num_timesteps=153000, episode_reward=60.18 +/- 8.22
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 60.2     |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    actor_loss       | -2.68    |
|    critic_loss      | 0.167    |
|    learning_rate    | 0.001    |
|    n_updates        | 30498    |
|    total_actor_loss | -2.61    |
|    value_loss       | 0.0708   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 131      |
| time/               |          |
|    episodes         | 104      |
|    fps              | 5        |
|    time_elapsed     | 29211    |
|    total timesteps  | 156104   |
| train/              |          |
|    actor_loss       | -2.46    |
|    critic_loss      | 0.279    |
|    learning_rate    | 0.001    |
|    n_updates        | 31120    |
|    total_actor_loss | -2.21    |
|    value_loss       | 0.257    |
----------------------------------
Eval num_timesteps=162000, episode_reward=147.01 +/- 102.89
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 147      |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    actor_loss       | -2.55    |
|    critic_loss      | 0.213    |
|    learning_rate    | 0.001    |
|    n_updates        | 32298    |
|    total_actor_loss | -2.44    |
|    value_loss       | 0.116    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 132      |
| time/               |          |
|    episodes         | 108      |
|    fps              | 5        |
|    time_elapsed     | 30499    |
|    total timesteps  | 162108   |
| train/              |          |
|    actor_loss       | -2.64    |
|    critic_loss      | 0.312    |
|    learning_rate    | 0.001    |
|    n_updates        | 32320    |
|    total_actor_loss | -2.44    |
|    value_loss       | 0.2      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 134      |
| time/               |          |
|    episodes         | 112      |
|    fps              | 5        |
|    time_elapsed     | 31362    |
|    total timesteps  | 168112   |
| train/              |          |
|    actor_loss       | -2.96    |
|    critic_loss      | 0.33     |
|    learning_rate    | 0.001    |
|    n_updates        | 33522    |
|    total_actor_loss | -2.75    |
|    value_loss       | 0.214    |
----------------------------------
Eval num_timesteps=171000, episode_reward=193.33 +/- 95.75
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 193      |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    actor_loss       | -2.8     |
|    critic_loss      | 0.177    |
|    learning_rate    | 0.001    |
|    n_updates        | 34098    |
|    total_actor_loss | -2.66    |
|    value_loss       | 0.146    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 134      |
| time/               |          |
|    episodes         | 116      |
|    fps              | 5        |
|    time_elapsed     | 32655    |
|    total timesteps  | 174116   |
| train/              |          |
|    actor_loss       | -2.89    |
|    critic_loss      | 0.133    |
|    learning_rate    | 0.001    |
|    n_updates        | 34722    |
|    total_actor_loss | -2.82    |
|    value_loss       | 0.0715   |
----------------------------------
Eval num_timesteps=180000, episode_reward=74.80 +/- 23.32
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 74.8     |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    actor_loss       | -2.94    |
|    critic_loss      | 0.204    |
|    learning_rate    | 0.001    |
|    n_updates        | 35898    |
|    total_actor_loss | -2.87    |
|    value_loss       | 0.0733   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 135      |
| time/               |          |
|    episodes         | 120      |
|    fps              | 5        |
|    time_elapsed     | 34010    |
|    total timesteps  | 180120   |
| train/              |          |
|    actor_loss       | -2.69    |
|    critic_loss      | 0.264    |
|    learning_rate    | 0.001    |
|    n_updates        | 35922    |
|    total_actor_loss | -2.63    |
|    value_loss       | 0.0568   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | 132      |
| time/               |          |
|    episodes         | 124      |
|    fps              | 5        |
|    time_elapsed     | 34867    |
|    total timesteps  | 186124   |
| train/              |          |
|    actor_loss       | -2.73    |
|    critic_loss      | 0.127    |
|    learning_rate    | 0.001    |
|    n_updates        | 37124    |
|    total_actor_loss | -2.62    |
|    value_loss       | 0.113    |
----------------------------------
Eval num_timesteps=189000, episode_reward=123.02 +/- 86.69
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 123      |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    actor_loss       | -2.95    |
|    critic_loss      | 0.234    |
|    learning_rate    | 0.001    |
|    n_updates        | 37698    |
|    total_actor_loss | -2.88    |
|    value_loss       | 0.0672   |
----------------------------------
