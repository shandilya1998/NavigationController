2021-12-04 23:41:51.447763: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-04 23:41:51.447830: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp8/TD3_9
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 757      |
|    ep_rew_mean     | 230      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 54       |
|    time_elapsed    | 55       |
|    total timesteps | 3027     |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 975      |
|    ep_rew_mean      | 200      |
| time/               |          |
|    episodes         | 8        |
|    fps              | 22       |
|    time_elapsed     | 342      |
|    total timesteps  | 7803     |
| train/              |          |
|    actor_loss       | -0.187   |
|    critic_loss      | 0.442    |
|    learning_rate    | 0.001    |
|    n_updates        | 360      |
|    total_actor_loss | 0.105    |
|    value_loss       | 0.292    |
----------------------------------
Eval num_timesteps=9000, episode_reward=21.46 +/- 49.41
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 21.5     |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | -0.474   |
|    critic_loss      | 0.13     |
|    learning_rate    | 0.001    |
|    n_updates        | 598      |
|    total_actor_loss | -0.429   |
|    value_loss       | 0.0445   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.15e+03 |
|    ep_rew_mean      | 141      |
| time/               |          |
|    episodes         | 12       |
|    fps              | 9        |
|    time_elapsed     | 1502     |
|    total timesteps  | 13807    |
| train/              |          |
|    actor_loss       | -0.307   |
|    critic_loss      | 0.159    |
|    learning_rate    | 0.001    |
|    n_updates        | 1560     |
|    total_actor_loss | -0.173   |
|    value_loss       | 0.133    |
----------------------------------
Eval num_timesteps=18000, episode_reward=-3.56 +/- 33.92
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -3.56    |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | -0.439   |
|    critic_loss      | 0.423    |
|    learning_rate    | 0.001    |
|    n_updates        | 2398     |
|    total_actor_loss | -0.242   |
|    value_loss       | 0.197    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.24e+03 |
|    ep_rew_mean      | 106      |
| time/               |          |
|    episodes         | 16       |
|    fps              | 7        |
|    time_elapsed     | 2664     |
|    total timesteps  | 19811    |
| train/              |          |
|    actor_loss       | -0.565   |
|    critic_loss      | 0.256    |
|    learning_rate    | 0.001    |
|    n_updates        | 2762     |
|    total_actor_loss | -0.364   |
|    value_loss       | 0.201    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.29e+03 |
|    ep_rew_mean      | 91.4     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 7        |
|    time_elapsed     | 3446     |
|    total timesteps  | 25815    |
| train/              |          |
|    actor_loss       | -1.03    |
|    critic_loss      | 0.961    |
|    learning_rate    | 0.001    |
|    n_updates        | 3962     |
|    total_actor_loss | -0.599   |
|    value_loss       | 0.436    |
----------------------------------
Eval num_timesteps=27000, episode_reward=18.22 +/- 63.14
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 18.2     |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | -0.705   |
|    critic_loss      | 1.69     |
|    learning_rate    | 0.001    |
|    n_updates        | 4198     |
|    total_actor_loss | 0.77     |
|    value_loss       | 1.47     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.33e+03 |
|    ep_rew_mean      | 76       |
| time/               |          |
|    episodes         | 24       |
|    fps              | 6        |
|    time_elapsed     | 4618     |
|    total timesteps  | 31819    |
| train/              |          |
|    actor_loss       | -0.619   |
|    critic_loss      | 0.316    |
|    learning_rate    | 0.001    |
|    n_updates        | 5162     |
|    total_actor_loss | -0.386   |
|    value_loss       | 0.233    |
----------------------------------
Eval num_timesteps=36000, episode_reward=-17.87 +/- 50.62
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -17.9    |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | -0.567   |
|    critic_loss      | 0.539    |
|    learning_rate    | 0.001    |
|    n_updates        | 5998     |
|    total_actor_loss | -0.402   |
|    value_loss       | 0.165    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.35e+03 |
|    ep_rew_mean      | 68.7     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 6        |
|    time_elapsed     | 5792     |
|    total timesteps  | 37823    |
| train/              |          |
|    actor_loss       | -0.418   |
|    critic_loss      | 0.549    |
|    learning_rate    | 0.001    |
|    n_updates        | 6364     |
|    total_actor_loss | -0.301   |
|    value_loss       | 0.117    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.37e+03 |
|    ep_rew_mean      | 55.7     |
| time/               |          |
|    episodes         | 32       |
|    fps              | 6        |
|    time_elapsed     | 6573     |
|    total timesteps  | 43827    |
| train/              |          |
|    actor_loss       | -0.461   |
|    critic_loss      | 0.685    |
|    learning_rate    | 0.001    |
|    n_updates        | 7564     |
|    total_actor_loss | -0.199   |
|    value_loss       | 0.262    |
----------------------------------
Eval num_timesteps=45000, episode_reward=-7.16 +/- 49.22
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -7.16    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | -0.704   |
|    critic_loss      | 0.536    |
|    learning_rate    | 0.001    |
|    n_updates        | 7798     |
|    total_actor_loss | 0.152    |
|    value_loss       | 0.857    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.38e+03 |
|    ep_rew_mean      | 56.7     |
| time/               |          |
|    episodes         | 36       |
|    fps              | 6        |
|    time_elapsed     | 7723     |
|    total timesteps  | 49831    |
| train/              |          |
|    actor_loss       | -0.857   |
|    critic_loss      | 1.94     |
|    learning_rate    | 0.001    |
|    n_updates        | 8766     |
|    total_actor_loss | 0.0597   |
|    value_loss       | 0.917    |
----------------------------------
Eval num_timesteps=54000, episode_reward=-29.28 +/- 18.67
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -29.3    |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | -0.981   |
|    critic_loss      | 2.36     |
|    learning_rate    | 0.001    |
|    n_updates        | 9598     |
|    total_actor_loss | 0.863    |
|    value_loss       | 1.84     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.4e+03  |
|    ep_rew_mean      | 49.1     |
| time/               |          |
|    episodes         | 40       |
|    fps              | 6        |
|    time_elapsed     | 8878     |
|    total timesteps  | 55835    |
| train/              |          |
|    actor_loss       | -0.749   |
|    critic_loss      | 0.928    |
|    learning_rate    | 0.001    |
|    n_updates        | 9966     |
|    total_actor_loss | 0.179    |
|    value_loss       | 0.927    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.41e+03 |
|    ep_rew_mean      | 45.3     |
| time/               |          |
|    episodes         | 44       |
|    fps              | 6        |
|    time_elapsed     | 9659     |
|    total timesteps  | 61839    |
| train/              |          |
|    actor_loss       | -0.515   |
|    critic_loss      | 2.85     |
|    learning_rate    | 0.001    |
|    n_updates        | 11166    |
|    total_actor_loss | -0.331   |
|    value_loss       | 0.184    |
----------------------------------
Eval num_timesteps=63000, episode_reward=98.22 +/- 109.10
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 98.2     |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    actor_loss       | -0.73    |
|    critic_loss      | 0.442    |
|    learning_rate    | 0.001    |
|    n_updates        | 11398    |
|    total_actor_loss | -0.56    |
|    value_loss       | 0.17     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.41e+03 |
|    ep_rew_mean      | 55.9     |
| time/               |          |
|    episodes         | 48       |
|    fps              | 6        |
|    time_elapsed     | 10808    |
|    total timesteps  | 67843    |
| train/              |          |
|    actor_loss       | -0.788   |
|    critic_loss      | 1.22     |
|    learning_rate    | 0.001    |
|    n_updates        | 12368    |
|    total_actor_loss | 0.108    |
|    value_loss       | 0.896    |
----------------------------------
[W python_anomaly_mode.cpp:104] Warning: Error detected in ExpBackward. Traceback of forward call that caused the error:
  File "train.py", line 39, in <module>
    model.learn(args.timesteps)
  File "/root/trainer/learning/explore.py", line 78, in learn
    callback = self.rl_callback
  File "/root/trainer/utils/td3_utils.py", line 713, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 369, in learn
    self.train(batch_size=self.batch_size, gradient_steps=gradient_steps)
  File "/root/trainer/utils/td3_utils.py", line 667, in train
    actions, vt = self.actor(replay_data.observations)
  File "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/trainer/utils/td3_utils.py", line 94, in forward
    return self.mu(features)
  File "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/trainer/bg/models.py", line 227, in forward
    bg_out, vt  = self.bg([stimulus_t, stimulus_t_1])
  File "/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/trainer/bg/models.py", line 87, in forward
    lamd2 = 1 / (1 + torch.exp(self.log_a2.exp() * (deltavf - self.thetad2)))
 (function _print_stack)
Traceback (most recent call last):
  File "train.py", line 39, in <module>
    model.learn(args.timesteps)
  File "/root/trainer/learning/explore.py", line 78, in learn
    callback = self.rl_callback
  File "/root/trainer/utils/td3_utils.py", line 713, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/usr/local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 369, in learn
    self.train(batch_size=self.batch_size, gradient_steps=gradient_steps)
  File "/root/trainer/utils/td3_utils.py", line 676, in train
    loss.backward()
  File "/usr/local/lib/python3.6/site-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 149, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
RuntimeError: Function 'ExpBackward' returned nan values in its 0th output.
running build_ext
building 'mujoco_py.cymj' extension
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o -fopenmp -w
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py
gcc -pthread -shared /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mjpro150/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mjpro150/bin -lmujoco150 -lglewegl -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp
2021-12-05 11:20:19.620462: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-05 11:20:19.620531: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp8/TD3_10
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 807      |
|    ep_rew_mean     | 117      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 52       |
|    time_elapsed    | 61       |
|    total timesteps | 3228     |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 870      |
|    ep_rew_mean      | 90.3     |
| time/               |          |
|    episodes         | 8        |
|    fps              | 29       |
|    time_elapsed     | 236      |
|    total timesteps  | 6960     |
| train/              |          |
|    actor_loss       | -0.116   |
|    critic_loss      | 0.158    |
|    learning_rate    | 0.001    |
|    n_updates        | 190      |
|    total_actor_loss | -0.0312  |
|    value_loss       | 0.085    |
----------------------------------
Eval num_timesteps=9000, episode_reward=51.18 +/- 16.54
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 51.2     |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    actor_loss       | -0.181   |
|    critic_loss      | 0.0887   |
|    learning_rate    | 0.001    |
|    n_updates        | 598      |
|    total_actor_loss | -0.151   |
|    value_loss       | 0.0295   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.08e+03 |
|    ep_rew_mean      | 87.3     |
| time/               |          |
|    episodes         | 12       |
|    fps              | 8        |
|    time_elapsed     | 1448     |
|    total timesteps  | 12964    |
| train/              |          |
|    actor_loss       | -0.173   |
|    critic_loss      | 0.0769   |
|    learning_rate    | 0.001    |
|    n_updates        | 1392     |
|    total_actor_loss | -0.131   |
|    value_loss       | 0.0426   |
----------------------------------
Eval num_timesteps=18000, episode_reward=55.99 +/- 30.63
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 56       |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    actor_loss       | -0.264   |
|    critic_loss      | 0.0816   |
|    learning_rate    | 0.001    |
|    n_updates        | 2398     |
|    total_actor_loss | -0.221   |
|    value_loss       | 0.0431   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.19e+03 |
|    ep_rew_mean      | 76.3     |
| time/               |          |
|    episodes         | 16       |
|    fps              | 6        |
|    time_elapsed     | 2709     |
|    total timesteps  | 18968    |
| train/              |          |
|    actor_loss       | -0.377   |
|    critic_loss      | 0.243    |
|    learning_rate    | 0.001    |
|    n_updates        | 2592     |
|    total_actor_loss | -0.324   |
|    value_loss       | 0.0528   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.25e+03 |
|    ep_rew_mean      | 73.7     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 7        |
|    time_elapsed     | 3513     |
|    total timesteps  | 24972    |
| train/              |          |
|    actor_loss       | -0.448   |
|    critic_loss      | 0.0582   |
|    learning_rate    | 0.001    |
|    n_updates        | 3794     |
|    total_actor_loss | -0.339   |
|    value_loss       | 0.109    |
----------------------------------
Eval num_timesteps=27000, episode_reward=49.29 +/- 36.51
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 49.3     |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    actor_loss       | -0.393   |
|    critic_loss      | 0.0821   |
|    learning_rate    | 0.001    |
|    n_updates        | 4198     |
|    total_actor_loss | -0.383   |
|    value_loss       | 0.0103   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.29e+03 |
|    ep_rew_mean      | 70.2     |
| time/               |          |
|    episodes         | 24       |
|    fps              | 6        |
|    time_elapsed     | 4726     |
|    total timesteps  | 30976    |
| train/              |          |
|    actor_loss       | -0.589   |
|    critic_loss      | 1.77     |
|    learning_rate    | 0.001    |
|    n_updates        | 4994     |
|    total_actor_loss | -0.481   |
|    value_loss       | 0.108    |
----------------------------------
Eval num_timesteps=36000, episode_reward=47.09 +/- 11.13
Episode length: 1401.00 +/- 200.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.4e+03  |
|    mean_reward      | 47.1     |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | -0.622   |
|    critic_loss      | 0.157    |
|    learning_rate    | 0.001    |
|    n_updates        | 5998     |
|    total_actor_loss | -0.494   |
|    value_loss       | 0.128    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.32e+03 |
|    ep_rew_mean      | 66.9     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 6        |
|    time_elapsed     | 5922     |
|    total timesteps  | 36980    |
| train/              |          |
|    actor_loss       | -0.634   |
|    critic_loss      | 0.148    |
|    learning_rate    | 0.001    |
|    n_updates        | 6194     |
|    total_actor_loss | -0.323   |
|    value_loss       | 0.311    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.34e+03 |
|    ep_rew_mean      | 72.1     |
| time/               |          |
|    episodes         | 32       |
|    fps              | 6        |
|    time_elapsed     | 6734     |
|    total timesteps  | 42984    |
| train/              |          |
|    actor_loss       | -0.541   |
|    critic_loss      | 1.84     |
|    learning_rate    | 0.001    |
|    n_updates        | 7396     |
|    total_actor_loss | 1.31     |
|    value_loss       | 1.86     |
----------------------------------
Eval num_timesteps=45000, episode_reward=138.85 +/- 113.32
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 139      |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    actor_loss       | -0.703   |
|    critic_loss      | 0.257    |
|    learning_rate    | 0.001    |
|    n_updates        | 7798     |
|    total_actor_loss | -0.532   |
|    value_loss       | 0.171    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.36e+03 |
|    ep_rew_mean      | 84.7     |
| time/               |          |
|    episodes         | 36       |
|    fps              | 6        |
|    time_elapsed     | 7953     |
|    total timesteps  | 48988    |
| train/              |          |
|    actor_loss       | -0.797   |
|    critic_loss      | 0.27     |
|    learning_rate    | 0.001    |
|    n_updates        | 8596     |
|    total_actor_loss | -0.385   |
|    value_loss       | 0.411    |
----------------------------------
Eval num_timesteps=54000, episode_reward=131.85 +/- 84.16
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 132      |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | -0.811   |
|    critic_loss      | 0.265    |
|    learning_rate    | 0.001    |
|    n_updates        | 9598     |
|    total_actor_loss | -0.675   |
|    value_loss       | 0.136    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.37e+03 |
|    ep_rew_mean      | 81.6     |
| time/               |          |
|    episodes         | 40       |
|    fps              | 5        |
|    time_elapsed     | 9166     |
|    total timesteps  | 54992    |
| train/              |          |
|    actor_loss       | -0.987   |
|    critic_loss      | 0.189    |
|    learning_rate    | 0.001    |
|    n_updates        | 9798     |
|    total_actor_loss | -0.877   |
|    value_loss       | 0.111    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.38e+03 |
|    ep_rew_mean      | 87.1     |
| time/               |          |
|    episodes         | 44       |
|    fps              | 6        |
|    time_elapsed     | 9906     |
|    total timesteps  | 60537    |
| train/              |          |
|    actor_loss       | -1.18    |
|    critic_loss      | 0.23     |
|    learning_rate    | 0.001    |
|    n_updates        | 10906    |
|    total_actor_loss | -1.03    |
|    value_loss       | 0.155    |
----------------------------------
Eval num_timesteps=63000, episode_reward=58.41 +/- 38.05
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 58.4     |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    actor_loss       | -1.06    |
|    critic_loss      | 0.327    |
|    learning_rate    | 0.001    |
|    n_updates        | 11398    |
|    total_actor_loss | -1.02    |
|    value_loss       | 0.0408   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.39e+03 |
|    ep_rew_mean      | 93.4     |
| time/               |          |
|    episodes         | 48       |
|    fps              | 5        |
|    time_elapsed     | 11124    |
|    total timesteps  | 66541    |
| train/              |          |
|    actor_loss       | -1.13    |
|    critic_loss      | 0.112    |
|    learning_rate    | 0.001    |
|    n_updates        | 12108    |
|    total_actor_loss | -0.941   |
|    value_loss       | 0.187    |
----------------------------------
Eval num_timesteps=72000, episode_reward=74.79 +/- 55.98
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 74.8     |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    actor_loss       | -1.3     |
|    critic_loss      | 2.06     |
|    learning_rate    | 0.001    |
|    n_updates        | 13198    |
|    total_actor_loss | 0.804    |
|    value_loss       | 2.1      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.4e+03  |
|    ep_rew_mean      | 91.8     |
| time/               |          |
|    episodes         | 52       |
|    fps              | 5        |
|    time_elapsed     | 12334    |
|    total timesteps  | 72545    |
| train/              |          |
|    actor_loss       | -1.49    |
|    critic_loss      | 0.479    |
|    learning_rate    | 0.001    |
|    n_updates        | 13308    |
|    total_actor_loss | -1.21    |
|    value_loss       | 0.283    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.4e+03  |
|    ep_rew_mean      | 96.7     |
| time/               |          |
|    episodes         | 56       |
|    fps              | 5        |
|    time_elapsed     | 13145    |
|    total timesteps  | 78549    |
| train/              |          |
|    actor_loss       | -1.42    |
|    critic_loss      | 0.228    |
|    learning_rate    | 0.001    |
|    n_updates        | 14508    |
|    total_actor_loss | -1.19    |
|    value_loss       | 0.225    |
----------------------------------
Eval num_timesteps=81000, episode_reward=123.66 +/- 111.74
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 124      |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    actor_loss       | -1.53    |
|    critic_loss      | 0.28     |
|    learning_rate    | 0.001    |
|    n_updates        | 14998    |
|    total_actor_loss | -1.41    |
|    value_loss       | 0.123    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.41e+03 |
|    ep_rew_mean      | 102      |
| time/               |          |
|    episodes         | 60       |
|    fps              | 5        |
|    time_elapsed     | 14372    |
|    total timesteps  | 84553    |
| train/              |          |
|    actor_loss       | -1.69    |
|    critic_loss      | 0.359    |
|    learning_rate    | 0.001    |
|    n_updates        | 15710    |
|    total_actor_loss | -1.36    |
|    value_loss       | 0.33     |
----------------------------------
Eval num_timesteps=90000, episode_reward=108.93 +/- 97.67
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 109      |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    actor_loss       | -1.73    |
|    critic_loss      | 0.344    |
|    learning_rate    | 0.001    |
|    n_updates        | 16798    |
|    total_actor_loss | -1.62    |
|    value_loss       | 0.116    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.41e+03 |
|    ep_rew_mean      | 102      |
| time/               |          |
|    episodes         | 64       |
|    fps              | 5        |
|    time_elapsed     | 15574    |
|    total timesteps  | 90557    |
| train/              |          |
|    actor_loss       | -2.09    |
|    critic_loss      | 0.461    |
|    learning_rate    | 0.001    |
|    n_updates        | 16910    |
|    total_actor_loss | -1.68    |
|    value_loss       | 0.408    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.42e+03 |
|    ep_rew_mean      | 102      |
| time/               |          |
|    episodes         | 68       |
|    fps              | 5        |
|    time_elapsed     | 16385    |
|    total timesteps  | 96561    |
| train/              |          |
|    actor_loss       | -2.16    |
|    critic_loss      | 2.43     |
|    learning_rate    | 0.001    |
|    n_updates        | 18112    |
|    total_actor_loss | -1.9     |
|    value_loss       | 0.258    |
----------------------------------
Eval num_timesteps=99000, episode_reward=41.69 +/- 10.15
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 41.7     |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    actor_loss       | -2.29    |
|    critic_loss      | 0.287    |
|    learning_rate    | 0.001    |
|    n_updates        | 18598    |
|    total_actor_loss | -2.13    |
|    value_loss       | 0.16     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.42e+03 |
|    ep_rew_mean      | 102      |
| time/               |          |
|    episodes         | 72       |
|    fps              | 5        |
|    time_elapsed     | 17589    |
|    total timesteps  | 102565   |
| train/              |          |
|    actor_loss       | -2.46    |
|    critic_loss      | 0.414    |
|    learning_rate    | 0.001    |
|    n_updates        | 19312    |
|    total_actor_loss | -2.28    |
|    value_loss       | 0.179    |
----------------------------------
Eval num_timesteps=108000, episode_reward=125.64 +/- 92.92
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 126      |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    actor_loss       | -2.44    |
|    critic_loss      | 0.643    |
|    learning_rate    | 0.001    |
|    n_updates        | 20398    |
|    total_actor_loss | -2.19    |
|    value_loss       | 0.247    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.43e+03 |
|    ep_rew_mean      | 99.2     |
| time/               |          |
|    episodes         | 76       |
|    fps              | 5        |
|    time_elapsed     | 18794    |
|    total timesteps  | 108569   |
| train/              |          |
|    actor_loss       | -2.54    |
|    critic_loss      | 0.792    |
|    learning_rate    | 0.001    |
|    n_updates        | 20512    |
|    total_actor_loss | -2.21    |
|    value_loss       | 0.336    |
----------------------------------
Killed
running build_ext
building 'mujoco_py.cymj' extension
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o -fopenmp -w
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py
gcc -pthread -shared /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mjpro150/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mjpro150/bin -lmujoco150 -lglewegl -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp
2021-12-05 20:34:40.874983: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-05 20:34:40.875054: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp8/TD3_11
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 986      |
|    ep_rew_mean     | 133      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 54       |
|    time_elapsed    | 72       |
|    total timesteps | 3944     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 711      |
|    ep_rew_mean     | 108      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 54       |
|    time_elapsed    | 104      |
|    total timesteps | 5687     |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=9000, episode_reward=68.12 +/- 12.63
Episode length: 1339.60 +/- 322.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.34e+03 |
|    mean_reward     | 68.1     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 818      |
|    ep_rew_mean     | 104      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 18       |
|    time_elapsed    | 527      |
|    total timesteps | 9814     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 808      |
|    ep_rew_mean     | 114      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 22       |
|    time_elapsed    | 584      |
|    total timesteps | 12926    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 722      |
|    ep_rew_mean     | 109      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 23       |
|    time_elapsed    | 612      |
|    total timesteps | 14439    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 739      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 26       |
|    time_elapsed    | 671      |
|    total timesteps | 17743    |
---------------------------------
running build_ext
building 'mujoco_py.cymj' extension
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o -fopenmp -w
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/lib/python3.6/site-packages/mujoco_py -I/root/.mujoco/mjpro150/include -I/usr/local/lib/python3.6/site-packages/numpy/core/include -I/usr/local/lib/python3.6/site-packages/mujoco_py/vendor/egl -I/usr/local/include/python3.6m -c /usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.c -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -fopenmp -w
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6
creating /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py
gcc -pthread -shared /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/cymj.o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/temp.linux-x86_64-3.6/usr/local/lib/python3.6/site-packages/mujoco_py/gl/eglshim.o -L/root/.mujoco/mjpro150/bin -Wl,--enable-new-dtags,-R/root/.mujoco/mjpro150/bin -lmujoco150 -lglewegl -o /usr/local/lib/python3.6/site-packages/mujoco_py/generated/_pyxbld_1.50.1.68_36_linuxgpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp
2021-12-05 20:58:55.676500: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-05 20:58:55.676567: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp8/TD3_12
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 382      |
|    ep_rew_mean     | 77.5     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 48       |
|    time_elapsed    | 31       |
|    total timesteps | 1527     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | 91       |
| time/              |          |
|    episodes        | 8        |
|    fps             | 53       |
|    time_elapsed    | 77       |
|    total timesteps | 4130     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 818      |
|    ep_rew_mean     | 99.8     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 56       |
|    time_elapsed    | 174      |
|    total timesteps | 9822     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 806      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 56       |
|    time_elapsed    | 226      |
|    total timesteps | 12891    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 766      |
|    ep_rew_mean     | 107      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 57       |
|    time_elapsed    | 268      |
|    total timesteps | 15330    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 740      |
|    ep_rew_mean     | 107      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 57       |
|    time_elapsed    | 310      |
|    total timesteps | 17757    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=18000, episode_reward=61.61 +/- 26.02
Episode length: 1501.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.5e+03  |
|    mean_reward     | 61.6     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
Terminated
2021-12-05 21:13:57.103633: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-05 21:13:57.103699: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp8/TD3_13
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 904      |
|    ep_rew_mean     | 60.1     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 54       |
|    time_elapsed    | 65       |
|    total timesteps | 3615     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 833      |
|    ep_rew_mean     | 123      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 55       |
|    time_elapsed    | 119      |
|    total timesteps | 6667     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 752      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 55       |
|    time_elapsed    | 162      |
|    total timesteps | 9021     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 826      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 56       |
|    time_elapsed    | 235      |
|    total timesteps | 13209    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 757      |
|    ep_rew_mean     | 164      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 56       |
|    time_elapsed    | 269      |
|    total timesteps | 15137    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 732      |
|    ep_rew_mean     | 159      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 56       |
|    time_elapsed    | 311      |
|    total timesteps | 17565    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=18000, episode_reward=52.36 +/- 103.65
Episode length: 1328.60 +/- 344.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.33e+03 |
|    mean_reward     | 52.4     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
Terminated
2021-12-05 21:34:16.173460: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-05 21:34:16.173532: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Logging to assets/out/models/exp8/TD3_14
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.2e+03  |
|    ep_rew_mean     | 170      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 57       |
|    time_elapsed    | 83       |
|    total timesteps | 4787     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 783      |
|    ep_rew_mean     | 205      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 57       |
|    time_elapsed    | 109      |
|    total timesteps | 6262     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 668      |
|    ep_rew_mean     | 206      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 57       |
|    time_elapsed    | 140      |
|    total timesteps | 8019     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 750      |
|    ep_rew_mean     | 204      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 57       |
|    time_elapsed    | 207      |
|    total timesteps | 12001    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 682      |
|    ep_rew_mean     | 215      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 57       |
|    time_elapsed    | 236      |
|    total timesteps | 13637    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 748      |
|    ep_rew_mean     | 212      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 57       |
|    time_elapsed    | 310      |
|    total timesteps | 17958    |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=18000, episode_reward=-79.72 +/- 121.05
Episode length: 1501.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.5e+03  |
|    mean_reward     | -79.7    |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 856      |
|    ep_rew_mean      | 178      |
| time/               |          |
|    episodes         | 28       |
|    fps              | 16       |
|    time_elapsed     | 1477     |
|    total timesteps  | 23962    |
| train/              |          |
|    actor_loss       | -0.425   |
|    critic_loss      | 0.919    |
|    learning_rate    | 0.001    |
|    n_updates        | 1192     |
|    total_actor_loss | 0.113    |
|    value_loss       | 0.538    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 936      |
|    ep_rew_mean      | 145      |
| time/               |          |
|    episodes         | 32       |
|    fps              | 13       |
|    time_elapsed     | 2275     |
|    total timesteps  | 29966    |
| train/              |          |
|    actor_loss       | -0.97    |
|    critic_loss      | 0.588    |
|    learning_rate    | 0.001    |
|    n_updates        | 2392     |
|    total_actor_loss | -0.591   |
|    value_loss       | 0.379    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 999      |
|    ep_rew_mean      | 112      |
| time/               |          |
|    episodes         | 36       |
|    fps              | 11       |
|    time_elapsed     | 3078     |
|    total timesteps  | 35970    |
| train/              |          |
|    actor_loss       | -1.21    |
|    critic_loss      | 0.566    |
|    learning_rate    | 0.001    |
|    n_updates        | 3592     |
|    total_actor_loss | -0.933   |
|    value_loss       | 0.275    |
----------------------------------
Eval num_timesteps=36000, episode_reward=-86.97 +/- 91.09
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -87      |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    actor_loss       | -1.14    |
|    critic_loss      | 0.996    |
|    learning_rate    | 0.001    |
|    n_updates        | 3598     |
|    total_actor_loss | -0.791   |
|    value_loss       | 0.353    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.05e+03 |
|    ep_rew_mean      | 80.4     |
| time/               |          |
|    episodes         | 40       |
|    fps              | 9        |
|    time_elapsed     | 4296     |
|    total timesteps  | 41974    |
| train/              |          |
|    actor_loss       | -1.29    |
|    critic_loss      | 1.61     |
|    learning_rate    | 0.001    |
|    n_updates        | 4794     |
|    total_actor_loss | 0.227    |
|    value_loss       | 1.52     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.09e+03 |
|    ep_rew_mean      | 66.8     |
| time/               |          |
|    episodes         | 44       |
|    fps              | 9        |
|    time_elapsed     | 5107     |
|    total timesteps  | 47978    |
| train/              |          |
|    actor_loss       | -1.3     |
|    critic_loss      | 1.26     |
|    learning_rate    | 0.001    |
|    n_updates        | 5994     |
|    total_actor_loss | -0.647   |
|    value_loss       | 0.658    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.12e+03 |
|    ep_rew_mean      | 54.5     |
| time/               |          |
|    episodes         | 48       |
|    fps              | 9        |
|    time_elapsed     | 5900     |
|    total timesteps  | 53982    |
| train/              |          |
|    actor_loss       | -1.48    |
|    critic_loss      | 0.464    |
|    learning_rate    | 0.001    |
|    n_updates        | 7196     |
|    total_actor_loss | -1.23    |
|    value_loss       | 0.242    |
----------------------------------
Eval num_timesteps=54000, episode_reward=-80.97 +/- 77.47
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -81      |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    actor_loss       | -1.47    |
|    critic_loss      | 0.526    |
|    learning_rate    | 0.001    |
|    n_updates        | 7198     |
|    total_actor_loss | -1.19    |
|    value_loss       | 0.285    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.15e+03 |
|    ep_rew_mean      | 36.1     |
| time/               |          |
|    episodes         | 52       |
|    fps              | 8        |
|    time_elapsed     | 7109     |
|    total timesteps  | 59986    |
| train/              |          |
|    actor_loss       | -1.25    |
|    critic_loss      | 0.339    |
|    learning_rate    | 0.001    |
|    n_updates        | 8396     |
|    total_actor_loss | -1.02    |
|    value_loss       | 0.228    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.18e+03 |
|    ep_rew_mean      | 26.5     |
| time/               |          |
|    episodes         | 56       |
|    fps              | 8        |
|    time_elapsed     | 7902     |
|    total timesteps  | 65990    |
| train/              |          |
|    actor_loss       | -0.815   |
|    critic_loss      | 0.877    |
|    learning_rate    | 0.001    |
|    n_updates        | 9596     |
|    total_actor_loss | -0.545   |
|    value_loss       | 0.27     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.2e+03  |
|    ep_rew_mean      | 15.4     |
| time/               |          |
|    episodes         | 60       |
|    fps              | 8        |
|    time_elapsed     | 8700     |
|    total timesteps  | 71994    |
| train/              |          |
|    actor_loss       | -0.758   |
|    critic_loss      | 0.516    |
|    learning_rate    | 0.001    |
|    n_updates        | 10798    |
|    total_actor_loss | -0.593   |
|    value_loss       | 0.165    |
----------------------------------
Eval num_timesteps=72000, episode_reward=-141.04 +/- 60.60
Episode length: 1501.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.5e+03  |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 72000    |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.22e+03 |
|    ep_rew_mean      | 10.5     |
| time/               |          |
|    episodes         | 64       |
|    fps              | 7        |
|    time_elapsed     | 9923     |
|    total timesteps  | 77998    |
| train/              |          |
|    actor_loss       | -0.703   |
|    critic_loss      | 0.336    |
|    learning_rate    | 0.001    |
|    n_updates        | 11998    |
|    total_actor_loss | -0.456   |
|    value_loss       | 0.247    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.24e+03 |
|    ep_rew_mean      | 1.91     |
| time/               |          |
|    episodes         | 68       |
|    fps              | 7        |
|    time_elapsed     | 10713    |
|    total timesteps  | 84002    |
| train/              |          |
|    actor_loss       | -0.456   |
|    critic_loss      | 156      |
|    learning_rate    | 0.001    |
|    n_updates        | 13200    |
|    total_actor_loss | 153      |
|    value_loss       | 153      |
----------------------------------
Eval num_timesteps=90000, episode_reward=-3.57 +/- 158.17
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -3.57    |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    actor_loss       | -0.445   |
|    critic_loss      | 0.655    |
|    learning_rate    | 0.001    |
|    n_updates        | 14398    |
|    total_actor_loss | -0.312   |
|    value_loss       | 0.133    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.25e+03 |
|    ep_rew_mean      | -6.48    |
| time/               |          |
|    episodes         | 72       |
|    fps              | 7        |
|    time_elapsed     | 11925    |
|    total timesteps  | 90006    |
| train/              |          |
|    actor_loss       | -0.447   |
|    critic_loss      | 0.751    |
|    learning_rate    | 0.001    |
|    n_updates        | 14400    |
|    total_actor_loss | 0.133    |
|    value_loss       | 0.581    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.26e+03 |
|    ep_rew_mean      | -13      |
| time/               |          |
|    episodes         | 76       |
|    fps              | 7        |
|    time_elapsed     | 12719    |
|    total timesteps  | 96010    |
| train/              |          |
|    actor_loss       | -0.27    |
|    critic_loss      | 158      |
|    learning_rate    | 0.001    |
|    n_updates        | 15600    |
|    total_actor_loss | 159      |
|    value_loss       | 160      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.28e+03 |
|    ep_rew_mean      | -19.1    |
| time/               |          |
|    episodes         | 80       |
|    fps              | 7        |
|    time_elapsed     | 13522    |
|    total timesteps  | 102014   |
| train/              |          |
|    actor_loss       | -0.175   |
|    critic_loss      | 158      |
|    learning_rate    | 0.001    |
|    n_updates        | 16802    |
|    total_actor_loss | 160      |
|    value_loss       | 160      |
----------------------------------
Eval num_timesteps=108000, episode_reward=-82.58 +/- 79.72
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -82.6    |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    actor_loss       | -0.0297  |
|    critic_loss      | 0.501    |
|    learning_rate    | 0.001    |
|    n_updates        | 17998    |
|    total_actor_loss | 0.142    |
|    value_loss       | 0.172    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.29e+03 |
|    ep_rew_mean      | -22.2    |
| time/               |          |
|    episodes         | 84       |
|    fps              | 7        |
|    time_elapsed     | 14709    |
|    total timesteps  | 108018   |
| train/              |          |
|    actor_loss       | -0.0343  |
|    critic_loss      | 0.579    |
|    learning_rate    | 0.001    |
|    n_updates        | 18002    |
|    total_actor_loss | 0.19     |
|    value_loss       | 0.225    |
----------------------------------
Killed
2021-12-06 13:07:47.470301: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-06 13:07:47.470377: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Sufficient Memory Available. Using 20.73744 GB with 30.081908736 GB available
Logging to assets/out/models/exp8/TD3_18
Terminated
2021-12-06 13:19:43.471745: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-06 13:19:43.471806: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Sufficient Memory Available. Using 23.0416 GB with 30.079987712 GB available
Logging to assets/out/models/exp8/TD3_20
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 707      |
|    ep_rew_mean     | 124      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 52       |
|    time_elapsed    | 53       |
|    total timesteps | 2828     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 702      |
|    ep_rew_mean     | 169      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 54       |
|    time_elapsed    | 103      |
|    total timesteps | 5616     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 694      |
|    ep_rew_mean     | 211      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 54       |
|    time_elapsed    | 151      |
|    total timesteps | 8323     |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=10000, episode_reward=-217.76 +/- 167.28
Episode length: 1501.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.5e+03  |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 895      |
|    ep_rew_mean      | 146      |
| time/               |          |
|    episodes         | 16       |
|    fps              | 12       |
|    time_elapsed     | 1133     |
|    total timesteps  | 14327    |
| train/              |          |
|    actor_loss       | -0.518   |
|    critic_loss      | 0.75     |
|    learning_rate    | 0.001    |
|    n_updates        | 864      |
|    total_actor_loss | -0.257   |
|    value_loss       | 0.261    |
----------------------------------
Terminated
2021-12-06 13:46:22.240776: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/nvidia-470:/usr/local/cuda/lib64:/root/.mujoco/mjpro150/bin:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-12-06 13:46:22.240839: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
/root/trainer
Using cuda device
Sufficient Memory Available. Using 23.0416 GB with 30.0793856 GB available
Logging to assets/out/models/exp8/TD3_21
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 929      |
|    ep_rew_mean     | 191      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 55       |
|    time_elapsed    | 67       |
|    total timesteps | 3715     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 952      |
|    ep_rew_mean     | 129      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 56       |
|    time_elapsed    | 134      |
|    total timesteps | 7620     |
---------------------------------
Found 1 GPUs for rendering. Using device 0.
Eval num_timesteps=10000, episode_reward=-148.22 +/- 116.05
Episode length: 1501.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.5e+03  |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 949      |
|    ep_rew_mean      | 136      |
| time/               |          |
|    episodes         | 12       |
|    fps              | 9        |
|    time_elapsed     | 1156     |
|    total timesteps  | 11387    |
| train/              |          |
|    actor_loss       | -0.197   |
|    critic_loss      | 32.3     |
|    learning_rate    | 0.001    |
|    n_updates        | 1380     |
|    total_actor_loss | 0.349    |
|    value_loss       | 0.545    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.07e+03 |
|    ep_rew_mean      | 90       |
| time/               |          |
|    episodes         | 16       |
|    fps              | 4        |
|    time_elapsed     | 3670     |
|    total timesteps  | 17182    |
| train/              |          |
|    actor_loss       | -0.629   |
|    critic_loss      | 0.851    |
|    learning_rate    | 0.001    |
|    n_updates        | 7180     |
|    total_actor_loss | -0.209   |
|    value_loss       | 0.42     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-120.56 +/- 113.25
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -121     |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    actor_loss       | -0.575   |
|    critic_loss      | 1.07     |
|    learning_rate    | 0.001    |
|    n_updates        | 9990     |
|    total_actor_loss | -0.0541  |
|    value_loss       | 0.52     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.1e+03  |
|    ep_rew_mean      | 69.8     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 3        |
|    time_elapsed     | 6139     |
|    total timesteps  | 21954    |
| train/              |          |
|    actor_loss       | -0.368   |
|    critic_loss      | 0.892    |
|    learning_rate    | 0.001    |
|    n_updates        | 11950    |
|    total_actor_loss | -0.0108  |
|    value_loss       | 0.357    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.16e+03 |
|    ep_rew_mean      | 55.9     |
| time/               |          |
|    episodes         | 24       |
|    fps              | 3        |
|    time_elapsed     | 8777     |
|    total timesteps  | 27958    |
| train/              |          |
|    actor_loss       | 0.00631  |
|    critic_loss      | 0.578    |
|    learning_rate    | 0.001    |
|    n_updates        | 17950    |
|    total_actor_loss | 0.373    |
|    value_loss       | 0.367    |
----------------------------------
Eval num_timesteps=30000, episode_reward=-73.97 +/- 141.49
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -74      |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    actor_loss       | 0.278    |
|    critic_loss      | 0.892    |
|    learning_rate    | 0.001    |
|    n_updates        | 19990    |
|    total_actor_loss | 0.906    |
|    value_loss       | 0.628    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.21e+03 |
|    ep_rew_mean      | 29.8     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 2        |
|    time_elapsed     | 11821    |
|    total timesteps  | 33962    |
| train/              |          |
|    actor_loss       | 1.23     |
|    critic_loss      | 31.5     |
|    learning_rate    | 0.001    |
|    n_updates        | 23960    |
|    total_actor_loss | 34.5     |
|    value_loss       | 33.2     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.25e+03 |
|    ep_rew_mean      | 9.4      |
| time/               |          |
|    episodes         | 32       |
|    fps              | 2        |
|    time_elapsed     | 14409    |
|    total timesteps  | 39966    |
| train/              |          |
|    actor_loss       | 2.17     |
|    critic_loss      | 0.789    |
|    learning_rate    | 0.001    |
|    n_updates        | 29960    |
|    total_actor_loss | 2.68     |
|    value_loss       | 0.514    |
----------------------------------
Eval num_timesteps=40000, episode_reward=-70.80 +/- 135.87
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -70.8    |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    actor_loss       | 2.23     |
|    critic_loss      | 0.629    |
|    learning_rate    | 0.001    |
|    n_updates        | 29990    |
|    total_actor_loss | 2.61     |
|    value_loss       | 0.384    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.28e+03 |
|    ep_rew_mean      | -9.96    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 2        |
|    time_elapsed     | 17382    |
|    total timesteps  | 45970    |
| train/              |          |
|    actor_loss       | 3.2      |
|    critic_loss      | 29.5     |
|    learning_rate    | 0.001    |
|    n_updates        | 35960    |
|    total_actor_loss | 3.63     |
|    value_loss       | 0.431    |
----------------------------------
Eval num_timesteps=50000, episode_reward=-43.77 +/- 86.26
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -43.8    |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    actor_loss       | 3.92     |
|    critic_loss      | 29.4     |
|    learning_rate    | 0.001    |
|    n_updates        | 39990    |
|    total_actor_loss | 4.26     |
|    value_loss       | 0.339    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.3e+03  |
|    ep_rew_mean      | -17      |
| time/               |          |
|    episodes         | 40       |
|    fps              | 2        |
|    time_elapsed     | 20397    |
|    total timesteps  | 51974    |
| train/              |          |
|    actor_loss       | 4.26     |
|    critic_loss      | 29.7     |
|    learning_rate    | 0.001    |
|    n_updates        | 41970    |
|    total_actor_loss | 37.1     |
|    value_loss       | 32.8     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.32e+03 |
|    ep_rew_mean      | -33.9    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 2        |
|    time_elapsed     | 23008    |
|    total timesteps  | 57978    |
| train/              |          |
|    actor_loss       | 4.84     |
|    critic_loss      | 1        |
|    learning_rate    | 0.001    |
|    n_updates        | 47970    |
|    total_actor_loss | 5.29     |
|    value_loss       | 0.446    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-119.75 +/- 107.03
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -120     |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    actor_loss       | 5.6      |
|    critic_loss      | 0.855    |
|    learning_rate    | 0.001    |
|    n_updates        | 49990    |
|    total_actor_loss | 6.13     |
|    value_loss       | 0.528    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.33e+03 |
|    ep_rew_mean      | -39.8    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 2        |
|    time_elapsed     | 25985    |
|    total timesteps  | 63982    |
| train/              |          |
|    actor_loss       | 6.79     |
|    critic_loss      | 28       |
|    learning_rate    | 0.001    |
|    n_updates        | 53980    |
|    total_actor_loss | 38.1     |
|    value_loss       | 31.3     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.35e+03 |
|    ep_rew_mean      | -48.5    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 2        |
|    time_elapsed     | 28593    |
|    total timesteps  | 69986    |
| train/              |          |
|    actor_loss       | 7.1      |
|    critic_loss      | 28.1     |
|    learning_rate    | 0.001    |
|    n_updates        | 59980    |
|    total_actor_loss | 38.2     |
|    value_loss       | 31.1     |
----------------------------------
Eval num_timesteps=70000, episode_reward=-211.24 +/- 54.47
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -211     |
| time/               |          |
|    total_timesteps  | 70000    |
| train/              |          |
|    actor_loss       | 7.03     |
|    critic_loss      | 0.584    |
|    learning_rate    | 0.001    |
|    n_updates        | 59990    |
|    total_actor_loss | 7.31     |
|    value_loss       | 0.273    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.36e+03 |
|    ep_rew_mean      | -44.5    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 2        |
|    time_elapsed     | 31571    |
|    total timesteps  | 75990    |
| train/              |          |
|    actor_loss       | 9.23     |
|    critic_loss      | 52.5     |
|    learning_rate    | 0.001    |
|    n_updates        | 65980    |
|    total_actor_loss | 39.9     |
|    value_loss       | 30.7     |
----------------------------------
Eval num_timesteps=80000, episode_reward=-162.91 +/- 168.02
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -163     |
| time/               |          |
|    total_timesteps  | 80000    |
| train/              |          |
|    actor_loss       | 10.8     |
|    critic_loss      | 0.946    |
|    learning_rate    | 0.001    |
|    n_updates        | 69990    |
|    total_actor_loss | 11.1     |
|    value_loss       | 0.298    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.37e+03 |
|    ep_rew_mean      | -58.3    |
| time/               |          |
|    episodes         | 60       |
|    fps              | 2        |
|    time_elapsed     | 34548    |
|    total timesteps  | 81994    |
| train/              |          |
|    actor_loss       | 11.3     |
|    critic_loss      | 25.4     |
|    learning_rate    | 0.001    |
|    n_updates        | 71990    |
|    total_actor_loss | 41.1     |
|    value_loss       | 29.9     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.37e+03 |
|    ep_rew_mean      | -68.4    |
| time/               |          |
|    episodes         | 64       |
|    fps              | 2        |
|    time_elapsed     | 37133    |
|    total timesteps  | 87998    |
| train/              |          |
|    actor_loss       | 13.4     |
|    critic_loss      | 1.1      |
|    learning_rate    | 0.001    |
|    n_updates        | 77990    |
|    total_actor_loss | 13.8     |
|    value_loss       | 0.432    |
----------------------------------
Eval num_timesteps=90000, episode_reward=0.33 +/- 192.98
Episode length: 1371.80 +/- 258.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.37e+03 |
|    mean_reward      | 0.325    |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    actor_loss       | 13.7     |
|    critic_loss      | 0.71     |
|    learning_rate    | 0.001    |
|    n_updates        | 79990    |
|    total_actor_loss | 14       |
|    value_loss       | 0.275    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.38e+03 |
|    ep_rew_mean      | -61.9    |
| time/               |          |
|    episodes         | 68       |
|    fps              | 2        |
|    time_elapsed     | 40100    |
|    total timesteps  | 94002    |
| train/              |          |
|    actor_loss       | 14.2     |
|    critic_loss      | 0.571    |
|    learning_rate    | 0.001    |
|    n_updates        | 84000    |
|    total_actor_loss | 14.4     |
|    value_loss       | 0.247    |
----------------------------------
Eval num_timesteps=100000, episode_reward=-148.39 +/- 147.29
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -148     |
| time/               |          |
|    total_timesteps  | 100000   |
| train/              |          |
|    actor_loss       | 15       |
|    critic_loss      | 0.94     |
|    learning_rate    | 0.001    |
|    n_updates        | 89990    |
|    total_actor_loss | 15.5     |
|    value_loss       | 0.448    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.39e+03 |
|    ep_rew_mean      | -68.9    |
| time/               |          |
|    episodes         | 72       |
|    fps              | 2        |
|    time_elapsed     | 43061    |
|    total timesteps  | 100006   |
| train/              |          |
|    actor_loss       | 15.1     |
|    critic_loss      | 0.435    |
|    learning_rate    | 0.001    |
|    n_updates        | 90000    |
|    total_actor_loss | 15.3     |
|    value_loss       | 0.249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.39e+03 |
|    ep_rew_mean      | -66.9    |
| time/               |          |
|    episodes         | 76       |
|    fps              | 2        |
|    time_elapsed     | 45657    |
|    total timesteps  | 106010   |
| train/              |          |
|    actor_loss       | 15.6     |
|    critic_loss      | 0.668    |
|    learning_rate    | 0.001    |
|    n_updates        | 96000    |
|    total_actor_loss | 16       |
|    value_loss       | 0.415    |
----------------------------------
Eval num_timesteps=110000, episode_reward=-114.60 +/- 118.81
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -115     |
| time/               |          |
|    total_timesteps  | 110000   |
| train/              |          |
|    actor_loss       | 15.8     |
|    critic_loss      | 0.589    |
|    learning_rate    | 0.001    |
|    n_updates        | 99990    |
|    total_actor_loss | 16.1     |
|    value_loss       | 0.302    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.4e+03  |
|    ep_rew_mean      | -70.8    |
| time/               |          |
|    episodes         | 80       |
|    fps              | 2        |
|    time_elapsed     | 48619    |
|    total timesteps  | 112014   |
| train/              |          |
|    actor_loss       | 15.9     |
|    critic_loss      | 0.627    |
|    learning_rate    | 0.001    |
|    n_updates        | 102010   |
|    total_actor_loss | 16.2     |
|    value_loss       | 0.242    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.4e+03  |
|    ep_rew_mean      | -75.6    |
| time/               |          |
|    episodes         | 84       |
|    fps              | 2        |
|    time_elapsed     | 51214    |
|    total timesteps  | 118018   |
| train/              |          |
|    actor_loss       | 16.6     |
|    critic_loss      | 0.559    |
|    learning_rate    | 0.001    |
|    n_updates        | 108010   |
|    total_actor_loss | 16.9     |
|    value_loss       | 0.276    |
----------------------------------
Eval num_timesteps=120000, episode_reward=55.18 +/- 118.50
Episode length: 1425.80 +/- 150.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.43e+03 |
|    mean_reward      | 55.2     |
| time/               |          |
|    total_timesteps  | 120000   |
| train/              |          |
|    actor_loss       | 16.8     |
|    critic_loss      | 22.2     |
|    learning_rate    | 0.001    |
|    n_updates        | 109990   |
|    total_actor_loss | 17.1     |
|    value_loss       | 0.292    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.41e+03 |
|    ep_rew_mean      | -71.4    |
| time/               |          |
|    episodes         | 88       |
|    fps              | 2        |
|    time_elapsed     | 54098    |
|    total timesteps  | 123837   |
| train/              |          |
|    actor_loss       | 16.8     |
|    critic_loss      | 0.805    |
|    learning_rate    | 0.001    |
|    n_updates        | 113830   |
|    total_actor_loss | 17.2     |
|    value_loss       | 0.401    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.41e+03 |
|    ep_rew_mean      | -67.8    |
| time/               |          |
|    episodes         | 92       |
|    fps              | 2        |
|    time_elapsed     | 56687    |
|    total timesteps  | 129841   |
| train/              |          |
|    actor_loss       | 16.5     |
|    critic_loss      | 0.528    |
|    learning_rate    | 0.001    |
|    n_updates        | 119840   |
|    total_actor_loss | 16.7     |
|    value_loss       | 0.236    |
----------------------------------
Eval num_timesteps=130000, episode_reward=-59.92 +/- 63.76
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -59.9    |
| time/               |          |
|    total_timesteps  | 130000   |
| train/              |          |
|    actor_loss       | 16.5     |
|    critic_loss      | 0.821    |
|    learning_rate    | 0.001    |
|    n_updates        | 119990   |
|    total_actor_loss | 16.9     |
|    value_loss       | 0.36     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.42e+03 |
|    ep_rew_mean      | -71.4    |
| time/               |          |
|    episodes         | 96       |
|    fps              | 2        |
|    time_elapsed     | 59633    |
|    total timesteps  | 135845   |
| train/              |          |
|    actor_loss       | 16.3     |
|    critic_loss      | 0.48     |
|    learning_rate    | 0.001    |
|    n_updates        | 125840   |
|    total_actor_loss | 16.5     |
|    value_loss       | 0.21     |
----------------------------------
Eval num_timesteps=140000, episode_reward=-194.03 +/- 116.19
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -194     |
| time/               |          |
|    total_timesteps  | 140000   |
| train/              |          |
|    actor_loss       | 16.2     |
|    critic_loss      | 21.6     |
|    learning_rate    | 0.001    |
|    n_updates        | 129990   |
|    total_actor_loss | 16.3     |
|    value_loss       | 0.135    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.42e+03 |
|    ep_rew_mean      | -72.1    |
| time/               |          |
|    episodes         | 100      |
|    fps              | 2        |
|    time_elapsed     | 62647    |
|    total timesteps  | 141849   |
| train/              |          |
|    actor_loss       | 16.2     |
|    critic_loss      | 0.616    |
|    learning_rate    | 0.001    |
|    n_updates        | 131840   |
|    total_actor_loss | 16.4     |
|    value_loss       | 0.208    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.44e+03 |
|    ep_rew_mean      | -86.2    |
| time/               |          |
|    episodes         | 104      |
|    fps              | 2        |
|    time_elapsed     | 65246    |
|    total timesteps  | 147853   |
| train/              |          |
|    actor_loss       | 16.6     |
|    critic_loss      | 22.4     |
|    learning_rate    | 0.001    |
|    n_updates        | 137850   |
|    total_actor_loss | 17       |
|    value_loss       | 0.41     |
----------------------------------
Eval num_timesteps=150000, episode_reward=-47.77 +/- 81.67
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -47.8    |
| time/               |          |
|    total_timesteps  | 150000   |
| train/              |          |
|    actor_loss       | 16.5     |
|    critic_loss      | 0.779    |
|    learning_rate    | 0.001    |
|    n_updates        | 139990   |
|    total_actor_loss | 16.9     |
|    value_loss       | 0.368    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.46e+03 |
|    ep_rew_mean      | -95      |
| time/               |          |
|    episodes         | 108      |
|    fps              | 2        |
|    time_elapsed     | 68222    |
|    total timesteps  | 153857   |
| train/              |          |
|    actor_loss       | 17       |
|    critic_loss      | 0.721    |
|    learning_rate    | 0.001    |
|    n_updates        | 143850   |
|    total_actor_loss | 17.4     |
|    value_loss       | 0.358    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.48e+03 |
|    ep_rew_mean      | -101     |
| time/               |          |
|    episodes         | 112      |
|    fps              | 2        |
|    time_elapsed     | 70673    |
|    total timesteps  | 159519   |
| train/              |          |
|    actor_loss       | 17.2     |
|    critic_loss      | 22       |
|    learning_rate    | 0.001    |
|    n_updates        | 149510   |
|    total_actor_loss | 45.3     |
|    value_loss       | 28.1     |
----------------------------------
Eval num_timesteps=160000, episode_reward=-88.88 +/- 111.62
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -88.9    |
| time/               |          |
|    total_timesteps  | 160000   |
| train/              |          |
|    actor_loss       | 17.2     |
|    critic_loss      | 21.8     |
|    learning_rate    | 0.001    |
|    n_updates        | 149990   |
|    total_actor_loss | 17.4     |
|    value_loss       | 0.2      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.48e+03 |
|    ep_rew_mean      | -108     |
| time/               |          |
|    episodes         | 116      |
|    fps              | 2        |
|    time_elapsed     | 73682    |
|    total timesteps  | 165523   |
| train/              |          |
|    actor_loss       | 17.8     |
|    critic_loss      | 0.534    |
|    learning_rate    | 0.001    |
|    n_updates        | 155520   |
|    total_actor_loss | 17.9     |
|    value_loss       | 0.162    |
----------------------------------
Eval num_timesteps=170000, episode_reward=-103.88 +/- 108.71
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -104     |
| time/               |          |
|    total_timesteps  | 170000   |
| train/              |          |
|    actor_loss       | 18.1     |
|    critic_loss      | 42.7     |
|    learning_rate    | 0.001    |
|    n_updates        | 159990   |
|    total_actor_loss | 45.9     |
|    value_loss       | 27.9     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -110     |
| time/               |          |
|    episodes         | 120      |
|    fps              | 2        |
|    time_elapsed     | 76675    |
|    total timesteps  | 171527   |
| train/              |          |
|    actor_loss       | 18.1     |
|    critic_loss      | 21.7     |
|    learning_rate    | 0.001    |
|    n_updates        | 161520   |
|    total_actor_loss | 18.3     |
|    value_loss       | 0.245    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -114     |
| time/               |          |
|    episodes         | 124      |
|    fps              | 2        |
|    time_elapsed     | 79280    |
|    total timesteps  | 177531   |
| train/              |          |
|    actor_loss       | 18.7     |
|    critic_loss      | 0.692    |
|    learning_rate    | 0.001    |
|    n_updates        | 167530   |
|    total_actor_loss | 19       |
|    value_loss       | 0.367    |
----------------------------------
Eval num_timesteps=180000, episode_reward=29.88 +/- 58.57
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | 29.9     |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    actor_loss       | 18.8     |
|    critic_loss      | 0.807    |
|    learning_rate    | 0.001    |
|    n_updates        | 169990   |
|    total_actor_loss | 19.2     |
|    value_loss       | 0.34     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -109     |
| time/               |          |
|    episodes         | 128      |
|    fps              | 2        |
|    time_elapsed     | 82282    |
|    total timesteps  | 183535   |
| train/              |          |
|    actor_loss       | 19       |
|    critic_loss      | 0.644    |
|    learning_rate    | 0.001    |
|    n_updates        | 173530   |
|    total_actor_loss | 19.2     |
|    value_loss       | 0.241    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -104     |
| time/               |          |
|    episodes         | 132      |
|    fps              | 2        |
|    time_elapsed     | 84872    |
|    total timesteps  | 189539   |
| train/              |          |
|    actor_loss       | 18.7     |
|    critic_loss      | 0.54     |
|    learning_rate    | 0.001    |
|    n_updates        | 179530   |
|    total_actor_loss | 18.9     |
|    value_loss       | 0.245    |
----------------------------------
Eval num_timesteps=190000, episode_reward=-34.89 +/- 152.46
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -34.9    |
| time/               |          |
|    total_timesteps  | 190000   |
| train/              |          |
|    actor_loss       | 18.8     |
|    critic_loss      | 41.9     |
|    learning_rate    | 0.001    |
|    n_updates        | 179990   |
|    total_actor_loss | 74       |
|    value_loss       | 55.2     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -99.1    |
| time/               |          |
|    episodes         | 136      |
|    fps              | 2        |
|    time_elapsed     | 87830    |
|    total timesteps  | 195543   |
| train/              |          |
|    actor_loss       | 18.4     |
|    critic_loss      | 0.541    |
|    learning_rate    | 0.001    |
|    n_updates        | 185540   |
|    total_actor_loss | 18.5     |
|    value_loss       | 0.131    |
----------------------------------
Eval num_timesteps=200000, episode_reward=-53.81 +/- 146.11
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -53.8    |
| time/               |          |
|    total_timesteps  | 200000   |
| train/              |          |
|    actor_loss       | 17.6     |
|    critic_loss      | 21.7     |
|    learning_rate    | 0.001    |
|    n_updates        | 189990   |
|    total_actor_loss | 45.6     |
|    value_loss       | 28       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -96.8    |
| time/               |          |
|    episodes         | 140      |
|    fps              | 2        |
|    time_elapsed     | 90774    |
|    total timesteps  | 201547   |
| train/              |          |
|    actor_loss       | 17.5     |
|    critic_loss      | 0.61     |
|    learning_rate    | 0.001    |
|    n_updates        | 191540   |
|    total_actor_loss | 17.8     |
|    value_loss       | 0.373    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -94.6    |
| time/               |          |
|    episodes         | 144      |
|    fps              | 2        |
|    time_elapsed     | 93348    |
|    total timesteps  | 207551   |
| train/              |          |
|    actor_loss       | 17       |
|    critic_loss      | 0.542    |
|    learning_rate    | 0.001    |
|    n_updates        | 197550   |
|    total_actor_loss | 17.2     |
|    value_loss       | 0.213    |
----------------------------------
Eval num_timesteps=210000, episode_reward=-75.79 +/- 92.49
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -75.8    |
| time/               |          |
|    total_timesteps  | 210000   |
| train/              |          |
|    actor_loss       | 16.9     |
|    critic_loss      | 0.441    |
|    learning_rate    | 0.001    |
|    n_updates        | 199990   |
|    total_actor_loss | 17.1     |
|    value_loss       | 0.189    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -101     |
| time/               |          |
|    episodes         | 148      |
|    fps              | 2        |
|    time_elapsed     | 96331    |
|    total timesteps  | 213555   |
| train/              |          |
|    actor_loss       | 17       |
|    critic_loss      | 0.539    |
|    learning_rate    | 0.001    |
|    n_updates        | 203550   |
|    total_actor_loss | 17.2     |
|    value_loss       | 0.284    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -99.5    |
| time/               |          |
|    episodes         | 152      |
|    fps              | 2        |
|    time_elapsed     | 98892    |
|    total timesteps  | 219559   |
| train/              |          |
|    actor_loss       | 17       |
|    critic_loss      | 22       |
|    learning_rate    | 0.001    |
|    n_updates        | 209550   |
|    total_actor_loss | 17.3     |
|    value_loss       | 0.234    |
----------------------------------
Eval num_timesteps=220000, episode_reward=-108.39 +/- 104.41
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -108     |
| time/               |          |
|    total_timesteps  | 220000   |
| train/              |          |
|    actor_loss       | 17       |
|    critic_loss      | 0.53     |
|    learning_rate    | 0.001    |
|    n_updates        | 209990   |
|    total_actor_loss | 17.3     |
|    value_loss       | 0.326    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -102     |
| time/               |          |
|    episodes         | 156      |
|    fps              | 2        |
|    time_elapsed     | 101817   |
|    total timesteps  | 225563   |
| train/              |          |
|    actor_loss       | 16.6     |
|    critic_loss      | 0.616    |
|    learning_rate    | 0.001    |
|    n_updates        | 215560   |
|    total_actor_loss | 17       |
|    value_loss       | 0.369    |
----------------------------------
Eval num_timesteps=230000, episode_reward=-100.35 +/- 109.67
Episode length: 1501.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 1.5e+03  |
|    mean_reward      | -100     |
| time/               |          |
|    total_timesteps  | 230000   |
| train/              |          |
|    actor_loss       | 16.4     |
|    critic_loss      | 22.7     |
|    learning_rate    | 0.001    |
|    n_updates        | 219990   |
|    total_actor_loss | 16.7     |
|    value_loss       | 0.359    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 1.5e+03  |
|    ep_rew_mean      | -97.1    |
| time/               |          |
|    episodes         | 160      |
|    fps              | 2        |
|    time_elapsed     | 104747   |
|    total timesteps  | 231567   |
| train/              |          |
|    actor_loss       | 16.5     |
|    critic_loss      | 22.4     |
|    learning_rate    | 0.001    |
|    n_updates        | 221560   |
|    total_actor_loss | 44.8     |
|    value_loss       | 28.3     |
----------------------------------
Terminated
